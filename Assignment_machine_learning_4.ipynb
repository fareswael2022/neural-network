{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "DOOs_D51xGkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsppGwlWvlaN",
        "outputId": "12452315-a869-48ba-aa5b-5dbb09709a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 93 | Iteration: 16768 | Loss: 0.39565422584758636\n",
            "Epoch: 93 | Iteration: 16832 | Loss: 0.4675080930301407\n",
            "Epoch: 93 | Iteration: 16896 | Loss: 0.3883000177527065\n",
            "Epoch: 93 | Iteration: 16960 | Loss: 0.3291251781563637\n",
            "Epoch: 93 | Iteration: 17024 | Loss: 0.31784632896320586\n",
            "Epoch: 93 | Iteration: 17088 | Loss: 0.6048128625243274\n",
            "Epoch: 93 | Iteration: 17152 | Loss: 0.4446983248567203\n",
            "Epoch: 93 | Iteration: 17216 | Loss: 0.5395938319832034\n",
            "Epoch: 93 | Iteration: 17280 | Loss: 0.014365136811070116\n",
            "Epoch: 93 | Iteration: 17344 | Loss: 0.44985589860612196\n",
            "Epoch: 93 | Iteration: 17408 | Loss: 0.1400194234675026\n",
            "Epoch: 93 | Iteration: 17472 | Loss: 0.2148879446198782\n",
            "Epoch: 93 | Iteration: 17536 | Loss: 0.6858312927003232\n",
            "Epoch: 93 | Iteration: 17600 | Loss: 0.21218880734272716\n",
            "Epoch: 93 | Iteration: 17664 | Loss: 0.4071716504186482\n",
            "Epoch: 93 | Iteration: 17728 | Loss: 1.576655583408189\n",
            "Epoch: 93 | Iteration: 17792 | Loss: 0.22635240028225206\n",
            "Epoch: 93 | Iteration: 17856 | Loss: 0.4291698748031404\n",
            "Epoch: 93 | Iteration: 17920 | Loss: 0.047434830844025846\n",
            "Epoch: 93 | Iteration: 17984 | Loss: 0.0898496620078032\n",
            "Epoch: 93 | Iteration: 18048 | Loss: 0.5043927623523395\n",
            "Epoch: 93 | Iteration: 18112 | Loss: 0.23935429975862627\n",
            "Epoch: 93 | Iteration: 18176 | Loss: 0.11001406545619269\n",
            "Epoch: 93 | Iteration: 18240 | Loss: 0.6262817605575621\n",
            "Epoch: 93 | Iteration: 18304 | Loss: 0.487570736077355\n",
            "Epoch: 93 | Iteration: 18368 | Loss: 0.3821665840837555\n",
            "Epoch: 93 | Iteration: 18432 | Loss: 0.7311326065691837\n",
            "Epoch: 93 | Iteration: 18496 | Loss: 0.17218870177760853\n",
            "Epoch: 93 | Iteration: 18560 | Loss: 0.22632101383661815\n",
            "Epoch: 93 | Iteration: 18624 | Loss: 0.0761976370343974\n",
            "Epoch: 93 | Iteration: 18688 | Loss: 0.37596808616713606\n",
            "Epoch: 93 | Iteration: 18752 | Loss: 0.08474312945233696\n",
            "Epoch: 93 | Iteration: 18816 | Loss: 0.08760575641421453\n",
            "Epoch: 93 | Iteration: 18880 | Loss: 0.22782122532282556\n",
            "Epoch: 93 | Iteration: 18944 | Loss: 0.21644839030940077\n",
            "Epoch: 93 | Iteration: 19008 | Loss: 0.15540523674866524\n",
            "Epoch: 93 | Iteration: 19072 | Loss: 1.3611383678962037\n",
            "Epoch: 93 | Iteration: 19136 | Loss: 0.1657275704688954\n",
            "Epoch: 93 | Iteration: 19200 | Loss: 0.19600443325382153\n",
            "Epoch: 93 | Iteration: 19264 | Loss: 0.5203066627451474\n",
            "Epoch: 93 | Iteration: 19328 | Loss: 1.648722653760542\n",
            "Epoch: 93 | Iteration: 19392 | Loss: 0.34844305066017045\n",
            "Epoch: 93 | Iteration: 19456 | Loss: 0.2635712699265607\n",
            "Epoch: 93 | Iteration: 19520 | Loss: 0.05730430688734946\n",
            "Epoch: 93 | Iteration: 19584 | Loss: 0.13064119567430782\n",
            "Epoch: 93 | Iteration: 19648 | Loss: 0.11965492058717957\n",
            "Epoch: 93 | Iteration: 19712 | Loss: 0.015317896256260689\n",
            "Epoch: 93 | Iteration: 19776 | Loss: 0.8903944190775201\n",
            "Epoch: 93 | Iteration: 19840 | Loss: 0.4222418467492236\n",
            "Epoch: 93 | Iteration: 19904 | Loss: 0.3931568773895025\n",
            "Epoch: 93 | Iteration: 19968 | Loss: 0.31598068228837245\n",
            "Epoch: 93 | Iteration: 20032 | Loss: 0.28344877941321206\n",
            "Epoch: 93 | Iteration: 20096 | Loss: 0.09906662575009793\n",
            "Epoch: 93 | Iteration: 20160 | Loss: 0.9463188141843821\n",
            "Epoch: 93 | Iteration: 20224 | Loss: 0.46132624364296176\n",
            "Epoch: 93 | Iteration: 20288 | Loss: 0.8306988740786758\n",
            "Epoch: 93 | Iteration: 20352 | Loss: 0.007590944969738661\n",
            "Epoch: 93 | Iteration: 20416 | Loss: 0.01641343984076169\n",
            "Epoch: 93 | Iteration: 20480 | Loss: 0.10867682757684222\n",
            "Epoch: 93 | Iteration: 20544 | Loss: 0.23718798253491397\n",
            "Epoch: 93 | Iteration: 20608 | Loss: 0.13527622159660413\n",
            "Epoch: 93 | Iteration: 20672 | Loss: 1.881093386942305\n",
            "Epoch: 93 | Iteration: 20736 | Loss: 1.203787284322012\n",
            "Epoch: 93 | Iteration: 20800 | Loss: 0.4210506306343683\n",
            "Epoch: 93 | Iteration: 20864 | Loss: 0.38820696808807575\n",
            "Epoch: 93 | Iteration: 20928 | Loss: 0.3253042796547594\n",
            "Epoch: 93 | Iteration: 20992 | Loss: 0.42174762860160075\n",
            "Epoch: 93 | Iteration: 21056 | Loss: 0.2965821890459783\n",
            "Epoch: 93 | Iteration: 21120 | Loss: 0.37229578333080104\n",
            "Epoch: 93 | Iteration: 21184 | Loss: 0.22785310611940754\n",
            "Epoch: 93 | Iteration: 21248 | Loss: 0.21660409350027066\n",
            "Epoch: 93 | Iteration: 21312 | Loss: 0.4444647144359497\n",
            "Epoch: 93 | Iteration: 21376 | Loss: 0.1910047206635256\n",
            "Epoch: 93 | Iteration: 21440 | Loss: 0.6105074585036548\n",
            "Epoch: 93 | Iteration: 21504 | Loss: 0.20150033764906916\n",
            "Epoch: 93 | Iteration: 21568 | Loss: 1.448114327026338\n",
            "Epoch: 93 | Iteration: 21632 | Loss: 0.25018956990941726\n",
            "Epoch: 93 | Iteration: 21696 | Loss: 0.11863233994399244\n",
            "Epoch: 93 | Iteration: 21760 | Loss: 0.00636599506019521\n",
            "Epoch: 93 | Iteration: 21824 | Loss: 0.003993304904606339\n",
            "Epoch: 93 | Iteration: 21888 | Loss: 0.5696216637333256\n",
            "Epoch: 93 | Iteration: 21952 | Loss: 0.39912363282270197\n",
            "Epoch: 93 | Iteration: 22016 | Loss: 0.07966241458569379\n",
            "Epoch: 93 | Iteration: 22080 | Loss: 0.44438927684599017\n",
            "Epoch: 93 | Iteration: 22144 | Loss: 0.38559341318417273\n",
            "Epoch: 93 | Iteration: 22208 | Loss: 0.254341705212049\n",
            "Epoch: 93 | Iteration: 22272 | Loss: 0.7543412163383685\n",
            "Epoch: 93 | Iteration: 22336 | Loss: 0.042353960664879974\n",
            "Epoch: 93 | Iteration: 22400 | Loss: 0.21095431189462174\n",
            "Epoch: 93 | Iteration: 22464 | Loss: 0.650237393722127\n",
            "Epoch: 93 | Iteration: 22528 | Loss: 0.9972482259068283\n",
            "Epoch: 93 | Iteration: 22592 | Loss: 0.5225928649500763\n",
            "Epoch: 93 | Iteration: 22656 | Loss: 0.1313510514242164\n",
            "Epoch: 93 | Iteration: 22720 | Loss: 0.6031145005542302\n",
            "Epoch: 93 | Iteration: 22784 | Loss: 0.1723312088271681\n",
            "Epoch: 93 | Iteration: 22848 | Loss: 0.2896499100342806\n",
            "Epoch: 93 | Iteration: 22912 | Loss: 0.0018557250646405539\n",
            "Epoch: 93 | Iteration: 22976 | Loss: 0.2993860671875145\n",
            "Epoch: 93 | Iteration: 23040 | Loss: 0.25461558637612847\n",
            "Epoch: 93 | Iteration: 23104 | Loss: 0.12990800592026336\n",
            "Epoch: 93 | Iteration: 23168 | Loss: 0.5505314787335878\n",
            "Epoch: 93 | Iteration: 23232 | Loss: 0.02682020705178299\n",
            "Epoch: 93 | Iteration: 23296 | Loss: 0.1221651947712292\n",
            "Epoch: 93 | Iteration: 23360 | Loss: 0.33389720537535567\n",
            "Epoch: 93 | Iteration: 23424 | Loss: 0.10334874695954645\n",
            "Epoch: 93 | Iteration: 23488 | Loss: 0.09163666704514542\n",
            "Epoch: 93 | Iteration: 23552 | Loss: 0.6509691378804705\n",
            "Epoch: 93 | Iteration: 23616 | Loss: 0.735445957364778\n",
            "Epoch: 93 | Iteration: 23680 | Loss: 0.8848362556722207\n",
            "Epoch: 93 | Iteration: 23744 | Loss: 0.013512315546345983\n",
            "Epoch: 93 | Iteration: 23808 | Loss: 0.4065221354607894\n",
            "Epoch: 93 | Iteration: 23872 | Loss: 0.6343148312508499\n",
            "Epoch: 93 | Iteration: 23936 | Loss: 1.0612283214848124\n",
            "Epoch: 93 | Iteration: 24000 | Loss: 0.1730870375325771\n",
            "Epoch: 93 | Iteration: 24064 | Loss: 0.08203688403129404\n",
            "Epoch: 93 | Iteration: 24128 | Loss: 0.15866028906631582\n",
            "Epoch: 93 | Iteration: 24192 | Loss: 0.3101895058913721\n",
            "Epoch: 93 | Iteration: 24256 | Loss: 0.7395645063065891\n",
            "Epoch: 93 | Iteration: 24320 | Loss: 0.4028282751056248\n",
            "Epoch: 93 | Iteration: 24384 | Loss: 0.010540273550261162\n",
            "Epoch: 93 | Iteration: 24448 | Loss: 0.24473914244706546\n",
            "Epoch: 93 | Iteration: 24512 | Loss: 0.2174878105968277\n",
            "Epoch: 93 | Iteration: 24576 | Loss: 0.41220565198593495\n",
            "Epoch: 93 | Iteration: 24640 | Loss: 0.20350368015487047\n",
            "Epoch: 93 | Iteration: 24704 | Loss: 0.12805223558369566\n",
            "Epoch: 93 | Iteration: 24768 | Loss: 0.6409158411723099\n",
            "Epoch: 93 | Iteration: 24832 | Loss: 0.05928261882847115\n",
            "Epoch: 93 | Iteration: 24896 | Loss: 0.29773621387187266\n",
            "Epoch: 93 | Iteration: 24960 | Loss: 0.012061233388871904\n",
            "Epoch: 93 | Iteration: 25024 | Loss: 0.045256324944675394\n",
            "Epoch: 93 | Iteration: 25088 | Loss: 0.3415145675164142\n",
            "Epoch: 93 | Iteration: 25152 | Loss: 0.764018281794921\n",
            "Epoch: 93 | Iteration: 25216 | Loss: 0.21226866710953846\n",
            "Epoch: 93 | Iteration: 25280 | Loss: 0.15927808962670553\n",
            "Epoch: 93 | Iteration: 25344 | Loss: 0.02580686145492557\n",
            "Epoch: 93 | Iteration: 25408 | Loss: 0.03368007891440554\n",
            "Epoch: 93 | Iteration: 25472 | Loss: 0.6007889473419794\n",
            "Epoch: 93 | Iteration: 25536 | Loss: 1.2094410541456249\n",
            "Epoch: 93 | Iteration: 25600 | Loss: 0.08150003356837836\n",
            "Epoch: 93 | Iteration: 25664 | Loss: 0.6581573038553536\n",
            "Epoch: 93 | Iteration: 25728 | Loss: 0.6048192607650603\n",
            "Epoch: 93 | Iteration: 25792 | Loss: 0.9368353795559567\n",
            "Epoch: 93 | Iteration: 25856 | Loss: 0.08475167050387643\n",
            "Epoch: 93 | Iteration: 25920 | Loss: 0.12288650375331647\n",
            "Epoch: 93 | Iteration: 25984 | Loss: 0.05047974412774511\n",
            "Epoch: 93 | Iteration: 26048 | Loss: 0.32612609320255054\n",
            "Epoch: 93 | Iteration: 26112 | Loss: 0.2771363727198153\n",
            "Epoch: 93 | Iteration: 26176 | Loss: 0.2050239829168152\n",
            "Epoch: 93 | Iteration: 26240 | Loss: 0.22047340391132003\n",
            "Epoch: 93 | Iteration: 26304 | Loss: 0.5649549443340964\n",
            "Epoch: 93 | Iteration: 26368 | Loss: 0.9587533047220166\n",
            "Epoch: 93 | Iteration: 26432 | Loss: 0.982332322892155\n",
            "Epoch: 93 | Iteration: 26496 | Loss: 1.5023189388851077\n",
            "Epoch: 93 | Iteration: 26560 | Loss: 0.9283519255075781\n",
            "Epoch: 93 | Iteration: 26624 | Loss: 0.9306992212705039\n",
            "Epoch: 93 | Iteration: 26688 | Loss: 0.8516211885191447\n",
            "Epoch: 93 | Iteration: 26752 | Loss: 0.9713613228059997\n",
            "Epoch: 93 | Iteration: 26816 | Loss: 0.28000282522910236\n",
            "Epoch: 93 | Iteration: 26880 | Loss: 1.1040520828209563\n",
            "Epoch: 93 | Iteration: 26944 | Loss: 0.0167116415217236\n",
            "Epoch: 93 | Iteration: 27008 | Loss: 0.12907021822285064\n",
            "Epoch: 93 | Iteration: 27072 | Loss: 0.0796135019717602\n",
            "Epoch: 93 | Iteration: 27136 | Loss: 1.6746169070599657\n",
            "Epoch: 93 | Iteration: 27200 | Loss: 0.9575373835741561\n",
            "Epoch: 93 | Iteration: 27264 | Loss: 0.12030735587599554\n",
            "Epoch: 93 | Iteration: 27328 | Loss: 0.021950047404480404\n",
            "Epoch: 93 | Iteration: 27392 | Loss: 0.15784532678876936\n",
            "Epoch: 93 | Iteration: 27456 | Loss: 0.5869081928215398\n",
            "Epoch: 93 | Iteration: 27520 | Loss: 0.6499716625612378\n",
            "Epoch: 93 | Iteration: 27584 | Loss: 0.7697320446914167\n",
            "Epoch: 93 | Iteration: 27648 | Loss: 0.6050121032316704\n",
            "Epoch: 93 | Iteration: 27712 | Loss: 0.25044765820972115\n",
            "Epoch: 93 | Iteration: 27776 | Loss: 0.32591070571439584\n",
            "Epoch: 93 | Iteration: 27840 | Loss: 0.9349862003546285\n",
            "Epoch: 93 | Iteration: 27904 | Loss: 0.09375482855634426\n",
            "Epoch: 93 | Iteration: 27968 | Loss: 0.027558943615776252\n",
            "Epoch: 93 | Iteration: 28032 | Loss: 0.2293677682575972\n",
            "Epoch: 93 | Iteration: 28096 | Loss: 0.1471730773865441\n",
            "Epoch: 93 | Iteration: 28160 | Loss: 0.6408105562595046\n",
            "Epoch: 93 | Iteration: 28224 | Loss: 0.07597322151002706\n",
            "Epoch: 93 | Iteration: 28288 | Loss: 0.013276856131470088\n",
            "Epoch: 93 | Iteration: 28352 | Loss: 0.7863272542734627\n",
            "Epoch: 93 | Iteration: 28416 | Loss: 0.10983988777631729\n",
            "Epoch: 93 | Iteration: 28480 | Loss: 0.09385464770175991\n",
            "Epoch: 93 | Iteration: 28544 | Loss: 0.13055420789573152\n",
            "Epoch: 93 | Iteration: 28608 | Loss: 0.7238291312757454\n",
            "Epoch: 93 | Iteration: 28672 | Loss: 0.3136419677226753\n",
            "Epoch: 93 | Iteration: 28736 | Loss: 0.12529234776216136\n",
            "Epoch: 93 | Iteration: 28800 | Loss: 0.20928507560763798\n",
            "Epoch: 93 | Iteration: 28864 | Loss: 0.10938688774350767\n",
            "Epoch: 93 | Iteration: 28928 | Loss: 0.5221370895818498\n",
            "Epoch: 93 | Iteration: 28992 | Loss: 0.2257713188089701\n",
            "Epoch: 93 | Iteration: 29056 | Loss: 0.18390659864735215\n",
            "Epoch: 93 | Iteration: 29120 | Loss: 0.33293388357851955\n",
            "Epoch: 93 | Iteration: 29184 | Loss: 0.6631843140171811\n",
            "Epoch: 93 | Iteration: 29248 | Loss: 0.3928849405736228\n",
            "Epoch: 93 | Iteration: 29312 | Loss: 0.12827148802926683\n",
            "Epoch: 93 | Iteration: 29376 | Loss: 0.19384869482926897\n",
            "Epoch: 93 | Iteration: 29440 | Loss: 0.4905049821973018\n",
            "Epoch: 93 | Iteration: 29504 | Loss: 0.21806325924897946\n",
            "Epoch: 93 | Iteration: 29568 | Loss: 0.24076687897608712\n",
            "Epoch: 93 | Iteration: 29632 | Loss: 0.15252657856021354\n",
            "Epoch: 93 | Iteration: 29696 | Loss: 0.5795253223355894\n",
            "Epoch: 93 | Iteration: 29760 | Loss: 0.05124678721706133\n",
            "Epoch: 93 | Iteration: 29824 | Loss: 0.2916072789712648\n",
            "Epoch: 93 | Iteration: 29888 | Loss: 0.5197834505746869\n",
            "Epoch: 93 | Iteration: 29952 | Loss: 0.0705390899594771\n",
            "Epoch: 93 | Iteration: 30016 | Loss: 0.7150435037453386\n",
            "Epoch: 93 | Iteration: 30080 | Loss: 0.24744996907034328\n",
            "Epoch: 93 | Iteration: 30144 | Loss: 0.31113273865027885\n",
            "Epoch: 93 | Iteration: 30208 | Loss: 0.11917438276696335\n",
            "Epoch: 93 | Iteration: 30272 | Loss: 0.06092293751552758\n",
            "Epoch: 93 | Iteration: 30336 | Loss: 0.24700725593824924\n",
            "Epoch: 93 | Iteration: 30400 | Loss: 0.056328002073269594\n",
            "Epoch: 93 | Iteration: 30464 | Loss: 0.3264138538683751\n",
            "Epoch: 93 | Iteration: 30528 | Loss: 0.15368702626436812\n",
            "Epoch: 93 | Iteration: 30592 | Loss: 0.1097531870427829\n",
            "Epoch: 93 | Iteration: 30656 | Loss: 0.6182322606511038\n",
            "Epoch: 93 | Iteration: 30720 | Loss: 0.27230043754391114\n",
            "Epoch: 93 | Iteration: 30784 | Loss: 0.2027234053530716\n",
            "Epoch: 93 | Iteration: 30848 | Loss: 0.3844175101036744\n",
            "Epoch: 93 | Iteration: 30912 | Loss: 0.8251224143777478\n",
            "Epoch: 93 | Iteration: 30976 | Loss: 0.1828908149190939\n",
            "Epoch: 93 | Iteration: 31040 | Loss: 0.5097013614916509\n",
            "Epoch: 93 | Iteration: 31104 | Loss: 0.4916154424120064\n",
            "Epoch: 93 | Iteration: 31168 | Loss: 1.746025751094712\n",
            "Epoch: 93 | Iteration: 31232 | Loss: 0.1955807823441889\n",
            "Epoch: 93 | Iteration: 31296 | Loss: 0.8943314825695721\n",
            "Epoch: 93 | Iteration: 31360 | Loss: 0.3044591163890909\n",
            "Epoch: 93 | Iteration: 31424 | Loss: 0.3358469337049216\n",
            "Epoch: 93 | Iteration: 31488 | Loss: 0.08539978371687054\n",
            "Epoch: 93 | Iteration: 31552 | Loss: 1.1355421860445296\n",
            "Epoch: 93 | Iteration: 31616 | Loss: 0.37542038280131607\n",
            "Epoch: 93 | Iteration: 31680 | Loss: 1.4213585166578695\n",
            "Epoch: 93 | Iteration: 31744 | Loss: 0.7750337841407363\n",
            "Epoch: 93 | Iteration: 31808 | Loss: 0.22785751441269117\n",
            "Epoch: 93 | Iteration: 31872 | Loss: 0.1983396480992896\n",
            "Epoch: 93 | Iteration: 31936 | Loss: 0.2687766057427219\n",
            "Epoch: 93 | Iteration: 32000 | Loss: 0.3130945226005681\n",
            "Epoch: 93 | Iteration: 32064 | Loss: 0.32630120677662133\n",
            "Epoch: 93 | Iteration: 32128 | Loss: 0.1743370722039313\n",
            "Epoch: 93 | Iteration: 32192 | Loss: 0.4950458783681327\n",
            "Epoch: 93 | Iteration: 32256 | Loss: 0.23640192748786926\n",
            "Epoch: 93 | Iteration: 32320 | Loss: 0.8028881845062812\n",
            "Epoch: 93 | Iteration: 32384 | Loss: 0.3480820500360901\n",
            "Epoch: 93 | Iteration: 32448 | Loss: 1.0635455360726234\n",
            "Epoch: 93 | Iteration: 32512 | Loss: 0.32404355662957374\n",
            "Epoch: 93 | Iteration: 32576 | Loss: 0.0007930411800729927\n",
            "Epoch: 93 | Iteration: 32640 | Loss: 0.3672162198737747\n",
            "Epoch: 93 | Iteration: 32704 | Loss: 0.3155060128216411\n",
            "Epoch: 93 | Iteration: 32768 | Loss: 0.9800391205171267\n",
            "Epoch: 93 | Iteration: 32832 | Loss: 0.7256731754982569\n",
            "Epoch: 93 | Iteration: 32896 | Loss: 0.7213875054686747\n",
            "Epoch: 93 | Iteration: 32960 | Loss: 0.01647734667678312\n",
            "Epoch: 93 | Iteration: 33024 | Loss: 0.3204488260293589\n",
            "Epoch: 93 | Iteration: 33088 | Loss: 0.036575311842528754\n",
            "Epoch: 93 | Iteration: 33152 | Loss: 0.08992820230504373\n",
            "Epoch: 93 | Iteration: 33216 | Loss: 0.24773495049476826\n",
            "Epoch: 93 | Iteration: 33280 | Loss: 0.30031663593750146\n",
            "Epoch: 93 | Iteration: 33344 | Loss: 0.38475009959574513\n",
            "Epoch: 93 | Iteration: 33408 | Loss: 0.6689317725164584\n",
            "Epoch: 93 | Iteration: 33472 | Loss: 0.5831268739718896\n",
            "Epoch: 93 | Iteration: 33536 | Loss: 0.4657617174752299\n",
            "Epoch: 93 | Iteration: 33600 | Loss: 0.07646105041446616\n",
            "Epoch: 93 | Iteration: 33664 | Loss: 0.16018577793188626\n",
            "Epoch: 93 | Iteration: 33728 | Loss: 0.5500152626598815\n",
            "Epoch: 93 | Iteration: 33792 | Loss: 0.12740431784622677\n",
            "Epoch: 93 | Iteration: 33856 | Loss: 0.012254803876899794\n",
            "Epoch: 93 | Iteration: 33920 | Loss: 0.014020568210895435\n",
            "Epoch: 93 | Iteration: 33984 | Loss: 0.5432476625762487\n",
            "Epoch: 93 | Iteration: 34048 | Loss: 0.5316068867131176\n",
            "Epoch: 93 | Iteration: 34112 | Loss: 0.01327719721202186\n",
            "Epoch: 93 | Iteration: 34176 | Loss: 0.0657361178935242\n",
            "Epoch: 93 | Iteration: 34240 | Loss: 0.003274114216905465\n",
            "Epoch: 93 | Iteration: 34304 | Loss: 0.11049422738644973\n",
            "Epoch: 93 | Iteration: 34368 | Loss: 1.3767536383357686\n",
            "Epoch: 93 | Iteration: 34432 | Loss: 0.1709753988617499\n",
            "Epoch: 93 | Iteration: 34496 | Loss: 0.18400254733724092\n",
            "Epoch: 93 | Iteration: 34560 | Loss: 0.13980322557260463\n",
            "Epoch: 93 | Iteration: 34624 | Loss: 0.6543307041873523\n",
            "Epoch: 93 | Iteration: 34688 | Loss: 0.8438032435746133\n",
            "Epoch: 93 | Iteration: 34752 | Loss: 0.4052839281135992\n",
            "Epoch: 93 | Iteration: 34816 | Loss: 0.8373682176839077\n",
            "Epoch: 93 | Iteration: 34880 | Loss: 0.5081598635622635\n",
            "Epoch: 93 | Iteration: 34944 | Loss: 0.04692584615930978\n",
            "Epoch: 93 | Iteration: 35008 | Loss: 1.3894256656848722\n",
            "Epoch: 93 | Iteration: 35072 | Loss: 0.03685376686838232\n",
            "Epoch: 93 | Iteration: 35136 | Loss: 1.0252972209632587\n",
            "Epoch: 93 | Iteration: 35200 | Loss: 1.21603088264604\n",
            "Epoch: 93 | Iteration: 35264 | Loss: 0.12600076509316493\n",
            "Epoch: 93 | Iteration: 35328 | Loss: 0.1698274847463086\n",
            "Epoch: 93 | Iteration: 35392 | Loss: 0.2391412188372259\n",
            "Epoch: 93 | Iteration: 35456 | Loss: 1.179762863246721\n",
            "Epoch: 93 | Iteration: 35520 | Loss: 0.14774299193026239\n",
            "Epoch: 93 | Iteration: 35584 | Loss: 1.0001774458284538\n",
            "Epoch: 93 | Iteration: 35648 | Loss: 0.27548806266660175\n",
            "Epoch: 93 | Iteration: 35712 | Loss: 0.10934256702830902\n",
            "Epoch: 93 | Iteration: 35776 | Loss: 0.04807613859607386\n",
            "Epoch: 93 | Iteration: 35840 | Loss: 0.2592203233892593\n",
            "Epoch: 93 | Iteration: 35904 | Loss: 0.3217337792009619\n",
            "Epoch: 93 | Iteration: 35968 | Loss: 0.07268625447513785\n",
            "Epoch: 93 | Iteration: 36032 | Loss: 1.0078016929696096\n",
            "Epoch: 93 | Iteration: 36096 | Loss: 1.3465344472690757\n",
            "Epoch: 93 | Iteration: 36160 | Loss: 0.4569690461872653\n",
            "Epoch: 93 | Iteration: 36224 | Loss: 0.11875495673297005\n",
            "Epoch: 93 | Iteration: 36288 | Loss: 0.022836419008233497\n",
            "Epoch: 93 | Iteration: 36352 | Loss: 0.20734924549459166\n",
            "Epoch: 93 | Iteration: 36416 | Loss: 0.7838031994410497\n",
            "Epoch: 93 | Iteration: 36480 | Loss: 0.022532432643629433\n",
            "Epoch: 93 | Iteration: 36544 | Loss: 0.2600087003320459\n",
            "Epoch: 93 | Iteration: 36608 | Loss: 0.0687639071539988\n",
            "Epoch: 93 | Iteration: 36672 | Loss: 0.23697098562025115\n",
            "Epoch: 93 | Iteration: 36736 | Loss: 0.9768259254599154\n",
            "Epoch: 93 | Iteration: 36800 | Loss: 0.41414437060216813\n",
            "Epoch: 93 | Iteration: 36864 | Loss: 0.1623144017171821\n",
            "Epoch: 93 | Iteration: 36928 | Loss: 0.3535719455861524\n",
            "Epoch: 93 | Iteration: 36992 | Loss: 1.0982880788233327\n",
            "Epoch: 93 | Iteration: 37056 | Loss: 0.33360102124882085\n",
            "Epoch: 93 | Iteration: 37120 | Loss: 0.5137100585271024\n",
            "Epoch: 93 | Iteration: 37184 | Loss: 0.11502624568783516\n",
            "Epoch: 93 | Iteration: 37248 | Loss: 0.40711935321501347\n",
            "Epoch: 93 | Iteration: 37312 | Loss: 1.0703267843651614\n",
            "Epoch: 93 | Iteration: 37376 | Loss: 0.9821689240000417\n",
            "Epoch: 93 | Iteration: 37440 | Loss: 0.5310664569459406\n",
            "Epoch: 93 | Iteration: 37504 | Loss: 0.2350014529738793\n",
            "Epoch: 93 | Iteration: 37568 | Loss: 0.060053408084890035\n",
            "Epoch: 93 | Iteration: 37632 | Loss: 0.21924810530344688\n",
            "Epoch: 93 | Iteration: 37696 | Loss: 0.7019892312224655\n",
            "Epoch: 93 | Iteration: 37760 | Loss: 0.772302017747562\n",
            "Epoch: 93 | Iteration: 37824 | Loss: 1.0192930222486436\n",
            "Epoch: 93 | Iteration: 37888 | Loss: 0.06889324649348477\n",
            "Epoch: 93 | Iteration: 37952 | Loss: 0.0509605687510519\n",
            "Epoch: 93 | Iteration: 38016 | Loss: 0.16750004456115453\n",
            "Epoch: 93 | Iteration: 38080 | Loss: 0.07476397099725311\n",
            "Epoch: 93 | Iteration: 38144 | Loss: 0.17031358099286764\n",
            "Epoch: 93 | Iteration: 38208 | Loss: 0.06933712660058367\n",
            "Epoch: 93 | Iteration: 38272 | Loss: 0.5529451954000002\n",
            "Epoch: 93 | Iteration: 38336 | Loss: 0.7361879891250637\n",
            "Epoch: 93 | Iteration: 38400 | Loss: 0.1582062646605959\n",
            "Epoch: 93 | Iteration: 38464 | Loss: 0.6428249201099802\n",
            "Epoch: 93 | Iteration: 38528 | Loss: 0.5722295732268952\n",
            "Epoch: 93 | Iteration: 38592 | Loss: 0.3183892080835238\n",
            "Epoch: 93 | Iteration: 38656 | Loss: 0.5893429133784059\n",
            "Epoch: 93 | Iteration: 38720 | Loss: 0.22456984502119204\n",
            "Epoch: 93 | Iteration: 38784 | Loss: 0.008246624268360446\n",
            "Epoch: 93 | Iteration: 38848 | Loss: 0.08605175269819773\n",
            "Epoch: 93 | Iteration: 38912 | Loss: 0.2785259276331977\n",
            "Epoch: 93 | Iteration: 38976 | Loss: 0.2226137539467399\n",
            "Epoch: 93 | Iteration: 39040 | Loss: 0.0073991563308134735\n",
            "Epoch: 93 | Iteration: 39104 | Loss: 0.10590034901074133\n",
            "Epoch: 93 | Iteration: 39168 | Loss: 0.7710393594332277\n",
            "Epoch: 93 | Iteration: 39232 | Loss: 0.4094503106964003\n",
            "Epoch: 93 | Iteration: 39296 | Loss: 0.9931361677662217\n",
            "Epoch: 93 | Iteration: 39360 | Loss: 1.0949488480458733\n",
            "Epoch: 93 | Iteration: 39424 | Loss: 0.1998801107634381\n",
            "Epoch: 93 | Iteration: 39488 | Loss: 0.0559360774888685\n",
            "Epoch: 93 | Iteration: 39552 | Loss: 0.44879735990098435\n",
            "Epoch: 93 | Iteration: 39616 | Loss: 0.4995045968768654\n",
            "Epoch: 93 | Iteration: 39680 | Loss: 0.4000703994422913\n",
            "Epoch: 93 | Iteration: 39744 | Loss: 0.6892126458698247\n",
            "Epoch: 93 | Iteration: 39808 | Loss: 0.75042787816255\n",
            "Epoch: 93 | Iteration: 39872 | Loss: 0.159087756326231\n",
            "Epoch: 93 | Iteration: 39936 | Loss: 0.06795098823742832\n",
            "Epoch: 93 | Iteration: 40000 | Loss: 0.25242251058462095\n",
            "Epoch: 93 | Iteration: 40064 | Loss: 0.2409715570209161\n",
            "Epoch: 93 | Iteration: 40128 | Loss: 0.1638190155798086\n",
            "Epoch: 93 | Iteration: 40192 | Loss: 0.3296851598605225\n",
            "Epoch: 93 | Iteration: 40256 | Loss: 0.46150975307649095\n",
            "Epoch: 93 | Iteration: 40320 | Loss: 0.2210594820871575\n",
            "Epoch: 93 | Iteration: 40384 | Loss: 0.24500579617024434\n",
            "Epoch: 93 | Iteration: 40448 | Loss: 0.07348679981518665\n",
            "Epoch: 93 | Iteration: 40512 | Loss: 0.10384178319592989\n",
            "Epoch: 93 | Iteration: 40576 | Loss: 0.6922626861577723\n",
            "Epoch: 93 | Iteration: 40640 | Loss: 0.6755449988718365\n",
            "Epoch: 93 | Iteration: 40704 | Loss: 0.5284521775158971\n",
            "Epoch: 93 | Iteration: 40768 | Loss: 0.21269227826824394\n",
            "Epoch: 93 | Iteration: 40832 | Loss: 0.14393087865219475\n",
            "Epoch: 93 | Iteration: 40896 | Loss: 0.08716411188842246\n",
            "Epoch: 93 | Iteration: 40960 | Loss: 0.816516774954534\n",
            "Epoch: 93 | Iteration: 41024 | Loss: 0.4274519646080979\n",
            "Epoch: 93 | Iteration: 41088 | Loss: 0.1600627587589787\n",
            "Epoch: 93 | Iteration: 41152 | Loss: 0.8625693156496412\n",
            "Epoch: 93 | Iteration: 41216 | Loss: 1.143415478602571\n",
            "Epoch: 93 | Iteration: 41280 | Loss: 0.8806470860088426\n",
            "Epoch: 93 | Iteration: 41344 | Loss: 0.32462552499294317\n",
            "Epoch: 93 | Iteration: 41408 | Loss: 0.31594230561851194\n",
            "Epoch: 93 | Iteration: 41472 | Loss: 0.043199943057544724\n",
            "Epoch: 93 | Iteration: 41536 | Loss: 1.006694629870927\n",
            "Epoch: 93 | Iteration: 41600 | Loss: 0.1359325070314471\n",
            "Epoch: 93 | Iteration: 41664 | Loss: 0.08111741364465146\n",
            "Epoch: 93 | Iteration: 41728 | Loss: 0.29376895620382515\n",
            "Epoch: 93 | Iteration: 41792 | Loss: 0.15673590863319486\n",
            "Epoch: 93 | Iteration: 41856 | Loss: 0.5124936868613401\n",
            "Epoch: 93 | Iteration: 41920 | Loss: 0.37650108562595785\n",
            "Epoch: 93 | Iteration: 41984 | Loss: 0.10089475328103786\n",
            "Epoch: 93 | Iteration: 42048 | Loss: 0.037540760743645235\n",
            "Epoch: 93 | Iteration: 42112 | Loss: 0.19536329468992752\n",
            "Epoch: 93 | Iteration: 42176 | Loss: 0.2121104023868369\n",
            "Epoch: 93 | Iteration: 42240 | Loss: 0.07159180944490054\n",
            "Epoch: 93 | Iteration: 42304 | Loss: 0.7977494331112644\n",
            "Epoch: 93 | Iteration: 42368 | Loss: 0.5583384308848676\n",
            "Epoch: 93 | Iteration: 42432 | Loss: 0.8507309110684735\n",
            "Epoch: 93 | Iteration: 42496 | Loss: 0.6276413439364245\n",
            "Epoch: 93 | Iteration: 42560 | Loss: 0.9243718299210357\n",
            "Epoch: 93 | Iteration: 42624 | Loss: 0.6345409194334022\n",
            "Epoch: 93 | Iteration: 42688 | Loss: 0.37140003645525765\n",
            "Epoch: 93 | Iteration: 42752 | Loss: 1.0605426145920498\n",
            "Epoch: 93 | Iteration: 42816 | Loss: 1.0769205731531368\n",
            "Epoch: 93 | Iteration: 42880 | Loss: 0.787837721513547\n",
            "Epoch: 93 | Iteration: 42944 | Loss: 0.47191248801012603\n",
            "Epoch: 93 | Iteration: 43008 | Loss: 0.6716119229659484\n",
            "Epoch: 93 | Iteration: 43072 | Loss: 1.359738020604519\n",
            "Epoch: 93 | Iteration: 43136 | Loss: 0.04396421815027736\n",
            "Epoch: 93 | Iteration: 43200 | Loss: 0.09727574686355425\n",
            "Epoch: 93 | Iteration: 43264 | Loss: 0.0030392502495336406\n",
            "Epoch: 93 | Iteration: 43328 | Loss: 0.017774820986923387\n",
            "Epoch: 93 | Iteration: 43392 | Loss: 0.42915963590637896\n",
            "Epoch: 93 | Iteration: 43456 | Loss: 0.08743909430794536\n",
            "Epoch: 93 | Iteration: 43520 | Loss: 0.4347376438022345\n",
            "Epoch: 93 | Iteration: 43584 | Loss: 0.2204740471986637\n",
            "Epoch: 93 | Iteration: 43648 | Loss: 1.3433498541130908\n",
            "Epoch: 93 | Iteration: 43712 | Loss: 0.0061118273014456196\n",
            "Epoch: 93 | Iteration: 43776 | Loss: 0.05020424378091168\n",
            "Epoch: 93 | Iteration: 43840 | Loss: 0.8248966280859029\n",
            "Epoch: 93 | Iteration: 43904 | Loss: 0.27906661574995995\n",
            "Epoch: 93 | Iteration: 43968 | Loss: 0.21749007528683673\n",
            "Epoch: 93 | Iteration: 44032 | Loss: 0.26343546598430173\n",
            "Epoch: 93 | Iteration: 44096 | Loss: 0.3008486533773716\n",
            "Epoch: 93 | Iteration: 44160 | Loss: 0.10999495976204476\n",
            "Epoch: 93 | Iteration: 44224 | Loss: 0.19624479300213826\n",
            "Epoch: 93 | Iteration: 44288 | Loss: 0.37655849087894433\n",
            "Epoch: 93 | Iteration: 44352 | Loss: 0.3519019169306352\n",
            "Epoch: 93 | Iteration: 44416 | Loss: 1.130223197637743\n",
            "Epoch: 93 | Iteration: 44480 | Loss: 0.32871481942689784\n",
            "Epoch: 93 | Iteration: 44544 | Loss: 0.03361723140381205\n",
            "Epoch: 93 | Iteration: 44608 | Loss: 0.45550111432681506\n",
            "Epoch: 93 | Iteration: 44672 | Loss: 0.07409594572264921\n",
            "Epoch: 93 | Iteration: 44736 | Loss: 0.03998541478673377\n",
            "Epoch: 93 | Iteration: 44800 | Loss: 0.46065588026228854\n",
            "Epoch: 93 | Iteration: 44864 | Loss: 0.20869408926199084\n",
            "Epoch: 93 | Iteration: 44928 | Loss: 0.6946589260810847\n",
            "Epoch: 93 | Iteration: 44992 | Loss: 0.18400394207226628\n",
            "Epoch: 93 | Iteration: 45056 | Loss: 0.1700012165898241\n",
            "Epoch: 93 | Iteration: 45120 | Loss: 0.845608865868511\n",
            "Epoch: 93 | Iteration: 45184 | Loss: 1.0242530709464226\n",
            "Epoch: 93 | Iteration: 45248 | Loss: 0.36984419522337\n",
            "Epoch: 93 | Iteration: 45312 | Loss: 0.5777259978954571\n",
            "Epoch: 93 | Iteration: 45376 | Loss: 0.33922033446323385\n",
            "Epoch: 93 | Iteration: 45440 | Loss: 0.8291002210265093\n",
            "Epoch: 93 | Iteration: 45504 | Loss: 0.39457774328751366\n",
            "Epoch: 93 | Iteration: 45568 | Loss: 0.6660099699699904\n",
            "Epoch: 93 | Iteration: 45632 | Loss: 0.15416670657383982\n",
            "Epoch: 93 | Iteration: 45696 | Loss: 0.0076818189864019705\n",
            "Epoch: 93 | Iteration: 45760 | Loss: 0.5913473786432819\n",
            "Epoch: 93 | Iteration: 45824 | Loss: 0.36051286181174635\n",
            "Epoch: 93 | Iteration: 45888 | Loss: 0.6967629151509598\n",
            "Epoch: 93 | Iteration: 45952 | Loss: 0.5506928152760583\n",
            "Epoch: 93 | Iteration: 46016 | Loss: 1.0440244787293294\n",
            "Epoch: 93 | Iteration: 46080 | Loss: 1.2409369399144907\n",
            "Epoch: 93 | Iteration: 46144 | Loss: 0.08015168286685657\n",
            "Epoch: 93 | Iteration: 46208 | Loss: 1.4573076061576478\n",
            "Epoch: 93 | Iteration: 46272 | Loss: 1.3952629302819668\n",
            "Epoch: 93 | Iteration: 46336 | Loss: 0.49447476511141775\n",
            "Epoch: 93 | Iteration: 46400 | Loss: 0.3471713709031279\n",
            "Epoch: 93 | Iteration: 46464 | Loss: 0.018403518805186293\n",
            "Epoch: 93 | Iteration: 46528 | Loss: 0.042076184660899754\n",
            "Epoch: 93 | Iteration: 46592 | Loss: 0.049006982226745696\n",
            "Epoch: 93 | Iteration: 46656 | Loss: 0.2321298854488666\n",
            "Epoch: 93 | Iteration: 46720 | Loss: 0.9100949801819929\n",
            "Epoch: 93 | Iteration: 46784 | Loss: 0.04691944609912142\n",
            "Epoch: 93 | Iteration: 46848 | Loss: 0.7547485328872602\n",
            "Epoch: 93 | Iteration: 46912 | Loss: 0.3293053049108556\n",
            "Epoch: 93 | Iteration: 46976 | Loss: 1.315629371863373\n",
            "Epoch: 93 | Iteration: 47040 | Loss: 0.045456974141789716\n",
            "Epoch: 93 | Iteration: 47104 | Loss: 0.3215178779839011\n",
            "Epoch: 93 | Iteration: 47168 | Loss: 0.3584283166930422\n",
            "Epoch: 93 | Iteration: 47232 | Loss: 1.42879486919871\n",
            "Epoch: 93 | Iteration: 47296 | Loss: 0.3729799360542359\n",
            "Epoch: 93 | Iteration: 47360 | Loss: 0.9051755639236755\n",
            "Epoch: 93 | Iteration: 47424 | Loss: 0.2920776783079635\n",
            "Epoch: 93 | Iteration: 47488 | Loss: 0.09847321126776892\n",
            "Epoch: 93 | Iteration: 47552 | Loss: 1.25759482528693\n",
            "Epoch: 93 | Iteration: 47616 | Loss: 0.014869190078376768\n",
            "Epoch: 93 | Iteration: 47680 | Loss: 1.4153903374037071\n",
            "Epoch: 93 | Iteration: 47744 | Loss: 0.8638995529871409\n",
            "Epoch: 93 | Iteration: 47808 | Loss: 0.08713408249456202\n",
            "Epoch: 93 | Iteration: 47872 | Loss: 0.7957612694986773\n",
            "Epoch: 93 | Iteration: 47936 | Loss: 0.5070240101093658\n",
            "Epoch: 94 | Iteration: 0 | Loss: 0.15434903498272506\n",
            "Epoch: 94 | Iteration: 64 | Loss: 0.591374509726401\n",
            "Epoch: 94 | Iteration: 128 | Loss: 1.1448513697355533\n",
            "Epoch: 94 | Iteration: 192 | Loss: 0.2764608003712863\n",
            "Epoch: 94 | Iteration: 256 | Loss: 0.17704907856888186\n",
            "Epoch: 94 | Iteration: 320 | Loss: 0.017297590545182498\n",
            "Epoch: 94 | Iteration: 384 | Loss: 0.13636924724210708\n",
            "Epoch: 94 | Iteration: 448 | Loss: 1.8006180368568596\n",
            "Epoch: 94 | Iteration: 512 | Loss: 0.4652256723680377\n",
            "Epoch: 94 | Iteration: 576 | Loss: 0.41579112138140495\n",
            "Epoch: 94 | Iteration: 640 | Loss: 0.3946970004396365\n",
            "Epoch: 94 | Iteration: 704 | Loss: 0.19065383553403198\n",
            "Epoch: 94 | Iteration: 768 | Loss: 0.47405713708590347\n",
            "Epoch: 94 | Iteration: 832 | Loss: 0.4569855753323918\n",
            "Epoch: 94 | Iteration: 896 | Loss: 0.8856994421612331\n",
            "Epoch: 94 | Iteration: 960 | Loss: 0.5139329232669052\n",
            "Epoch: 94 | Iteration: 1024 | Loss: 1.0117613232492113\n",
            "Epoch: 94 | Iteration: 1088 | Loss: 0.7507126274595373\n",
            "Epoch: 94 | Iteration: 1152 | Loss: 0.33165447512011903\n",
            "Epoch: 94 | Iteration: 1216 | Loss: 0.8993314791494655\n",
            "Epoch: 94 | Iteration: 1280 | Loss: 0.18509516677309545\n",
            "Epoch: 94 | Iteration: 1344 | Loss: 1.6349975380959862\n",
            "Epoch: 94 | Iteration: 1408 | Loss: 0.11118607857300633\n",
            "Epoch: 94 | Iteration: 1472 | Loss: 0.5766523170092448\n",
            "Epoch: 94 | Iteration: 1536 | Loss: 0.050252201818805735\n",
            "Epoch: 94 | Iteration: 1600 | Loss: 1.0649792820545025\n",
            "Epoch: 94 | Iteration: 1664 | Loss: 0.10210748586111441\n",
            "Epoch: 94 | Iteration: 1728 | Loss: 0.16949932640881793\n",
            "Epoch: 94 | Iteration: 1792 | Loss: 0.1968031079790034\n",
            "Epoch: 94 | Iteration: 1856 | Loss: 0.18423797084314275\n",
            "Epoch: 94 | Iteration: 1920 | Loss: 0.9084659251698992\n",
            "Epoch: 94 | Iteration: 1984 | Loss: 0.2042615103092259\n",
            "Epoch: 94 | Iteration: 2048 | Loss: 0.3605097009639644\n",
            "Epoch: 94 | Iteration: 2112 | Loss: 0.03775637546748915\n",
            "Epoch: 94 | Iteration: 2176 | Loss: 0.37430472315018065\n",
            "Epoch: 94 | Iteration: 2240 | Loss: 0.10364198465294253\n",
            "Epoch: 94 | Iteration: 2304 | Loss: 0.1310966615572162\n",
            "Epoch: 94 | Iteration: 2368 | Loss: 0.20935255707154254\n",
            "Epoch: 94 | Iteration: 2432 | Loss: 0.013381927780266903\n",
            "Epoch: 94 | Iteration: 2496 | Loss: 0.1347076948522801\n",
            "Epoch: 94 | Iteration: 2560 | Loss: 0.428579880283268\n",
            "Epoch: 94 | Iteration: 2624 | Loss: 0.8750712401381805\n",
            "Epoch: 94 | Iteration: 2688 | Loss: 0.9118363079225592\n",
            "Epoch: 94 | Iteration: 2752 | Loss: 0.0706250112201885\n",
            "Epoch: 94 | Iteration: 2816 | Loss: 0.13274509430844117\n",
            "Epoch: 94 | Iteration: 2880 | Loss: 0.34038672774384277\n",
            "Epoch: 94 | Iteration: 2944 | Loss: 0.10242000556377061\n",
            "Epoch: 94 | Iteration: 3008 | Loss: 0.3223586883435663\n",
            "Epoch: 94 | Iteration: 3072 | Loss: 0.08819697600539654\n",
            "Epoch: 94 | Iteration: 3136 | Loss: 0.0015583388446340244\n",
            "Epoch: 94 | Iteration: 3200 | Loss: 0.10016766633440669\n",
            "Epoch: 94 | Iteration: 3264 | Loss: 0.07781233230913734\n",
            "Epoch: 94 | Iteration: 3328 | Loss: 0.07063530593164219\n",
            "Epoch: 94 | Iteration: 3392 | Loss: 0.05825485274297042\n",
            "Epoch: 94 | Iteration: 3456 | Loss: 0.24568744570978981\n",
            "Epoch: 94 | Iteration: 3520 | Loss: 0.16126965231455886\n",
            "Epoch: 94 | Iteration: 3584 | Loss: 0.07345927545543755\n",
            "Epoch: 94 | Iteration: 3648 | Loss: 0.5537542851852795\n",
            "Epoch: 94 | Iteration: 3712 | Loss: 0.112648703385966\n",
            "Epoch: 94 | Iteration: 3776 | Loss: 0.35697388924347434\n",
            "Epoch: 94 | Iteration: 3840 | Loss: 0.007389411707271654\n",
            "Epoch: 94 | Iteration: 3904 | Loss: 0.20965191862761845\n",
            "Epoch: 94 | Iteration: 3968 | Loss: 0.08007848444044396\n",
            "Epoch: 94 | Iteration: 4032 | Loss: 0.16087931471093223\n",
            "Epoch: 94 | Iteration: 4096 | Loss: 0.2993117674297838\n",
            "Epoch: 94 | Iteration: 4160 | Loss: 0.21751456142574677\n",
            "Epoch: 94 | Iteration: 4224 | Loss: 0.5289324703937056\n",
            "Epoch: 94 | Iteration: 4288 | Loss: 0.10469713010270354\n",
            "Epoch: 94 | Iteration: 4352 | Loss: 0.3146247031032031\n",
            "Epoch: 94 | Iteration: 4416 | Loss: 0.9566239596497148\n",
            "Epoch: 94 | Iteration: 4480 | Loss: 0.07414649887887337\n",
            "Epoch: 94 | Iteration: 4544 | Loss: 0.051406358782399866\n",
            "Epoch: 94 | Iteration: 4608 | Loss: 0.7478331971832111\n",
            "Epoch: 94 | Iteration: 4672 | Loss: 0.06356443191916647\n",
            "Epoch: 94 | Iteration: 4736 | Loss: 0.09841174909027284\n",
            "Epoch: 94 | Iteration: 4800 | Loss: 0.17071066594298906\n",
            "Epoch: 94 | Iteration: 4864 | Loss: 0.03165944000621958\n",
            "Epoch: 94 | Iteration: 4928 | Loss: 0.41871715358785205\n",
            "Epoch: 94 | Iteration: 4992 | Loss: 0.7723052941650883\n",
            "Epoch: 94 | Iteration: 5056 | Loss: 0.5470377481264684\n",
            "Epoch: 94 | Iteration: 5120 | Loss: 0.7975365847943788\n",
            "Epoch: 94 | Iteration: 5184 | Loss: 0.3066793446149154\n",
            "Epoch: 94 | Iteration: 5248 | Loss: 0.354632274636322\n",
            "Epoch: 94 | Iteration: 5312 | Loss: 0.3892834530540844\n",
            "Epoch: 94 | Iteration: 5376 | Loss: 0.11697755265041958\n",
            "Epoch: 94 | Iteration: 5440 | Loss: 0.12137878930560286\n",
            "Epoch: 94 | Iteration: 5504 | Loss: 0.3232530141532791\n",
            "Epoch: 94 | Iteration: 5568 | Loss: 0.17324044031126326\n",
            "Epoch: 94 | Iteration: 5632 | Loss: 0.17012443478652647\n",
            "Epoch: 94 | Iteration: 5696 | Loss: 0.9106781788196958\n",
            "Epoch: 94 | Iteration: 5760 | Loss: 1.2248509328297503\n",
            "Epoch: 94 | Iteration: 5824 | Loss: 0.1863333850187515\n",
            "Epoch: 94 | Iteration: 5888 | Loss: 1.0487608812854483\n",
            "Epoch: 94 | Iteration: 5952 | Loss: 0.05176184485152404\n",
            "Epoch: 94 | Iteration: 6016 | Loss: 0.07919311876510957\n",
            "Epoch: 94 | Iteration: 6080 | Loss: 0.5038540681484713\n",
            "Epoch: 94 | Iteration: 6144 | Loss: 0.4037946735757965\n",
            "Epoch: 94 | Iteration: 6208 | Loss: 0.49932843240791197\n",
            "Epoch: 94 | Iteration: 6272 | Loss: 0.03782986799180575\n",
            "Epoch: 94 | Iteration: 6336 | Loss: 0.193688393698894\n",
            "Epoch: 94 | Iteration: 6400 | Loss: 1.0571847231838445\n",
            "Epoch: 94 | Iteration: 6464 | Loss: 0.06676511637988006\n",
            "Epoch: 94 | Iteration: 6528 | Loss: 0.26919270872763723\n",
            "Epoch: 94 | Iteration: 6592 | Loss: 0.02524839511633881\n",
            "Epoch: 94 | Iteration: 6656 | Loss: 0.2787586698091835\n",
            "Epoch: 94 | Iteration: 6720 | Loss: 0.05700049238992474\n",
            "Epoch: 94 | Iteration: 6784 | Loss: 0.9508373853261898\n",
            "Epoch: 94 | Iteration: 6848 | Loss: 1.7804144336224974\n",
            "Epoch: 94 | Iteration: 6912 | Loss: 0.4651428171201019\n",
            "Epoch: 94 | Iteration: 6976 | Loss: 0.5934725837700687\n",
            "Epoch: 94 | Iteration: 7040 | Loss: 1.0916423060755012\n",
            "Epoch: 94 | Iteration: 7104 | Loss: 0.2385685282264683\n",
            "Epoch: 94 | Iteration: 7168 | Loss: 0.1323276407356692\n",
            "Epoch: 94 | Iteration: 7232 | Loss: 0.9045523156755709\n",
            "Epoch: 94 | Iteration: 7296 | Loss: 0.7751889272253963\n",
            "Epoch: 94 | Iteration: 7360 | Loss: 0.04763474725179205\n",
            "Epoch: 94 | Iteration: 7424 | Loss: 0.09389525908522398\n",
            "Epoch: 94 | Iteration: 7488 | Loss: 0.558907481638808\n",
            "Epoch: 94 | Iteration: 7552 | Loss: 0.7077260630652752\n",
            "Epoch: 94 | Iteration: 7616 | Loss: 0.19930034357498894\n",
            "Epoch: 94 | Iteration: 7680 | Loss: 0.593972235536239\n",
            "Epoch: 94 | Iteration: 7744 | Loss: 0.34785194142364956\n",
            "Epoch: 94 | Iteration: 7808 | Loss: 0.5372356858030731\n",
            "Epoch: 94 | Iteration: 7872 | Loss: 0.602624101351433\n",
            "Epoch: 94 | Iteration: 7936 | Loss: 0.3003044384021925\n",
            "Epoch: 94 | Iteration: 8000 | Loss: 0.11199046347112429\n",
            "Epoch: 94 | Iteration: 8064 | Loss: 0.33745264006991005\n",
            "Epoch: 94 | Iteration: 8128 | Loss: 0.10633729346526277\n",
            "Epoch: 94 | Iteration: 8192 | Loss: 1.9026669468010837\n",
            "Epoch: 94 | Iteration: 8256 | Loss: 0.15071268680272537\n",
            "Epoch: 94 | Iteration: 8320 | Loss: 0.35324049633136445\n",
            "Epoch: 94 | Iteration: 8384 | Loss: 0.3658753503899401\n",
            "Epoch: 94 | Iteration: 8448 | Loss: 0.6408396062746615\n",
            "Epoch: 94 | Iteration: 8512 | Loss: 0.06792819623008803\n",
            "Epoch: 94 | Iteration: 8576 | Loss: 0.17145885992169954\n",
            "Epoch: 94 | Iteration: 8640 | Loss: 0.2169572743217092\n",
            "Epoch: 94 | Iteration: 8704 | Loss: 1.1231840876923773\n",
            "Epoch: 94 | Iteration: 8768 | Loss: 0.5961920155692879\n",
            "Epoch: 94 | Iteration: 8832 | Loss: 1.1363259335280338\n",
            "Epoch: 94 | Iteration: 8896 | Loss: 0.5825238351087987\n",
            "Epoch: 94 | Iteration: 8960 | Loss: 0.22572162685372116\n",
            "Epoch: 94 | Iteration: 9024 | Loss: 0.19805813751753074\n",
            "Epoch: 94 | Iteration: 9088 | Loss: 0.25549983688240485\n",
            "Epoch: 94 | Iteration: 9152 | Loss: 0.09241536007675638\n",
            "Epoch: 94 | Iteration: 9216 | Loss: 0.2770273972975123\n",
            "Epoch: 94 | Iteration: 9280 | Loss: 0.8254949859914319\n",
            "Epoch: 94 | Iteration: 9344 | Loss: 0.2234861231695776\n",
            "Epoch: 94 | Iteration: 9408 | Loss: 0.6393398695649495\n",
            "Epoch: 94 | Iteration: 9472 | Loss: 0.5592176336948675\n",
            "Epoch: 94 | Iteration: 9536 | Loss: 0.08897159604661634\n",
            "Epoch: 94 | Iteration: 9600 | Loss: 0.1619903129765616\n",
            "Epoch: 94 | Iteration: 9664 | Loss: 0.003335240815848765\n",
            "Epoch: 94 | Iteration: 9728 | Loss: 0.4497541393317875\n",
            "Epoch: 94 | Iteration: 9792 | Loss: 0.09228037884817145\n",
            "Epoch: 94 | Iteration: 9856 | Loss: 0.025084340538193847\n",
            "Epoch: 94 | Iteration: 9920 | Loss: 0.3433515850587392\n",
            "Epoch: 94 | Iteration: 9984 | Loss: 0.15542888924555354\n",
            "Epoch: 94 | Iteration: 10048 | Loss: 0.3551946331201065\n",
            "Epoch: 94 | Iteration: 10112 | Loss: 0.5543262259999169\n",
            "Epoch: 94 | Iteration: 10176 | Loss: 0.6630310288067552\n",
            "Epoch: 94 | Iteration: 10240 | Loss: 0.5455743638304755\n",
            "Epoch: 94 | Iteration: 10304 | Loss: 0.027935295776444637\n",
            "Epoch: 94 | Iteration: 10368 | Loss: 0.1313493998387249\n",
            "Epoch: 94 | Iteration: 10432 | Loss: 0.04388619847509907\n",
            "Epoch: 94 | Iteration: 10496 | Loss: 0.027925718031343597\n",
            "Epoch: 94 | Iteration: 10560 | Loss: 0.03645284339157675\n",
            "Epoch: 94 | Iteration: 10624 | Loss: 0.1522891727071876\n",
            "Epoch: 94 | Iteration: 10688 | Loss: 0.10802565742372591\n",
            "Epoch: 94 | Iteration: 10752 | Loss: 0.3895289976433639\n",
            "Epoch: 94 | Iteration: 10816 | Loss: 0.19510642101944384\n",
            "Epoch: 94 | Iteration: 10880 | Loss: 0.16978540316861074\n",
            "Epoch: 94 | Iteration: 10944 | Loss: 1.1384394764372812\n",
            "Epoch: 94 | Iteration: 11008 | Loss: 0.23246209070535184\n",
            "Epoch: 94 | Iteration: 11072 | Loss: 0.33266562488774193\n",
            "Epoch: 94 | Iteration: 11136 | Loss: 0.08688694453179426\n",
            "Epoch: 94 | Iteration: 11200 | Loss: 0.2731022279186222\n",
            "Epoch: 94 | Iteration: 11264 | Loss: 0.030693334133631258\n",
            "Epoch: 94 | Iteration: 11328 | Loss: 0.09292074286136785\n",
            "Epoch: 94 | Iteration: 11392 | Loss: 0.02909305814311902\n",
            "Epoch: 94 | Iteration: 11456 | Loss: 0.09610360322833553\n",
            "Epoch: 94 | Iteration: 11520 | Loss: 0.5083656445076684\n",
            "Epoch: 94 | Iteration: 11584 | Loss: 0.5095188244089225\n",
            "Epoch: 94 | Iteration: 11648 | Loss: 0.7081925826713952\n",
            "Epoch: 94 | Iteration: 11712 | Loss: 0.7603168239824689\n",
            "Epoch: 94 | Iteration: 11776 | Loss: 0.19527280498486904\n",
            "Epoch: 94 | Iteration: 11840 | Loss: 0.4864313439145854\n",
            "Epoch: 94 | Iteration: 11904 | Loss: 0.5167513845568187\n",
            "Epoch: 94 | Iteration: 11968 | Loss: 0.1747937827268098\n",
            "Epoch: 94 | Iteration: 12032 | Loss: 0.09143426777900793\n",
            "Epoch: 94 | Iteration: 12096 | Loss: 0.03479103279892201\n",
            "Epoch: 94 | Iteration: 12160 | Loss: 0.12470920667085761\n",
            "Epoch: 94 | Iteration: 12224 | Loss: 0.13673249482217886\n",
            "Epoch: 94 | Iteration: 12288 | Loss: 0.24078758609685896\n",
            "Epoch: 94 | Iteration: 12352 | Loss: 0.17830194414328931\n",
            "Epoch: 94 | Iteration: 12416 | Loss: 0.09896396336954548\n",
            "Epoch: 94 | Iteration: 12480 | Loss: 0.0605674002351985\n",
            "Epoch: 94 | Iteration: 12544 | Loss: 0.7184706407554251\n",
            "Epoch: 94 | Iteration: 12608 | Loss: 0.16160963827991665\n",
            "Epoch: 94 | Iteration: 12672 | Loss: 0.6693471508896076\n",
            "Epoch: 94 | Iteration: 12736 | Loss: 0.04902018904018818\n",
            "Epoch: 94 | Iteration: 12800 | Loss: 0.6265547539161612\n",
            "Epoch: 94 | Iteration: 12864 | Loss: 0.1530272028701071\n",
            "Epoch: 94 | Iteration: 12928 | Loss: 0.2401935302845106\n",
            "Epoch: 94 | Iteration: 12992 | Loss: 0.12544930121553244\n",
            "Epoch: 94 | Iteration: 13056 | Loss: 0.10456095955764957\n",
            "Epoch: 94 | Iteration: 13120 | Loss: 0.06828879656077749\n",
            "Epoch: 94 | Iteration: 13184 | Loss: 0.49121125068416704\n",
            "Epoch: 94 | Iteration: 13248 | Loss: 0.06348532894560335\n",
            "Epoch: 94 | Iteration: 13312 | Loss: 0.016193580684230196\n",
            "Epoch: 94 | Iteration: 13376 | Loss: 0.5017978005484545\n",
            "Epoch: 94 | Iteration: 13440 | Loss: 0.06991776015744575\n",
            "Epoch: 94 | Iteration: 13504 | Loss: 0.23895540290307465\n",
            "Epoch: 94 | Iteration: 13568 | Loss: 0.09701906779385397\n",
            "Epoch: 94 | Iteration: 13632 | Loss: 0.37358831724424274\n",
            "Epoch: 94 | Iteration: 13696 | Loss: 0.507751752636978\n",
            "Epoch: 94 | Iteration: 13760 | Loss: 0.025270844576883694\n",
            "Epoch: 94 | Iteration: 13824 | Loss: 0.27386575163399357\n",
            "Epoch: 94 | Iteration: 13888 | Loss: 0.1379730496527635\n",
            "Epoch: 94 | Iteration: 13952 | Loss: 0.8497318290717762\n",
            "Epoch: 94 | Iteration: 14016 | Loss: 0.25360429852417676\n",
            "Epoch: 94 | Iteration: 14080 | Loss: 0.2583328617608986\n",
            "Epoch: 94 | Iteration: 14144 | Loss: 0.23831610736881884\n",
            "Epoch: 94 | Iteration: 14208 | Loss: 0.6052001286739005\n",
            "Epoch: 94 | Iteration: 14272 | Loss: 0.45616931797022187\n",
            "Epoch: 94 | Iteration: 14336 | Loss: 0.26946388964339796\n",
            "Epoch: 94 | Iteration: 14400 | Loss: 0.16222604714333286\n",
            "Epoch: 94 | Iteration: 14464 | Loss: 0.08707388877533603\n",
            "Epoch: 94 | Iteration: 14528 | Loss: 0.32959729949749783\n",
            "Epoch: 94 | Iteration: 14592 | Loss: 0.18998256126658924\n",
            "Epoch: 94 | Iteration: 14656 | Loss: 0.09428918973336711\n",
            "Epoch: 94 | Iteration: 14720 | Loss: 0.8005304662570564\n",
            "Epoch: 94 | Iteration: 14784 | Loss: 0.8089040801587477\n",
            "Epoch: 94 | Iteration: 14848 | Loss: 0.23658296093852577\n",
            "Epoch: 94 | Iteration: 14912 | Loss: 0.22041252322275146\n",
            "Epoch: 94 | Iteration: 14976 | Loss: 0.0436229640172599\n",
            "Epoch: 94 | Iteration: 15040 | Loss: 0.14714302730193735\n",
            "Epoch: 94 | Iteration: 15104 | Loss: 0.6840414043304429\n",
            "Epoch: 94 | Iteration: 15168 | Loss: 0.14063760962253288\n",
            "Epoch: 94 | Iteration: 15232 | Loss: 0.14430876264431747\n",
            "Epoch: 94 | Iteration: 15296 | Loss: 0.4928758715572599\n",
            "Epoch: 94 | Iteration: 15360 | Loss: 0.3905408088839243\n",
            "Epoch: 94 | Iteration: 15424 | Loss: 0.07518174869281902\n",
            "Epoch: 94 | Iteration: 15488 | Loss: 0.10356874597289115\n",
            "Epoch: 94 | Iteration: 15552 | Loss: 0.2838450153025764\n",
            "Epoch: 94 | Iteration: 15616 | Loss: 0.055830325773619245\n",
            "Epoch: 94 | Iteration: 15680 | Loss: 0.23031184586215667\n",
            "Epoch: 94 | Iteration: 15744 | Loss: 0.43910871451746813\n",
            "Epoch: 94 | Iteration: 15808 | Loss: 0.5349272646971779\n",
            "Epoch: 94 | Iteration: 15872 | Loss: 0.26036135228097024\n",
            "Epoch: 94 | Iteration: 15936 | Loss: 1.041760820826178\n",
            "Epoch: 94 | Iteration: 16000 | Loss: 0.5276867177793764\n",
            "Epoch: 94 | Iteration: 16064 | Loss: 0.06315771089948499\n",
            "Epoch: 94 | Iteration: 16128 | Loss: 0.34496856750550253\n",
            "Epoch: 94 | Iteration: 16192 | Loss: 0.012298762577310347\n",
            "Epoch: 94 | Iteration: 16256 | Loss: 0.23763582456606921\n",
            "Epoch: 94 | Iteration: 16320 | Loss: 0.17691064586862765\n",
            "Epoch: 94 | Iteration: 16384 | Loss: 0.4307892857254509\n",
            "Epoch: 94 | Iteration: 16448 | Loss: 0.17801518536197564\n",
            "Epoch: 94 | Iteration: 16512 | Loss: 0.7852495916514305\n",
            "Epoch: 94 | Iteration: 16576 | Loss: 0.061308443062603954\n",
            "Epoch: 94 | Iteration: 16640 | Loss: 0.5389653561346374\n",
            "Epoch: 94 | Iteration: 16704 | Loss: 0.32267544105361723\n",
            "Epoch: 94 | Iteration: 16768 | Loss: 0.38349376565348325\n",
            "Epoch: 94 | Iteration: 16832 | Loss: 0.4561780582705277\n",
            "Epoch: 94 | Iteration: 16896 | Loss: 0.3813537650488519\n",
            "Epoch: 94 | Iteration: 16960 | Loss: 0.3184135356972523\n",
            "Epoch: 94 | Iteration: 17024 | Loss: 0.31127571001773796\n",
            "Epoch: 94 | Iteration: 17088 | Loss: 0.5844289469344798\n",
            "Epoch: 94 | Iteration: 17152 | Loss: 0.4344712652393077\n",
            "Epoch: 94 | Iteration: 17216 | Loss: 0.525312472749973\n",
            "Epoch: 94 | Iteration: 17280 | Loss: 0.013848331176170755\n",
            "Epoch: 94 | Iteration: 17344 | Loss: 0.4408495378175586\n",
            "Epoch: 94 | Iteration: 17408 | Loss: 0.13663608848590805\n",
            "Epoch: 94 | Iteration: 17472 | Loss: 0.20862975000840378\n",
            "Epoch: 94 | Iteration: 17536 | Loss: 0.6687233978162044\n",
            "Epoch: 94 | Iteration: 17600 | Loss: 0.20781467290056438\n",
            "Epoch: 94 | Iteration: 17664 | Loss: 0.398767030189041\n",
            "Epoch: 94 | Iteration: 17728 | Loss: 1.55071402867633\n",
            "Epoch: 94 | Iteration: 17792 | Loss: 0.2223427662245025\n",
            "Epoch: 94 | Iteration: 17856 | Loss: 0.42035830556516984\n",
            "Epoch: 94 | Iteration: 17920 | Loss: 0.0455743028148171\n",
            "Epoch: 94 | Iteration: 17984 | Loss: 0.08668019379986244\n",
            "Epoch: 94 | Iteration: 18048 | Loss: 0.49267904597258094\n",
            "Epoch: 94 | Iteration: 18112 | Loss: 0.2322623333425598\n",
            "Epoch: 94 | Iteration: 18176 | Loss: 0.10812594536351508\n",
            "Epoch: 94 | Iteration: 18240 | Loss: 0.6196704590949614\n",
            "Epoch: 94 | Iteration: 18304 | Loss: 0.471635519922755\n",
            "Epoch: 94 | Iteration: 18368 | Loss: 0.37121758352214607\n",
            "Epoch: 94 | Iteration: 18432 | Loss: 0.724994480977334\n",
            "Epoch: 94 | Iteration: 18496 | Loss: 0.16802857152463693\n",
            "Epoch: 94 | Iteration: 18560 | Loss: 0.21667265588551693\n",
            "Epoch: 94 | Iteration: 18624 | Loss: 0.07486624093853263\n",
            "Epoch: 94 | Iteration: 18688 | Loss: 0.3689146582311922\n",
            "Epoch: 94 | Iteration: 18752 | Loss: 0.08241345124494455\n",
            "Epoch: 94 | Iteration: 18816 | Loss: 0.08667350359499695\n",
            "Epoch: 94 | Iteration: 18880 | Loss: 0.22372035852205524\n",
            "Epoch: 94 | Iteration: 18944 | Loss: 0.21016031213358982\n",
            "Epoch: 94 | Iteration: 19008 | Loss: 0.15034800949746546\n",
            "Epoch: 94 | Iteration: 19072 | Loss: 1.3531197221635958\n",
            "Epoch: 94 | Iteration: 19136 | Loss: 0.16250928780694196\n",
            "Epoch: 94 | Iteration: 19200 | Loss: 0.19070213424209434\n",
            "Epoch: 94 | Iteration: 19264 | Loss: 0.513999496562659\n",
            "Epoch: 94 | Iteration: 19328 | Loss: 1.6278474149214737\n",
            "Epoch: 94 | Iteration: 19392 | Loss: 0.3377785461204009\n",
            "Epoch: 94 | Iteration: 19456 | Loss: 0.25690782601775347\n",
            "Epoch: 94 | Iteration: 19520 | Loss: 0.056458621594097536\n",
            "Epoch: 94 | Iteration: 19584 | Loss: 0.1262524716857365\n",
            "Epoch: 94 | Iteration: 19648 | Loss: 0.11571910694486962\n",
            "Epoch: 94 | Iteration: 19712 | Loss: 0.014917212285376177\n",
            "Epoch: 94 | Iteration: 19776 | Loss: 0.8810538153884806\n",
            "Epoch: 94 | Iteration: 19840 | Loss: 0.41463336392261035\n",
            "Epoch: 94 | Iteration: 19904 | Loss: 0.38624724757822926\n",
            "Epoch: 94 | Iteration: 19968 | Loss: 0.31068163551718625\n",
            "Epoch: 94 | Iteration: 20032 | Loss: 0.276722649639553\n",
            "Epoch: 94 | Iteration: 20096 | Loss: 0.09680147948066276\n",
            "Epoch: 94 | Iteration: 20160 | Loss: 0.9213415224427763\n",
            "Epoch: 94 | Iteration: 20224 | Loss: 0.4554590059431716\n",
            "Epoch: 94 | Iteration: 20288 | Loss: 0.8261579781293127\n",
            "Epoch: 94 | Iteration: 20352 | Loss: 0.007242996287498141\n",
            "Epoch: 94 | Iteration: 20416 | Loss: 0.016045333508913753\n",
            "Epoch: 94 | Iteration: 20480 | Loss: 0.1064138782454225\n",
            "Epoch: 94 | Iteration: 20544 | Loss: 0.22615807827357504\n",
            "Epoch: 94 | Iteration: 20608 | Loss: 0.13250191875468392\n",
            "Epoch: 94 | Iteration: 20672 | Loss: 1.844288740055981\n",
            "Epoch: 94 | Iteration: 20736 | Loss: 1.1782312702163384\n",
            "Epoch: 94 | Iteration: 20800 | Loss: 0.4085775732380117\n",
            "Epoch: 94 | Iteration: 20864 | Loss: 0.37299662529013533\n",
            "Epoch: 94 | Iteration: 20928 | Loss: 0.31191903119934317\n",
            "Epoch: 94 | Iteration: 20992 | Loss: 0.4125063231786859\n",
            "Epoch: 94 | Iteration: 21056 | Loss: 0.28955751497708426\n",
            "Epoch: 94 | Iteration: 21120 | Loss: 0.3634744602250807\n",
            "Epoch: 94 | Iteration: 21184 | Loss: 0.22327933801955244\n",
            "Epoch: 94 | Iteration: 21248 | Loss: 0.21179799225419527\n",
            "Epoch: 94 | Iteration: 21312 | Loss: 0.4413964117707395\n",
            "Epoch: 94 | Iteration: 21376 | Loss: 0.18590365550845878\n",
            "Epoch: 94 | Iteration: 21440 | Loss: 0.6054091489011519\n",
            "Epoch: 94 | Iteration: 21504 | Loss: 0.19953198101048458\n",
            "Epoch: 94 | Iteration: 21568 | Loss: 1.4322412853742033\n",
            "Epoch: 94 | Iteration: 21632 | Loss: 0.24668901271952665\n",
            "Epoch: 94 | Iteration: 21696 | Loss: 0.1137634114524938\n",
            "Epoch: 94 | Iteration: 21760 | Loss: 0.006257093094490556\n",
            "Epoch: 94 | Iteration: 21824 | Loss: 0.0038514379930773717\n",
            "Epoch: 94 | Iteration: 21888 | Loss: 0.562183734895368\n",
            "Epoch: 94 | Iteration: 21952 | Loss: 0.38924061223449324\n",
            "Epoch: 94 | Iteration: 22016 | Loss: 0.07801297744218015\n",
            "Epoch: 94 | Iteration: 22080 | Loss: 0.4373320991589345\n",
            "Epoch: 94 | Iteration: 22144 | Loss: 0.3756740370165027\n",
            "Epoch: 94 | Iteration: 22208 | Loss: 0.2489997165020349\n",
            "Epoch: 94 | Iteration: 22272 | Loss: 0.7455670273562781\n",
            "Epoch: 94 | Iteration: 22336 | Loss: 0.04131739115311262\n",
            "Epoch: 94 | Iteration: 22400 | Loss: 0.20588280470192652\n",
            "Epoch: 94 | Iteration: 22464 | Loss: 0.6362238681816252\n",
            "Epoch: 94 | Iteration: 22528 | Loss: 0.978867592515802\n",
            "Epoch: 94 | Iteration: 22592 | Loss: 0.5059047594278128\n",
            "Epoch: 94 | Iteration: 22656 | Loss: 0.12971528058663326\n",
            "Epoch: 94 | Iteration: 22720 | Loss: 0.5910340011769029\n",
            "Epoch: 94 | Iteration: 22784 | Loss: 0.16820056653845988\n",
            "Epoch: 94 | Iteration: 22848 | Loss: 0.28206565260866034\n",
            "Epoch: 94 | Iteration: 22912 | Loss: 0.0018138958931261224\n",
            "Epoch: 94 | Iteration: 22976 | Loss: 0.2887804825343538\n",
            "Epoch: 94 | Iteration: 23040 | Loss: 0.24570544786212162\n",
            "Epoch: 94 | Iteration: 23104 | Loss: 0.12730574653983195\n",
            "Epoch: 94 | Iteration: 23168 | Loss: 0.5333734791931882\n",
            "Epoch: 94 | Iteration: 23232 | Loss: 0.026081202824677442\n",
            "Epoch: 94 | Iteration: 23296 | Loss: 0.12051155799249445\n",
            "Epoch: 94 | Iteration: 23360 | Loss: 0.3257522332671593\n",
            "Epoch: 94 | Iteration: 23424 | Loss: 0.09789251544561298\n",
            "Epoch: 94 | Iteration: 23488 | Loss: 0.09057006611330908\n",
            "Epoch: 94 | Iteration: 23552 | Loss: 0.6457288913736745\n",
            "Epoch: 94 | Iteration: 23616 | Loss: 0.7260926601183324\n",
            "Epoch: 94 | Iteration: 23680 | Loss: 0.8704833438992918\n",
            "Epoch: 94 | Iteration: 23744 | Loss: 0.013182563307266065\n",
            "Epoch: 94 | Iteration: 23808 | Loss: 0.3967850023311912\n",
            "Epoch: 94 | Iteration: 23872 | Loss: 0.625494768192266\n",
            "Epoch: 94 | Iteration: 23936 | Loss: 1.0475325215530968\n",
            "Epoch: 94 | Iteration: 24000 | Loss: 0.1694212941358041\n",
            "Epoch: 94 | Iteration: 24064 | Loss: 0.08019885974178032\n",
            "Epoch: 94 | Iteration: 24128 | Loss: 0.15589643823791977\n",
            "Epoch: 94 | Iteration: 24192 | Loss: 0.29869356226521415\n",
            "Epoch: 94 | Iteration: 24256 | Loss: 0.7290681588127056\n",
            "Epoch: 94 | Iteration: 24320 | Loss: 0.3985985531137457\n",
            "Epoch: 94 | Iteration: 24384 | Loss: 0.010228871612258002\n",
            "Epoch: 94 | Iteration: 24448 | Loss: 0.2380823998707144\n",
            "Epoch: 94 | Iteration: 24512 | Loss: 0.20917446421689523\n",
            "Epoch: 94 | Iteration: 24576 | Loss: 0.39927729222047736\n",
            "Epoch: 94 | Iteration: 24640 | Loss: 0.19854229370875284\n",
            "Epoch: 94 | Iteration: 24704 | Loss: 0.12367134967844143\n",
            "Epoch: 94 | Iteration: 24768 | Loss: 0.627882537916198\n",
            "Epoch: 94 | Iteration: 24832 | Loss: 0.057815832379625245\n",
            "Epoch: 94 | Iteration: 24896 | Loss: 0.28959358825536585\n",
            "Epoch: 94 | Iteration: 24960 | Loss: 0.011764800169505641\n",
            "Epoch: 94 | Iteration: 25024 | Loss: 0.04429126356125765\n",
            "Epoch: 94 | Iteration: 25088 | Loss: 0.3326360621186959\n",
            "Epoch: 94 | Iteration: 25152 | Loss: 0.7494379084395455\n",
            "Epoch: 94 | Iteration: 25216 | Loss: 0.20528722255792703\n",
            "Epoch: 94 | Iteration: 25280 | Loss: 0.15527762821634017\n",
            "Epoch: 94 | Iteration: 25344 | Loss: 0.025306824508223365\n",
            "Epoch: 94 | Iteration: 25408 | Loss: 0.03280353779845696\n",
            "Epoch: 94 | Iteration: 25472 | Loss: 0.5907318419808368\n",
            "Epoch: 94 | Iteration: 25536 | Loss: 1.2040281224502403\n",
            "Epoch: 94 | Iteration: 25600 | Loss: 0.0781429635295186\n",
            "Epoch: 94 | Iteration: 25664 | Loss: 0.6434597456407336\n",
            "Epoch: 94 | Iteration: 25728 | Loss: 0.5968049951081222\n",
            "Epoch: 94 | Iteration: 25792 | Loss: 0.910178969821523\n",
            "Epoch: 94 | Iteration: 25856 | Loss: 0.0831303576354868\n",
            "Epoch: 94 | Iteration: 25920 | Loss: 0.11954427677595786\n",
            "Epoch: 94 | Iteration: 25984 | Loss: 0.04916827554839646\n",
            "Epoch: 94 | Iteration: 26048 | Loss: 0.3198510469974234\n",
            "Epoch: 94 | Iteration: 26112 | Loss: 0.27249136803363233\n",
            "Epoch: 94 | Iteration: 26176 | Loss: 0.20038892321623153\n",
            "Epoch: 94 | Iteration: 26240 | Loss: 0.2157184763529044\n",
            "Epoch: 94 | Iteration: 26304 | Loss: 0.5561122593657435\n",
            "Epoch: 94 | Iteration: 26368 | Loss: 0.929455436672933\n",
            "Epoch: 94 | Iteration: 26432 | Loss: 0.968961682912686\n",
            "Epoch: 94 | Iteration: 26496 | Loss: 1.4844501680831994\n",
            "Epoch: 94 | Iteration: 26560 | Loss: 0.9282672739584339\n",
            "Epoch: 94 | Iteration: 26624 | Loss: 0.9237134525983607\n",
            "Epoch: 94 | Iteration: 26688 | Loss: 0.8371044914404995\n",
            "Epoch: 94 | Iteration: 26752 | Loss: 0.9639651672674167\n",
            "Epoch: 94 | Iteration: 26816 | Loss: 0.2757956165764938\n",
            "Epoch: 94 | Iteration: 26880 | Loss: 1.0997262058050195\n",
            "Epoch: 94 | Iteration: 26944 | Loss: 0.016330161762864365\n",
            "Epoch: 94 | Iteration: 27008 | Loss: 0.12683962888851913\n",
            "Epoch: 94 | Iteration: 27072 | Loss: 0.07731680856542697\n",
            "Epoch: 94 | Iteration: 27136 | Loss: 1.6551866500372756\n",
            "Epoch: 94 | Iteration: 27200 | Loss: 0.9415605847466599\n",
            "Epoch: 94 | Iteration: 27264 | Loss: 0.11888726606821946\n",
            "Epoch: 94 | Iteration: 27328 | Loss: 0.021313578593421784\n",
            "Epoch: 94 | Iteration: 27392 | Loss: 0.15313953922645993\n",
            "Epoch: 94 | Iteration: 27456 | Loss: 0.5765849035277977\n",
            "Epoch: 94 | Iteration: 27520 | Loss: 0.6431928523422202\n",
            "Epoch: 94 | Iteration: 27584 | Loss: 0.7555337611186734\n",
            "Epoch: 94 | Iteration: 27648 | Loss: 0.5951368206277085\n",
            "Epoch: 94 | Iteration: 27712 | Loss: 0.24022916556093135\n",
            "Epoch: 94 | Iteration: 27776 | Loss: 0.31891055434327253\n",
            "Epoch: 94 | Iteration: 27840 | Loss: 0.9275576033266351\n",
            "Epoch: 94 | Iteration: 27904 | Loss: 0.09134934916893103\n",
            "Epoch: 94 | Iteration: 27968 | Loss: 0.02684575478983564\n",
            "Epoch: 94 | Iteration: 28032 | Loss: 0.2249516346856963\n",
            "Epoch: 94 | Iteration: 28096 | Loss: 0.14431317767440546\n",
            "Epoch: 94 | Iteration: 28160 | Loss: 0.635589490620939\n",
            "Epoch: 94 | Iteration: 28224 | Loss: 0.07503207398553366\n",
            "Epoch: 94 | Iteration: 28288 | Loss: 0.012838350308279026\n",
            "Epoch: 94 | Iteration: 28352 | Loss: 0.7592622577264492\n",
            "Epoch: 94 | Iteration: 28416 | Loss: 0.10656471644343074\n",
            "Epoch: 94 | Iteration: 28480 | Loss: 0.09114711229807634\n",
            "Epoch: 94 | Iteration: 28544 | Loss: 0.12732244307608953\n",
            "Epoch: 94 | Iteration: 28608 | Loss: 0.7065479443326683\n",
            "Epoch: 94 | Iteration: 28672 | Loss: 0.30512579360177505\n",
            "Epoch: 94 | Iteration: 28736 | Loss: 0.12033281629469102\n",
            "Epoch: 94 | Iteration: 28800 | Loss: 0.20380219216486203\n",
            "Epoch: 94 | Iteration: 28864 | Loss: 0.10872617566766705\n",
            "Epoch: 94 | Iteration: 28928 | Loss: 0.5074626307822031\n",
            "Epoch: 94 | Iteration: 28992 | Loss: 0.22165329046651117\n",
            "Epoch: 94 | Iteration: 29056 | Loss: 0.17778715077277393\n",
            "Epoch: 94 | Iteration: 29120 | Loss: 0.3231601075568173\n",
            "Epoch: 94 | Iteration: 29184 | Loss: 0.6487345539289491\n",
            "Epoch: 94 | Iteration: 29248 | Loss: 0.38703378226238355\n",
            "Epoch: 94 | Iteration: 29312 | Loss: 0.12458341709950937\n",
            "Epoch: 94 | Iteration: 29376 | Loss: 0.18618596791063702\n",
            "Epoch: 94 | Iteration: 29440 | Loss: 0.48233141456119133\n",
            "Epoch: 94 | Iteration: 29504 | Loss: 0.2122548979725088\n",
            "Epoch: 94 | Iteration: 29568 | Loss: 0.23344479310495808\n",
            "Epoch: 94 | Iteration: 29632 | Loss: 0.14922198514799276\n",
            "Epoch: 94 | Iteration: 29696 | Loss: 0.5650099651657535\n",
            "Epoch: 94 | Iteration: 29760 | Loss: 0.04999784505076599\n",
            "Epoch: 94 | Iteration: 29824 | Loss: 0.2829355874429864\n",
            "Epoch: 94 | Iteration: 29888 | Loss: 0.5104162342363355\n",
            "Epoch: 94 | Iteration: 29952 | Loss: 0.06838246821965674\n",
            "Epoch: 94 | Iteration: 30016 | Loss: 0.6985693227434647\n",
            "Epoch: 94 | Iteration: 30080 | Loss: 0.24203775197355243\n",
            "Epoch: 94 | Iteration: 30144 | Loss: 0.3032101375876052\n",
            "Epoch: 94 | Iteration: 30208 | Loss: 0.11500730950472102\n",
            "Epoch: 94 | Iteration: 30272 | Loss: 0.05901836622792056\n",
            "Epoch: 94 | Iteration: 30336 | Loss: 0.24153832450892043\n",
            "Epoch: 94 | Iteration: 30400 | Loss: 0.05504893263112805\n",
            "Epoch: 94 | Iteration: 30464 | Loss: 0.3155128912832576\n",
            "Epoch: 94 | Iteration: 30528 | Loss: 0.15061286774927515\n",
            "Epoch: 94 | Iteration: 30592 | Loss: 0.10533908248886323\n",
            "Epoch: 94 | Iteration: 30656 | Loss: 0.6077681189802538\n",
            "Epoch: 94 | Iteration: 30720 | Loss: 0.2634191466724483\n",
            "Epoch: 94 | Iteration: 30784 | Loss: 0.1976465936325307\n",
            "Epoch: 94 | Iteration: 30848 | Loss: 0.37236328442684713\n",
            "Epoch: 94 | Iteration: 30912 | Loss: 0.8029455252061932\n",
            "Epoch: 94 | Iteration: 30976 | Loss: 0.18191222608222302\n",
            "Epoch: 94 | Iteration: 31040 | Loss: 0.5009252249037781\n",
            "Epoch: 94 | Iteration: 31104 | Loss: 0.47757025644200823\n",
            "Epoch: 94 | Iteration: 31168 | Loss: 1.733538537899924\n",
            "Epoch: 94 | Iteration: 31232 | Loss: 0.19030865500731564\n",
            "Epoch: 94 | Iteration: 31296 | Loss: 0.8751895730514063\n",
            "Epoch: 94 | Iteration: 31360 | Loss: 0.29550217914151367\n",
            "Epoch: 94 | Iteration: 31424 | Loss: 0.3278904547880229\n",
            "Epoch: 94 | Iteration: 31488 | Loss: 0.0844204088714558\n",
            "Epoch: 94 | Iteration: 31552 | Loss: 1.1259325311232073\n",
            "Epoch: 94 | Iteration: 31616 | Loss: 0.3661429969766055\n",
            "Epoch: 94 | Iteration: 31680 | Loss: 1.40591130556684\n",
            "Epoch: 94 | Iteration: 31744 | Loss: 0.7690092476578717\n",
            "Epoch: 94 | Iteration: 31808 | Loss: 0.22067145203848973\n",
            "Epoch: 94 | Iteration: 31872 | Loss: 0.19295403765050673\n",
            "Epoch: 94 | Iteration: 31936 | Loss: 0.26274589784000774\n",
            "Epoch: 94 | Iteration: 32000 | Loss: 0.3056820725532391\n",
            "Epoch: 94 | Iteration: 32064 | Loss: 0.32110086619995065\n",
            "Epoch: 94 | Iteration: 32128 | Loss: 0.16960466051074702\n",
            "Epoch: 94 | Iteration: 32192 | Loss: 0.48396030784847377\n",
            "Epoch: 94 | Iteration: 32256 | Loss: 0.2281310019934915\n",
            "Epoch: 94 | Iteration: 32320 | Loss: 0.7817143685958633\n",
            "Epoch: 94 | Iteration: 32384 | Loss: 0.3397318410033795\n",
            "Epoch: 94 | Iteration: 32448 | Loss: 1.0568484392638782\n",
            "Epoch: 94 | Iteration: 32512 | Loss: 0.3165418455921317\n",
            "Epoch: 94 | Iteration: 32576 | Loss: 0.0007641507668441276\n",
            "Epoch: 94 | Iteration: 32640 | Loss: 0.3538899214875456\n",
            "Epoch: 94 | Iteration: 32704 | Loss: 0.31111907805657346\n",
            "Epoch: 94 | Iteration: 32768 | Loss: 0.9517943576874266\n",
            "Epoch: 94 | Iteration: 32832 | Loss: 0.7083017735519652\n",
            "Epoch: 94 | Iteration: 32896 | Loss: 0.7115831310931672\n",
            "Epoch: 94 | Iteration: 32960 | Loss: 0.015871624100369575\n",
            "Epoch: 94 | Iteration: 33024 | Loss: 0.30716279759166604\n",
            "Epoch: 94 | Iteration: 33088 | Loss: 0.03572188074469268\n",
            "Epoch: 94 | Iteration: 33152 | Loss: 0.08672521544709856\n",
            "Epoch: 94 | Iteration: 33216 | Loss: 0.23896072780816388\n",
            "Epoch: 94 | Iteration: 33280 | Loss: 0.29042066270652445\n",
            "Epoch: 94 | Iteration: 33344 | Loss: 0.373640668273629\n",
            "Epoch: 94 | Iteration: 33408 | Loss: 0.6538570949169884\n",
            "Epoch: 94 | Iteration: 33472 | Loss: 0.5681133649685066\n",
            "Epoch: 94 | Iteration: 33536 | Loss: 0.4538583599759206\n",
            "Epoch: 94 | Iteration: 33600 | Loss: 0.07461792628940131\n",
            "Epoch: 94 | Iteration: 33664 | Loss: 0.15316779691146173\n",
            "Epoch: 94 | Iteration: 33728 | Loss: 0.526632498791781\n",
            "Epoch: 94 | Iteration: 33792 | Loss: 0.12462608925312335\n",
            "Epoch: 94 | Iteration: 33856 | Loss: 0.011913388208462914\n",
            "Epoch: 94 | Iteration: 33920 | Loss: 0.013690531631313077\n",
            "Epoch: 94 | Iteration: 33984 | Loss: 0.5393899007954301\n",
            "Epoch: 94 | Iteration: 34048 | Loss: 0.5195361305581213\n",
            "Epoch: 94 | Iteration: 34112 | Loss: 0.013073214790017374\n",
            "Epoch: 94 | Iteration: 34176 | Loss: 0.06388451796664518\n",
            "Epoch: 94 | Iteration: 34240 | Loss: 0.003163693738711392\n",
            "Epoch: 94 | Iteration: 34304 | Loss: 0.10934035704862224\n",
            "Epoch: 94 | Iteration: 34368 | Loss: 1.3705298570131597\n",
            "Epoch: 94 | Iteration: 34432 | Loss: 0.1668002537213526\n",
            "Epoch: 94 | Iteration: 34496 | Loss: 0.18005365953598015\n",
            "Epoch: 94 | Iteration: 34560 | Loss: 0.13580591472483114\n",
            "Epoch: 94 | Iteration: 34624 | Loss: 0.6408104463933499\n",
            "Epoch: 94 | Iteration: 34688 | Loss: 0.833819007336752\n",
            "Epoch: 94 | Iteration: 34752 | Loss: 0.39598311936428326\n",
            "Epoch: 94 | Iteration: 34816 | Loss: 0.8189225636240962\n",
            "Epoch: 94 | Iteration: 34880 | Loss: 0.49662882803022285\n",
            "Epoch: 94 | Iteration: 34944 | Loss: 0.045796614702769196\n",
            "Epoch: 94 | Iteration: 35008 | Loss: 1.3664917066444238\n",
            "Epoch: 94 | Iteration: 35072 | Loss: 0.035026483675142654\n",
            "Epoch: 94 | Iteration: 35136 | Loss: 1.0199387558218924\n",
            "Epoch: 94 | Iteration: 35200 | Loss: 1.2079411381764804\n",
            "Epoch: 94 | Iteration: 35264 | Loss: 0.1232429316818973\n",
            "Epoch: 94 | Iteration: 35328 | Loss: 0.16673710436447975\n",
            "Epoch: 94 | Iteration: 35392 | Loss: 0.23520898507747356\n",
            "Epoch: 94 | Iteration: 35456 | Loss: 1.170678127470857\n",
            "Epoch: 94 | Iteration: 35520 | Loss: 0.14576433163170477\n",
            "Epoch: 94 | Iteration: 35584 | Loss: 0.9973913163007868\n",
            "Epoch: 94 | Iteration: 35648 | Loss: 0.26683273953522957\n",
            "Epoch: 94 | Iteration: 35712 | Loss: 0.10748067214375766\n",
            "Epoch: 94 | Iteration: 35776 | Loss: 0.046558210111065076\n",
            "Epoch: 94 | Iteration: 35840 | Loss: 0.2528744210251543\n",
            "Epoch: 94 | Iteration: 35904 | Loss: 0.31079638586974356\n",
            "Epoch: 94 | Iteration: 35968 | Loss: 0.07074682087481343\n",
            "Epoch: 94 | Iteration: 36032 | Loss: 0.9864049353043275\n",
            "Epoch: 94 | Iteration: 36096 | Loss: 1.33979094278755\n",
            "Epoch: 94 | Iteration: 36160 | Loss: 0.4442987933032583\n",
            "Epoch: 94 | Iteration: 36224 | Loss: 0.11542452910961715\n",
            "Epoch: 94 | Iteration: 36288 | Loss: 0.022238532895192085\n",
            "Epoch: 94 | Iteration: 36352 | Loss: 0.20067526630360114\n",
            "Epoch: 94 | Iteration: 36416 | Loss: 0.770779226690965\n",
            "Epoch: 94 | Iteration: 36480 | Loss: 0.02182407965542147\n",
            "Epoch: 94 | Iteration: 36544 | Loss: 0.25483700607802473\n",
            "Epoch: 94 | Iteration: 36608 | Loss: 0.06686597515034376\n",
            "Epoch: 94 | Iteration: 36672 | Loss: 0.2314989737518664\n",
            "Epoch: 94 | Iteration: 36736 | Loss: 0.9648876568870857\n",
            "Epoch: 94 | Iteration: 36800 | Loss: 0.4076695265025091\n",
            "Epoch: 94 | Iteration: 36864 | Loss: 0.1594368933089384\n",
            "Epoch: 94 | Iteration: 36928 | Loss: 0.3472712861493128\n",
            "Epoch: 94 | Iteration: 36992 | Loss: 1.0906081734610744\n",
            "Epoch: 94 | Iteration: 37056 | Loss: 0.32329759835137273\n",
            "Epoch: 94 | Iteration: 37120 | Loss: 0.4984809230195323\n",
            "Epoch: 94 | Iteration: 37184 | Loss: 0.11277480576606733\n",
            "Epoch: 94 | Iteration: 37248 | Loss: 0.39861904308668894\n",
            "Epoch: 94 | Iteration: 37312 | Loss: 1.0428012372509918\n",
            "Epoch: 94 | Iteration: 37376 | Loss: 0.973627196589926\n",
            "Epoch: 94 | Iteration: 37440 | Loss: 0.5134895049827056\n",
            "Epoch: 94 | Iteration: 37504 | Loss: 0.22933101668598777\n",
            "Epoch: 94 | Iteration: 37568 | Loss: 0.058277788420467116\n",
            "Epoch: 94 | Iteration: 37632 | Loss: 0.2147824290949466\n",
            "Epoch: 94 | Iteration: 37696 | Loss: 0.6856541997425428\n",
            "Epoch: 94 | Iteration: 37760 | Loss: 0.7677185043062656\n",
            "Epoch: 94 | Iteration: 37824 | Loss: 1.0139746645560523\n",
            "Epoch: 94 | Iteration: 37888 | Loss: 0.06777983471801322\n",
            "Epoch: 94 | Iteration: 37952 | Loss: 0.0488682660269993\n",
            "Epoch: 94 | Iteration: 38016 | Loss: 0.16411345871009608\n",
            "Epoch: 94 | Iteration: 38080 | Loss: 0.0723454301820766\n",
            "Epoch: 94 | Iteration: 38144 | Loss: 0.16693422556532136\n",
            "Epoch: 94 | Iteration: 38208 | Loss: 0.06807622315549279\n",
            "Epoch: 94 | Iteration: 38272 | Loss: 0.5380826340657384\n",
            "Epoch: 94 | Iteration: 38336 | Loss: 0.7265868428405664\n",
            "Epoch: 94 | Iteration: 38400 | Loss: 0.15327439818960376\n",
            "Epoch: 94 | Iteration: 38464 | Loss: 0.6337404330046729\n",
            "Epoch: 94 | Iteration: 38528 | Loss: 0.5607390221858841\n",
            "Epoch: 94 | Iteration: 38592 | Loss: 0.31224731864246685\n",
            "Epoch: 94 | Iteration: 38656 | Loss: 0.5874609759982408\n",
            "Epoch: 94 | Iteration: 38720 | Loss: 0.22052023199048335\n",
            "Epoch: 94 | Iteration: 38784 | Loss: 0.00795127435815526\n",
            "Epoch: 94 | Iteration: 38848 | Loss: 0.08478482438024912\n",
            "Epoch: 94 | Iteration: 38912 | Loss: 0.27057942456497064\n",
            "Epoch: 94 | Iteration: 38976 | Loss: 0.21684020702957524\n",
            "Epoch: 94 | Iteration: 39040 | Loss: 0.007145912483367727\n",
            "Epoch: 94 | Iteration: 39104 | Loss: 0.10328910659573226\n",
            "Epoch: 94 | Iteration: 39168 | Loss: 0.7532528636528191\n",
            "Epoch: 94 | Iteration: 39232 | Loss: 0.4018468424952593\n",
            "Epoch: 94 | Iteration: 39296 | Loss: 0.9672383282734044\n",
            "Epoch: 94 | Iteration: 39360 | Loss: 1.078109682349318\n",
            "Epoch: 94 | Iteration: 39424 | Loss: 0.19352308126351958\n",
            "Epoch: 94 | Iteration: 39488 | Loss: 0.05447983791522554\n",
            "Epoch: 94 | Iteration: 39552 | Loss: 0.4412051716434433\n",
            "Epoch: 94 | Iteration: 39616 | Loss: 0.4894890592589709\n",
            "Epoch: 94 | Iteration: 39680 | Loss: 0.39239510061991156\n",
            "Epoch: 94 | Iteration: 39744 | Loss: 0.6860860536852913\n",
            "Epoch: 94 | Iteration: 39808 | Loss: 0.7345068792102458\n",
            "Epoch: 94 | Iteration: 39872 | Loss: 0.1571765920899938\n",
            "Epoch: 94 | Iteration: 39936 | Loss: 0.06550208592390044\n",
            "Epoch: 94 | Iteration: 40000 | Loss: 0.24823732544957783\n",
            "Epoch: 94 | Iteration: 40064 | Loss: 0.23657086071748945\n",
            "Epoch: 94 | Iteration: 40128 | Loss: 0.15949655177547017\n",
            "Epoch: 94 | Iteration: 40192 | Loss: 0.3243163793154858\n",
            "Epoch: 94 | Iteration: 40256 | Loss: 0.45157688236236676\n",
            "Epoch: 94 | Iteration: 40320 | Loss: 0.21377824209598267\n",
            "Epoch: 94 | Iteration: 40384 | Loss: 0.2396236733618464\n",
            "Epoch: 94 | Iteration: 40448 | Loss: 0.07035876764984358\n",
            "Epoch: 94 | Iteration: 40512 | Loss: 0.1010358131061842\n",
            "Epoch: 94 | Iteration: 40576 | Loss: 0.6824674888962863\n",
            "Epoch: 94 | Iteration: 40640 | Loss: 0.6618684264321675\n",
            "Epoch: 94 | Iteration: 40704 | Loss: 0.5153102087767318\n",
            "Epoch: 94 | Iteration: 40768 | Loss: 0.20971854289319874\n",
            "Epoch: 94 | Iteration: 40832 | Loss: 0.14028941489963892\n",
            "Epoch: 94 | Iteration: 40896 | Loss: 0.08452265433861438\n",
            "Epoch: 94 | Iteration: 40960 | Loss: 0.8071776700648846\n",
            "Epoch: 94 | Iteration: 41024 | Loss: 0.4200528751396524\n",
            "Epoch: 94 | Iteration: 41088 | Loss: 0.1548690570675402\n",
            "Epoch: 94 | Iteration: 41152 | Loss: 0.8538175285577733\n",
            "Epoch: 94 | Iteration: 41216 | Loss: 1.1212181537961445\n",
            "Epoch: 94 | Iteration: 41280 | Loss: 0.8588514221025482\n",
            "Epoch: 94 | Iteration: 41344 | Loss: 0.31533427569518346\n",
            "Epoch: 94 | Iteration: 41408 | Loss: 0.30565250470336014\n",
            "Epoch: 94 | Iteration: 41472 | Loss: 0.042160096245791534\n",
            "Epoch: 94 | Iteration: 41536 | Loss: 0.9992108795431378\n",
            "Epoch: 94 | Iteration: 41600 | Loss: 0.1333979669435736\n",
            "Epoch: 94 | Iteration: 41664 | Loss: 0.07854990287628014\n",
            "Epoch: 94 | Iteration: 41728 | Loss: 0.2837770537943515\n",
            "Epoch: 94 | Iteration: 41792 | Loss: 0.1533913187137439\n",
            "Epoch: 94 | Iteration: 41856 | Loss: 0.5037908337881948\n",
            "Epoch: 94 | Iteration: 41920 | Loss: 0.36776792075489506\n",
            "Epoch: 94 | Iteration: 41984 | Loss: 0.09715646166201333\n",
            "Epoch: 94 | Iteration: 42048 | Loss: 0.03610863487044215\n",
            "Epoch: 94 | Iteration: 42112 | Loss: 0.19207030436589498\n",
            "Epoch: 94 | Iteration: 42176 | Loss: 0.20712313416332923\n",
            "Epoch: 94 | Iteration: 42240 | Loss: 0.07009106725595202\n",
            "Epoch: 94 | Iteration: 42304 | Loss: 0.7719302066590247\n",
            "Epoch: 94 | Iteration: 42368 | Loss: 0.5499222115398356\n",
            "Epoch: 94 | Iteration: 42432 | Loss: 0.8341270700538671\n",
            "Epoch: 94 | Iteration: 42496 | Loss: 0.6122444911940568\n",
            "Epoch: 94 | Iteration: 42560 | Loss: 0.9194833457075061\n",
            "Epoch: 94 | Iteration: 42624 | Loss: 0.6279609842581801\n",
            "Epoch: 94 | Iteration: 42688 | Loss: 0.36821078871872603\n",
            "Epoch: 94 | Iteration: 42752 | Loss: 1.053930480044256\n",
            "Epoch: 94 | Iteration: 42816 | Loss: 1.0657612287574185\n",
            "Epoch: 94 | Iteration: 42880 | Loss: 0.7754078299507501\n",
            "Epoch: 94 | Iteration: 42944 | Loss: 0.4607645280820814\n",
            "Epoch: 94 | Iteration: 43008 | Loss: 0.6575006750757013\n",
            "Epoch: 94 | Iteration: 43072 | Loss: 1.3520799616172594\n",
            "Epoch: 94 | Iteration: 43136 | Loss: 0.04244586911815397\n",
            "Epoch: 94 | Iteration: 43200 | Loss: 0.09479040020977292\n",
            "Epoch: 94 | Iteration: 43264 | Loss: 0.002951836339855219\n",
            "Epoch: 94 | Iteration: 43328 | Loss: 0.01719617355241468\n",
            "Epoch: 94 | Iteration: 43392 | Loss: 0.4205350610956413\n",
            "Epoch: 94 | Iteration: 43456 | Loss: 0.08682141347461676\n",
            "Epoch: 94 | Iteration: 43520 | Loss: 0.4220529796474194\n",
            "Epoch: 94 | Iteration: 43584 | Loss: 0.2142247735325784\n",
            "Epoch: 94 | Iteration: 43648 | Loss: 1.3187505950168208\n",
            "Epoch: 94 | Iteration: 43712 | Loss: 0.006103155843078377\n",
            "Epoch: 94 | Iteration: 43776 | Loss: 0.04958756315940235\n",
            "Epoch: 94 | Iteration: 43840 | Loss: 0.8178908915119463\n",
            "Epoch: 94 | Iteration: 43904 | Loss: 0.27194119707070036\n",
            "Epoch: 94 | Iteration: 43968 | Loss: 0.21263899766622596\n",
            "Epoch: 94 | Iteration: 44032 | Loss: 0.2591623752229551\n",
            "Epoch: 94 | Iteration: 44096 | Loss: 0.2939253439144213\n",
            "Epoch: 94 | Iteration: 44160 | Loss: 0.10768637770931666\n",
            "Epoch: 94 | Iteration: 44224 | Loss: 0.19217984052021692\n",
            "Epoch: 94 | Iteration: 44288 | Loss: 0.3659490688574605\n",
            "Epoch: 94 | Iteration: 44352 | Loss: 0.3430716902606531\n",
            "Epoch: 94 | Iteration: 44416 | Loss: 1.1161921174365754\n",
            "Epoch: 94 | Iteration: 44480 | Loss: 0.32317353136067806\n",
            "Epoch: 94 | Iteration: 44544 | Loss: 0.032888596840797195\n",
            "Epoch: 94 | Iteration: 44608 | Loss: 0.44212986872506277\n",
            "Epoch: 94 | Iteration: 44672 | Loss: 0.07157917403547083\n",
            "Epoch: 94 | Iteration: 44736 | Loss: 0.039236229185784\n",
            "Epoch: 94 | Iteration: 44800 | Loss: 0.4439437921997351\n",
            "Epoch: 94 | Iteration: 44864 | Loss: 0.2029561434406879\n",
            "Epoch: 94 | Iteration: 44928 | Loss: 0.679734453187839\n",
            "Epoch: 94 | Iteration: 44992 | Loss: 0.17771230927578832\n",
            "Epoch: 94 | Iteration: 45056 | Loss: 0.16766265355760168\n",
            "Epoch: 94 | Iteration: 45120 | Loss: 0.8440589506314696\n",
            "Epoch: 94 | Iteration: 45184 | Loss: 1.0118131779168793\n",
            "Epoch: 94 | Iteration: 45248 | Loss: 0.3629076213290247\n",
            "Epoch: 94 | Iteration: 45312 | Loss: 0.5748200436150768\n",
            "Epoch: 94 | Iteration: 45376 | Loss: 0.330439684211037\n",
            "Epoch: 94 | Iteration: 45440 | Loss: 0.8175802731790269\n",
            "Epoch: 94 | Iteration: 45504 | Loss: 0.38543506221318213\n",
            "Epoch: 94 | Iteration: 45568 | Loss: 0.6458221984172177\n",
            "Epoch: 94 | Iteration: 45632 | Loss: 0.15206849819463525\n",
            "Epoch: 94 | Iteration: 45696 | Loss: 0.007569736178775201\n",
            "Epoch: 94 | Iteration: 45760 | Loss: 0.5757438294533408\n",
            "Epoch: 94 | Iteration: 45824 | Loss: 0.35125506366512915\n",
            "Epoch: 94 | Iteration: 45888 | Loss: 0.6884050004176601\n",
            "Epoch: 94 | Iteration: 45952 | Loss: 0.5402066695226598\n",
            "Epoch: 94 | Iteration: 46016 | Loss: 1.0360955718552916\n",
            "Epoch: 94 | Iteration: 46080 | Loss: 1.2252914848514438\n",
            "Epoch: 94 | Iteration: 46144 | Loss: 0.07777054200100976\n",
            "Epoch: 94 | Iteration: 46208 | Loss: 1.431155368349575\n",
            "Epoch: 94 | Iteration: 46272 | Loss: 1.3665869058536875\n",
            "Epoch: 94 | Iteration: 46336 | Loss: 0.4835869964281011\n",
            "Epoch: 94 | Iteration: 46400 | Loss: 0.33598465954256684\n",
            "Epoch: 94 | Iteration: 46464 | Loss: 0.017841240853513803\n",
            "Epoch: 94 | Iteration: 46528 | Loss: 0.040934775260235895\n",
            "Epoch: 94 | Iteration: 46592 | Loss: 0.04741173772338289\n",
            "Epoch: 94 | Iteration: 46656 | Loss: 0.22985697901313057\n",
            "Epoch: 94 | Iteration: 46720 | Loss: 0.8951852135297649\n",
            "Epoch: 94 | Iteration: 46784 | Loss: 0.04536831427586927\n",
            "Epoch: 94 | Iteration: 46848 | Loss: 0.7455669252007395\n",
            "Epoch: 94 | Iteration: 46912 | Loss: 0.3255295979641204\n",
            "Epoch: 94 | Iteration: 46976 | Loss: 1.3023942014750278\n",
            "Epoch: 94 | Iteration: 47040 | Loss: 0.04377120819192034\n",
            "Epoch: 94 | Iteration: 47104 | Loss: 0.31384606279173966\n",
            "Epoch: 94 | Iteration: 47168 | Loss: 0.347222880146979\n",
            "Epoch: 94 | Iteration: 47232 | Loss: 1.4076911377417234\n",
            "Epoch: 94 | Iteration: 47296 | Loss: 0.36561214798934183\n",
            "Epoch: 94 | Iteration: 47360 | Loss: 0.8943127139226145\n",
            "Epoch: 94 | Iteration: 47424 | Loss: 0.285309089762185\n",
            "Epoch: 94 | Iteration: 47488 | Loss: 0.0960177841466664\n",
            "Epoch: 94 | Iteration: 47552 | Loss: 1.2435787088217571\n",
            "Epoch: 94 | Iteration: 47616 | Loss: 0.014564942446503993\n",
            "Epoch: 94 | Iteration: 47680 | Loss: 1.401615167236601\n",
            "Epoch: 94 | Iteration: 47744 | Loss: 0.8554305983078256\n",
            "Epoch: 94 | Iteration: 47808 | Loss: 0.08494800668297253\n",
            "Epoch: 94 | Iteration: 47872 | Loss: 0.782427545123288\n",
            "Epoch: 94 | Iteration: 47936 | Loss: 0.49560950339293103\n",
            "Epoch: 95 | Iteration: 0 | Loss: 0.15329980386204028\n",
            "Epoch: 95 | Iteration: 64 | Loss: 0.585141286282152\n",
            "Epoch: 95 | Iteration: 128 | Loss: 1.1299255314493184\n",
            "Epoch: 95 | Iteration: 192 | Loss: 0.27037162600773557\n",
            "Epoch: 95 | Iteration: 256 | Loss: 0.17398405436781195\n",
            "Epoch: 95 | Iteration: 320 | Loss: 0.016874831577730235\n",
            "Epoch: 95 | Iteration: 384 | Loss: 0.13237712296662674\n",
            "Epoch: 95 | Iteration: 448 | Loss: 1.791912338784654\n",
            "Epoch: 95 | Iteration: 512 | Loss: 0.45457655465712465\n",
            "Epoch: 95 | Iteration: 576 | Loss: 0.4053663213728144\n",
            "Epoch: 95 | Iteration: 640 | Loss: 0.3852292429347983\n",
            "Epoch: 95 | Iteration: 704 | Loss: 0.18481834307865777\n",
            "Epoch: 95 | Iteration: 768 | Loss: 0.4650589768035527\n",
            "Epoch: 95 | Iteration: 832 | Loss: 0.4459343414177402\n",
            "Epoch: 95 | Iteration: 896 | Loss: 0.880739955376701\n",
            "Epoch: 95 | Iteration: 960 | Loss: 0.5049279699871508\n",
            "Epoch: 95 | Iteration: 1024 | Loss: 0.9831652207982932\n",
            "Epoch: 95 | Iteration: 1088 | Loss: 0.7382870214732734\n",
            "Epoch: 95 | Iteration: 1152 | Loss: 0.32053070772606446\n",
            "Epoch: 95 | Iteration: 1216 | Loss: 0.8869822435697933\n",
            "Epoch: 95 | Iteration: 1280 | Loss: 0.18047582945095075\n",
            "Epoch: 95 | Iteration: 1344 | Loss: 1.609541848976875\n",
            "Epoch: 95 | Iteration: 1408 | Loss: 0.10716627008563827\n",
            "Epoch: 95 | Iteration: 1472 | Loss: 0.56192607965147\n",
            "Epoch: 95 | Iteration: 1536 | Loss: 0.049166890144822124\n",
            "Epoch: 95 | Iteration: 1600 | Loss: 1.0458196547762824\n",
            "Epoch: 95 | Iteration: 1664 | Loss: 0.10066460646561985\n",
            "Epoch: 95 | Iteration: 1728 | Loss: 0.16458562150854303\n",
            "Epoch: 95 | Iteration: 1792 | Loss: 0.19253976010656285\n",
            "Epoch: 95 | Iteration: 1856 | Loss: 0.18135176573959072\n",
            "Epoch: 95 | Iteration: 1920 | Loss: 0.890878386944572\n",
            "Epoch: 95 | Iteration: 1984 | Loss: 0.20243108334836013\n",
            "Epoch: 95 | Iteration: 2048 | Loss: 0.34924763846316864\n",
            "Epoch: 95 | Iteration: 2112 | Loss: 0.03694220074352699\n",
            "Epoch: 95 | Iteration: 2176 | Loss: 0.3710095934633867\n",
            "Epoch: 95 | Iteration: 2240 | Loss: 0.10141068345585579\n",
            "Epoch: 95 | Iteration: 2304 | Loss: 0.13031130855587156\n",
            "Epoch: 95 | Iteration: 2368 | Loss: 0.2054788994343916\n",
            "Epoch: 95 | Iteration: 2432 | Loss: 0.013195747633006308\n",
            "Epoch: 95 | Iteration: 2496 | Loss: 0.13381857260481417\n",
            "Epoch: 95 | Iteration: 2560 | Loss: 0.4207222820583469\n",
            "Epoch: 95 | Iteration: 2624 | Loss: 0.8681017917906014\n",
            "Epoch: 95 | Iteration: 2688 | Loss: 0.9070015828982172\n",
            "Epoch: 95 | Iteration: 2752 | Loss: 0.06870080146506491\n",
            "Epoch: 95 | Iteration: 2816 | Loss: 0.1313089326504\n",
            "Epoch: 95 | Iteration: 2880 | Loss: 0.33363698776893147\n",
            "Epoch: 95 | Iteration: 2944 | Loss: 0.10169289257205917\n",
            "Epoch: 95 | Iteration: 3008 | Loss: 0.3141592700439815\n",
            "Epoch: 95 | Iteration: 3072 | Loss: 0.08662608037284254\n",
            "Epoch: 95 | Iteration: 3136 | Loss: 0.0015168580963505378\n",
            "Epoch: 95 | Iteration: 3200 | Loss: 0.09910709799160486\n",
            "Epoch: 95 | Iteration: 3264 | Loss: 0.07594451642086475\n",
            "Epoch: 95 | Iteration: 3328 | Loss: 0.06973558041895715\n",
            "Epoch: 95 | Iteration: 3392 | Loss: 0.05647988007368704\n",
            "Epoch: 95 | Iteration: 3456 | Loss: 0.23818784047756278\n",
            "Epoch: 95 | Iteration: 3520 | Loss: 0.15908330191593179\n",
            "Epoch: 95 | Iteration: 3584 | Loss: 0.07237820539709697\n",
            "Epoch: 95 | Iteration: 3648 | Loss: 0.5404359033223385\n",
            "Epoch: 95 | Iteration: 3712 | Loss: 0.11063083587335482\n",
            "Epoch: 95 | Iteration: 3776 | Loss: 0.3506785252948402\n",
            "Epoch: 95 | Iteration: 3840 | Loss: 0.007158797535228998\n",
            "Epoch: 95 | Iteration: 3904 | Loss: 0.206693162537887\n",
            "Epoch: 95 | Iteration: 3968 | Loss: 0.07785252609147349\n",
            "Epoch: 95 | Iteration: 4032 | Loss: 0.15658280338431177\n",
            "Epoch: 95 | Iteration: 4096 | Loss: 0.2902009735545278\n",
            "Epoch: 95 | Iteration: 4160 | Loss: 0.211702073681088\n",
            "Epoch: 95 | Iteration: 4224 | Loss: 0.5206311283795223\n",
            "Epoch: 95 | Iteration: 4288 | Loss: 0.1012786077811355\n",
            "Epoch: 95 | Iteration: 4352 | Loss: 0.30710710113575757\n",
            "Epoch: 95 | Iteration: 4416 | Loss: 0.9543046888150055\n",
            "Epoch: 95 | Iteration: 4480 | Loss: 0.07102384181133174\n",
            "Epoch: 95 | Iteration: 4544 | Loss: 0.0505324873840493\n",
            "Epoch: 95 | Iteration: 4608 | Loss: 0.7315416548517681\n",
            "Epoch: 95 | Iteration: 4672 | Loss: 0.062012123909016746\n",
            "Epoch: 95 | Iteration: 4736 | Loss: 0.0963835218756483\n",
            "Epoch: 95 | Iteration: 4800 | Loss: 0.16752679106955864\n",
            "Epoch: 95 | Iteration: 4864 | Loss: 0.03052675257163274\n",
            "Epoch: 95 | Iteration: 4928 | Loss: 0.404873619543239\n",
            "Epoch: 95 | Iteration: 4992 | Loss: 0.7608821883232038\n",
            "Epoch: 95 | Iteration: 5056 | Loss: 0.5349598781662498\n",
            "Epoch: 95 | Iteration: 5120 | Loss: 0.7881249636316708\n",
            "Epoch: 95 | Iteration: 5184 | Loss: 0.2984778376834872\n",
            "Epoch: 95 | Iteration: 5248 | Loss: 0.3474112000631186\n",
            "Epoch: 95 | Iteration: 5312 | Loss: 0.3811020937346505\n",
            "Epoch: 95 | Iteration: 5376 | Loss: 0.11367528347939396\n",
            "Epoch: 95 | Iteration: 5440 | Loss: 0.12041770052251995\n",
            "Epoch: 95 | Iteration: 5504 | Loss: 0.3180064724343285\n",
            "Epoch: 95 | Iteration: 5568 | Loss: 0.17020091851232538\n",
            "Epoch: 95 | Iteration: 5632 | Loss: 0.16639851949521148\n",
            "Epoch: 95 | Iteration: 5696 | Loss: 0.8986469984217421\n",
            "Epoch: 95 | Iteration: 5760 | Loss: 1.2136438977267077\n",
            "Epoch: 95 | Iteration: 5824 | Loss: 0.17924990209966082\n",
            "Epoch: 95 | Iteration: 5888 | Loss: 1.0320581653112593\n",
            "Epoch: 95 | Iteration: 5952 | Loss: 0.05080135835748496\n",
            "Epoch: 95 | Iteration: 6016 | Loss: 0.0780939720518096\n",
            "Epoch: 95 | Iteration: 6080 | Loss: 0.49029393241777086\n",
            "Epoch: 95 | Iteration: 6144 | Loss: 0.4004865143384583\n",
            "Epoch: 95 | Iteration: 6208 | Loss: 0.49366039671008766\n",
            "Epoch: 95 | Iteration: 6272 | Loss: 0.03675066065863939\n",
            "Epoch: 95 | Iteration: 6336 | Loss: 0.18828774719851915\n",
            "Epoch: 95 | Iteration: 6400 | Loss: 1.0441007636558144\n",
            "Epoch: 95 | Iteration: 6464 | Loss: 0.06419690172726732\n",
            "Epoch: 95 | Iteration: 6528 | Loss: 0.2626243020361596\n",
            "Epoch: 95 | Iteration: 6592 | Loss: 0.024497607788785524\n",
            "Epoch: 95 | Iteration: 6656 | Loss: 0.273722181508424\n",
            "Epoch: 95 | Iteration: 6720 | Loss: 0.055448975530230864\n",
            "Epoch: 95 | Iteration: 6784 | Loss: 0.9362086658671697\n",
            "Epoch: 95 | Iteration: 6848 | Loss: 1.762296840084765\n",
            "Epoch: 95 | Iteration: 6912 | Loss: 0.4526547445854302\n",
            "Epoch: 95 | Iteration: 6976 | Loss: 0.5746701303353412\n",
            "Epoch: 95 | Iteration: 7040 | Loss: 1.0839259606354381\n",
            "Epoch: 95 | Iteration: 7104 | Loss: 0.232534789583435\n",
            "Epoch: 95 | Iteration: 7168 | Loss: 0.12877895679072596\n",
            "Epoch: 95 | Iteration: 7232 | Loss: 0.8914964460256536\n",
            "Epoch: 95 | Iteration: 7296 | Loss: 0.7676550726562171\n",
            "Epoch: 95 | Iteration: 7360 | Loss: 0.04679144417865537\n",
            "Epoch: 95 | Iteration: 7424 | Loss: 0.0909149004789378\n",
            "Epoch: 95 | Iteration: 7488 | Loss: 0.5464911680208592\n",
            "Epoch: 95 | Iteration: 7552 | Loss: 0.6977038375744683\n",
            "Epoch: 95 | Iteration: 7616 | Loss: 0.19523865094487708\n",
            "Epoch: 95 | Iteration: 7680 | Loss: 0.5856894403412214\n",
            "Epoch: 95 | Iteration: 7744 | Loss: 0.3414147147201977\n",
            "Epoch: 95 | Iteration: 7808 | Loss: 0.5170927767165091\n",
            "Epoch: 95 | Iteration: 7872 | Loss: 0.5929533388979853\n",
            "Epoch: 95 | Iteration: 7936 | Loss: 0.2966003497316914\n",
            "Epoch: 95 | Iteration: 8000 | Loss: 0.10960569221176107\n",
            "Epoch: 95 | Iteration: 8064 | Loss: 0.32742243324003795\n",
            "Epoch: 95 | Iteration: 8128 | Loss: 0.10441959387418226\n",
            "Epoch: 95 | Iteration: 8192 | Loss: 1.8741323578601334\n",
            "Epoch: 95 | Iteration: 8256 | Loss: 0.14754250066659305\n",
            "Epoch: 95 | Iteration: 8320 | Loss: 0.34626789413894055\n",
            "Epoch: 95 | Iteration: 8384 | Loss: 0.3593282902148424\n",
            "Epoch: 95 | Iteration: 8448 | Loss: 0.6289219098664323\n",
            "Epoch: 95 | Iteration: 8512 | Loss: 0.06610200109615391\n",
            "Epoch: 95 | Iteration: 8576 | Loss: 0.1647143844720106\n",
            "Epoch: 95 | Iteration: 8640 | Loss: 0.2084110008849186\n",
            "Epoch: 95 | Iteration: 8704 | Loss: 1.1087466399327468\n",
            "Epoch: 95 | Iteration: 8768 | Loss: 0.5807534711884748\n",
            "Epoch: 95 | Iteration: 8832 | Loss: 1.1140538683210786\n",
            "Epoch: 95 | Iteration: 8896 | Loss: 0.5722781224686592\n",
            "Epoch: 95 | Iteration: 8960 | Loss: 0.22082723738616217\n",
            "Epoch: 95 | Iteration: 9024 | Loss: 0.19544412319292126\n",
            "Epoch: 95 | Iteration: 9088 | Loss: 0.24712462746143848\n",
            "Epoch: 95 | Iteration: 9152 | Loss: 0.0899721320600175\n",
            "Epoch: 95 | Iteration: 9216 | Loss: 0.26817362725067156\n",
            "Epoch: 95 | Iteration: 9280 | Loss: 0.8179896069298511\n",
            "Epoch: 95 | Iteration: 9344 | Loss: 0.2185272618280886\n",
            "Epoch: 95 | Iteration: 9408 | Loss: 0.62773448752131\n",
            "Epoch: 95 | Iteration: 9472 | Loss: 0.5515839508801702\n",
            "Epoch: 95 | Iteration: 9536 | Loss: 0.08610882702851785\n",
            "Epoch: 95 | Iteration: 9600 | Loss: 0.1561051655507106\n",
            "Epoch: 95 | Iteration: 9664 | Loss: 0.003254743972379212\n",
            "Epoch: 95 | Iteration: 9728 | Loss: 0.44108968841719265\n",
            "Epoch: 95 | Iteration: 9792 | Loss: 0.09088776039365856\n",
            "Epoch: 95 | Iteration: 9856 | Loss: 0.024049915885415184\n",
            "Epoch: 95 | Iteration: 9920 | Loss: 0.33329784883027486\n",
            "Epoch: 95 | Iteration: 9984 | Loss: 0.14918036739487267\n",
            "Epoch: 95 | Iteration: 10048 | Loss: 0.3481143937775555\n",
            "Epoch: 95 | Iteration: 10112 | Loss: 0.5446176648180927\n",
            "Epoch: 95 | Iteration: 10176 | Loss: 0.6494977181434911\n",
            "Epoch: 95 | Iteration: 10240 | Loss: 0.5335916171884867\n",
            "Epoch: 95 | Iteration: 10304 | Loss: 0.027293209995072978\n",
            "Epoch: 95 | Iteration: 10368 | Loss: 0.12811257604167686\n",
            "Epoch: 95 | Iteration: 10432 | Loss: 0.0431015071086556\n",
            "Epoch: 95 | Iteration: 10496 | Loss: 0.027398887499520255\n",
            "Epoch: 95 | Iteration: 10560 | Loss: 0.03536044779459552\n",
            "Epoch: 95 | Iteration: 10624 | Loss: 0.14837632071453416\n",
            "Epoch: 95 | Iteration: 10688 | Loss: 0.10398004454819182\n",
            "Epoch: 95 | Iteration: 10752 | Loss: 0.3815977697665911\n",
            "Epoch: 95 | Iteration: 10816 | Loss: 0.18995234346855433\n",
            "Epoch: 95 | Iteration: 10880 | Loss: 0.16692202702830272\n",
            "Epoch: 95 | Iteration: 10944 | Loss: 1.1335517142790037\n",
            "Epoch: 95 | Iteration: 11008 | Loss: 0.2287848367704206\n",
            "Epoch: 95 | Iteration: 11072 | Loss: 0.32671969748086416\n",
            "Epoch: 95 | Iteration: 11136 | Loss: 0.08538958743979175\n",
            "Epoch: 95 | Iteration: 11200 | Loss: 0.2656989352066804\n",
            "Epoch: 95 | Iteration: 11264 | Loss: 0.02960953741290264\n",
            "Epoch: 95 | Iteration: 11328 | Loss: 0.0916533445649745\n",
            "Epoch: 95 | Iteration: 11392 | Loss: 0.027993581199729743\n",
            "Epoch: 95 | Iteration: 11456 | Loss: 0.09316772496397502\n",
            "Epoch: 95 | Iteration: 11520 | Loss: 0.49297343093602064\n",
            "Epoch: 95 | Iteration: 11584 | Loss: 0.498470737277645\n",
            "Epoch: 95 | Iteration: 11648 | Loss: 0.6890825956431644\n",
            "Epoch: 95 | Iteration: 11712 | Loss: 0.7565940987008223\n",
            "Epoch: 95 | Iteration: 11776 | Loss: 0.18924119908066495\n",
            "Epoch: 95 | Iteration: 11840 | Loss: 0.480479207707248\n",
            "Epoch: 95 | Iteration: 11904 | Loss: 0.5117925039175922\n",
            "Epoch: 95 | Iteration: 11968 | Loss: 0.1718663253357407\n",
            "Epoch: 95 | Iteration: 12032 | Loss: 0.08896330476638814\n",
            "Epoch: 95 | Iteration: 12096 | Loss: 0.03376941201533411\n",
            "Epoch: 95 | Iteration: 12160 | Loss: 0.12226160473744065\n",
            "Epoch: 95 | Iteration: 12224 | Loss: 0.1339428434441915\n",
            "Epoch: 95 | Iteration: 12288 | Loss: 0.2347414731728926\n",
            "Epoch: 95 | Iteration: 12352 | Loss: 0.1732726376771214\n",
            "Epoch: 95 | Iteration: 12416 | Loss: 0.09596541920654578\n",
            "Epoch: 95 | Iteration: 12480 | Loss: 0.05929050023260435\n",
            "Epoch: 95 | Iteration: 12544 | Loss: 0.6997387368431736\n",
            "Epoch: 95 | Iteration: 12608 | Loss: 0.1556426251905398\n",
            "Epoch: 95 | Iteration: 12672 | Loss: 0.6476233034420424\n",
            "Epoch: 95 | Iteration: 12736 | Loss: 0.04776769250415275\n",
            "Epoch: 95 | Iteration: 12800 | Loss: 0.6228145329283841\n",
            "Epoch: 95 | Iteration: 12864 | Loss: 0.15031501864460106\n",
            "Epoch: 95 | Iteration: 12928 | Loss: 0.23402010354617023\n",
            "Epoch: 95 | Iteration: 12992 | Loss: 0.12281841142477073\n",
            "Epoch: 95 | Iteration: 13056 | Loss: 0.1021061865742154\n",
            "Epoch: 95 | Iteration: 13120 | Loss: 0.06684771086976175\n",
            "Epoch: 95 | Iteration: 13184 | Loss: 0.48356839034993865\n",
            "Epoch: 95 | Iteration: 13248 | Loss: 0.06144851866978203\n",
            "Epoch: 95 | Iteration: 13312 | Loss: 0.0157628561007835\n",
            "Epoch: 95 | Iteration: 13376 | Loss: 0.491005301378868\n",
            "Epoch: 95 | Iteration: 13440 | Loss: 0.06698780766081507\n",
            "Epoch: 95 | Iteration: 13504 | Loss: 0.2347747828503881\n",
            "Epoch: 95 | Iteration: 13568 | Loss: 0.09519124413702723\n",
            "Epoch: 95 | Iteration: 13632 | Loss: 0.36360036285796493\n",
            "Epoch: 95 | Iteration: 13696 | Loss: 0.4948460871492394\n",
            "Epoch: 95 | Iteration: 13760 | Loss: 0.024648994100290693\n",
            "Epoch: 95 | Iteration: 13824 | Loss: 0.2656995775393151\n",
            "Epoch: 95 | Iteration: 13888 | Loss: 0.13362804765631087\n",
            "Epoch: 95 | Iteration: 13952 | Loss: 0.8335870326921828\n",
            "Epoch: 95 | Iteration: 14016 | Loss: 0.24757701859881676\n",
            "Epoch: 95 | Iteration: 14080 | Loss: 0.2526808865830598\n",
            "Epoch: 95 | Iteration: 14144 | Loss: 0.22868472460269756\n",
            "Epoch: 95 | Iteration: 14208 | Loss: 0.5983532466205435\n",
            "Epoch: 95 | Iteration: 14272 | Loss: 0.4476518920378295\n",
            "Epoch: 95 | Iteration: 14336 | Loss: 0.26334833481929665\n",
            "Epoch: 95 | Iteration: 14400 | Loss: 0.15973863269252925\n",
            "Epoch: 95 | Iteration: 14464 | Loss: 0.08587806156970529\n",
            "Epoch: 95 | Iteration: 14528 | Loss: 0.32023004933512744\n",
            "Epoch: 95 | Iteration: 14592 | Loss: 0.1855680088251747\n",
            "Epoch: 95 | Iteration: 14656 | Loss: 0.09205364374226593\n",
            "Epoch: 95 | Iteration: 14720 | Loss: 0.776300937667387\n",
            "Epoch: 95 | Iteration: 14784 | Loss: 0.7925215731986675\n",
            "Epoch: 95 | Iteration: 14848 | Loss: 0.23071807071953132\n",
            "Epoch: 95 | Iteration: 14912 | Loss: 0.21300571870799556\n",
            "Epoch: 95 | Iteration: 14976 | Loss: 0.04143411785566511\n",
            "Epoch: 95 | Iteration: 15040 | Loss: 0.14340715560231798\n",
            "Epoch: 95 | Iteration: 15104 | Loss: 0.6609459545634397\n",
            "Epoch: 95 | Iteration: 15168 | Loss: 0.1375713582415309\n",
            "Epoch: 95 | Iteration: 15232 | Loss: 0.13840814910938742\n",
            "Epoch: 95 | Iteration: 15296 | Loss: 0.48085405643296236\n",
            "Epoch: 95 | Iteration: 15360 | Loss: 0.3784684505552608\n",
            "Epoch: 95 | Iteration: 15424 | Loss: 0.0720116864007685\n",
            "Epoch: 95 | Iteration: 15488 | Loss: 0.10059889573479372\n",
            "Epoch: 95 | Iteration: 15552 | Loss: 0.2754137059866676\n",
            "Epoch: 95 | Iteration: 15616 | Loss: 0.05421269125674593\n",
            "Epoch: 95 | Iteration: 15680 | Loss: 0.22284578443695807\n",
            "Epoch: 95 | Iteration: 15744 | Loss: 0.429034232481483\n",
            "Epoch: 95 | Iteration: 15808 | Loss: 0.5192263640272734\n",
            "Epoch: 95 | Iteration: 15872 | Loss: 0.25603617622978636\n",
            "Epoch: 95 | Iteration: 15936 | Loss: 1.0353554261213034\n",
            "Epoch: 95 | Iteration: 16000 | Loss: 0.5151623855530169\n",
            "Epoch: 95 | Iteration: 16064 | Loss: 0.06208578979830755\n",
            "Epoch: 95 | Iteration: 16128 | Loss: 0.34061683933870457\n",
            "Epoch: 95 | Iteration: 16192 | Loss: 0.011783421289996404\n",
            "Epoch: 95 | Iteration: 16256 | Loss: 0.22865646602193207\n",
            "Epoch: 95 | Iteration: 16320 | Loss: 0.17121706119871813\n",
            "Epoch: 95 | Iteration: 16384 | Loss: 0.42110650989417825\n",
            "Epoch: 95 | Iteration: 16448 | Loss: 0.17144464080901436\n",
            "Epoch: 95 | Iteration: 16512 | Loss: 0.7699754035348195\n",
            "Epoch: 95 | Iteration: 16576 | Loss: 0.05923337800732674\n",
            "Epoch: 95 | Iteration: 16640 | Loss: 0.5292965287229239\n",
            "Epoch: 95 | Iteration: 16704 | Loss: 0.31650759218189395\n",
            "Epoch: 95 | Iteration: 16768 | Loss: 0.371692161747589\n",
            "Epoch: 95 | Iteration: 16832 | Loss: 0.4450670705551724\n",
            "Epoch: 95 | Iteration: 16896 | Loss: 0.3745677711982153\n",
            "Epoch: 95 | Iteration: 16960 | Loss: 0.3081602871241276\n",
            "Epoch: 95 | Iteration: 17024 | Loss: 0.304924116835363\n",
            "Epoch: 95 | Iteration: 17088 | Loss: 0.5646390554916305\n",
            "Epoch: 95 | Iteration: 17152 | Loss: 0.42453027024244605\n",
            "Epoch: 95 | Iteration: 17216 | Loss: 0.5113473970623941\n",
            "Epoch: 95 | Iteration: 17280 | Loss: 0.013362617142598005\n",
            "Epoch: 95 | Iteration: 17344 | Loss: 0.4319859660086355\n",
            "Epoch: 95 | Iteration: 17408 | Loss: 0.13335090617423723\n",
            "Epoch: 95 | Iteration: 17472 | Loss: 0.20260335886438968\n",
            "Epoch: 95 | Iteration: 17536 | Loss: 0.652072678331748\n",
            "Epoch: 95 | Iteration: 17600 | Loss: 0.20352796756178812\n",
            "Epoch: 95 | Iteration: 17664 | Loss: 0.3905865280585591\n",
            "Epoch: 95 | Iteration: 17728 | Loss: 1.5248077763005001\n",
            "Epoch: 95 | Iteration: 17792 | Loss: 0.21845245130633414\n",
            "Epoch: 95 | Iteration: 17856 | Loss: 0.4117325485014162\n",
            "Epoch: 95 | Iteration: 17920 | Loss: 0.04380805071571747\n",
            "Epoch: 95 | Iteration: 17984 | Loss: 0.0836825433996785\n",
            "Epoch: 95 | Iteration: 18048 | Loss: 0.48094962146024395\n",
            "Epoch: 95 | Iteration: 18112 | Loss: 0.2253682612062771\n",
            "Epoch: 95 | Iteration: 18176 | Loss: 0.10627506133901633\n",
            "Epoch: 95 | Iteration: 18240 | Loss: 0.6129221520096848\n",
            "Epoch: 95 | Iteration: 18304 | Loss: 0.45602913220454727\n",
            "Epoch: 95 | Iteration: 18368 | Loss: 0.3605772082667925\n",
            "Epoch: 95 | Iteration: 18432 | Loss: 0.718765486308648\n",
            "Epoch: 95 | Iteration: 18496 | Loss: 0.16398777197794856\n",
            "Epoch: 95 | Iteration: 18560 | Loss: 0.2074204123101886\n",
            "Epoch: 95 | Iteration: 18624 | Loss: 0.07356305238248224\n",
            "Epoch: 95 | Iteration: 18688 | Loss: 0.36203060433103784\n",
            "Epoch: 95 | Iteration: 18752 | Loss: 0.08014360482675435\n",
            "Epoch: 95 | Iteration: 18816 | Loss: 0.08577663889619953\n",
            "Epoch: 95 | Iteration: 18880 | Loss: 0.2196624615623781\n",
            "Epoch: 95 | Iteration: 18944 | Loss: 0.20402344311400356\n",
            "Epoch: 95 | Iteration: 19008 | Loss: 0.1454816752103663\n",
            "Epoch: 95 | Iteration: 19072 | Loss: 1.3452807961903548\n",
            "Epoch: 95 | Iteration: 19136 | Loss: 0.15941230068670742\n",
            "Epoch: 95 | Iteration: 19200 | Loss: 0.18563476934732806\n",
            "Epoch: 95 | Iteration: 19264 | Loss: 0.507637365740209\n",
            "Epoch: 95 | Iteration: 19328 | Loss: 1.6067895456170638\n",
            "Epoch: 95 | Iteration: 19392 | Loss: 0.32745671677319554\n",
            "Epoch: 95 | Iteration: 19456 | Loss: 0.2505175371197205\n",
            "Epoch: 95 | Iteration: 19520 | Loss: 0.05563127780292586\n",
            "Epoch: 95 | Iteration: 19584 | Loss: 0.12205767514626881\n",
            "Epoch: 95 | Iteration: 19648 | Loss: 0.11188968273323731\n",
            "Epoch: 95 | Iteration: 19712 | Loss: 0.014529152755084061\n",
            "Epoch: 95 | Iteration: 19776 | Loss: 0.8716537489008396\n",
            "Epoch: 95 | Iteration: 19840 | Loss: 0.40712026375260957\n",
            "Epoch: 95 | Iteration: 19904 | Loss: 0.37940369346366465\n",
            "Epoch: 95 | Iteration: 19968 | Loss: 0.30553622728569196\n",
            "Epoch: 95 | Iteration: 20032 | Loss: 0.27022903535208703\n",
            "Epoch: 95 | Iteration: 20096 | Loss: 0.09460655102520886\n",
            "Epoch: 95 | Iteration: 20160 | Loss: 0.8970435413837867\n",
            "Epoch: 95 | Iteration: 20224 | Loss: 0.44954282487320096\n",
            "Epoch: 95 | Iteration: 20288 | Loss: 0.8214082211416422\n",
            "Epoch: 95 | Iteration: 20352 | Loss: 0.006919102350023061\n",
            "Epoch: 95 | Iteration: 20416 | Loss: 0.01569262903684212\n",
            "Epoch: 95 | Iteration: 20480 | Loss: 0.10420314974358082\n",
            "Epoch: 95 | Iteration: 20544 | Loss: 0.21567138040045694\n",
            "Epoch: 95 | Iteration: 20608 | Loss: 0.12985122989227363\n",
            "Epoch: 95 | Iteration: 20672 | Loss: 1.807927561977488\n",
            "Epoch: 95 | Iteration: 20736 | Loss: 1.153025486800133\n",
            "Epoch: 95 | Iteration: 20800 | Loss: 0.3965719275986809\n",
            "Epoch: 95 | Iteration: 20864 | Loss: 0.35845139599572307\n",
            "Epoch: 95 | Iteration: 20928 | Loss: 0.29918669971000744\n",
            "Epoch: 95 | Iteration: 20992 | Loss: 0.40356054263815455\n",
            "Epoch: 95 | Iteration: 21056 | Loss: 0.2827719824457194\n",
            "Epoch: 95 | Iteration: 21120 | Loss: 0.35489296246567786\n",
            "Epoch: 95 | Iteration: 21184 | Loss: 0.21876634263864647\n",
            "Epoch: 95 | Iteration: 21248 | Loss: 0.2070807592061853\n",
            "Epoch: 95 | Iteration: 21312 | Loss: 0.4382800840054918\n",
            "Epoch: 95 | Iteration: 21376 | Loss: 0.1809417864927954\n",
            "Epoch: 95 | Iteration: 21440 | Loss: 0.6003115391521058\n",
            "Epoch: 95 | Iteration: 21504 | Loss: 0.1975168111237362\n",
            "Epoch: 95 | Iteration: 21568 | Loss: 1.4165800364968966\n",
            "Epoch: 95 | Iteration: 21632 | Loss: 0.24323599907790094\n",
            "Epoch: 95 | Iteration: 21696 | Loss: 0.10912101405475536\n",
            "Epoch: 95 | Iteration: 21760 | Loss: 0.006152225332218382\n",
            "Epoch: 95 | Iteration: 21824 | Loss: 0.0037165487508779224\n",
            "Epoch: 95 | Iteration: 21888 | Loss: 0.5548579472330957\n",
            "Epoch: 95 | Iteration: 21952 | Loss: 0.3796062182039122\n",
            "Epoch: 95 | Iteration: 22016 | Loss: 0.07642304800120485\n",
            "Epoch: 95 | Iteration: 22080 | Loss: 0.4302946166842975\n",
            "Epoch: 95 | Iteration: 22144 | Loss: 0.36599819174248627\n",
            "Epoch: 95 | Iteration: 22208 | Loss: 0.24379166367833988\n",
            "Epoch: 95 | Iteration: 22272 | Loss: 0.7368332892404666\n",
            "Epoch: 95 | Iteration: 22336 | Loss: 0.040318783166207545\n",
            "Epoch: 95 | Iteration: 22400 | Loss: 0.2008960345099055\n",
            "Epoch: 95 | Iteration: 22464 | Loss: 0.622556726504918\n",
            "Epoch: 95 | Iteration: 22528 | Loss: 0.9608791706848271\n",
            "Epoch: 95 | Iteration: 22592 | Loss: 0.4898763165441272\n",
            "Epoch: 95 | Iteration: 22656 | Loss: 0.12810357698019606\n",
            "Epoch: 95 | Iteration: 22720 | Loss: 0.5791054411605563\n",
            "Epoch: 95 | Iteration: 22784 | Loss: 0.16416432172562437\n",
            "Epoch: 95 | Iteration: 22848 | Loss: 0.2746845979267877\n",
            "Epoch: 95 | Iteration: 22912 | Loss: 0.001773429436491303\n",
            "Epoch: 95 | Iteration: 22976 | Loss: 0.27861696298364885\n",
            "Epoch: 95 | Iteration: 23040 | Loss: 0.23714664794760026\n",
            "Epoch: 95 | Iteration: 23104 | Loss: 0.12475787230675327\n",
            "Epoch: 95 | Iteration: 23168 | Loss: 0.5167682443348391\n",
            "Epoch: 95 | Iteration: 23232 | Loss: 0.02536890781616993\n",
            "Epoch: 95 | Iteration: 23296 | Loss: 0.11889853091901302\n",
            "Epoch: 95 | Iteration: 23360 | Loss: 0.31788222866358856\n",
            "Epoch: 95 | Iteration: 23424 | Loss: 0.0927729807195389\n",
            "Epoch: 95 | Iteration: 23488 | Loss: 0.08950995926018587\n",
            "Epoch: 95 | Iteration: 23552 | Loss: 0.6404168819816585\n",
            "Epoch: 95 | Iteration: 23616 | Loss: 0.7167480426857713\n",
            "Epoch: 95 | Iteration: 23680 | Loss: 0.8563877020679433\n",
            "Epoch: 95 | Iteration: 23744 | Loss: 0.0128640916230187\n",
            "Epoch: 95 | Iteration: 23808 | Loss: 0.3872801356571088\n",
            "Epoch: 95 | Iteration: 23872 | Loss: 0.6167545930016399\n",
            "Epoch: 95 | Iteration: 23936 | Loss: 1.033494053433475\n",
            "Epoch: 95 | Iteration: 24000 | Loss: 0.16585905481129556\n",
            "Epoch: 95 | Iteration: 24064 | Loss: 0.07842033697957781\n",
            "Epoch: 95 | Iteration: 24128 | Loss: 0.15319648617020004\n",
            "Epoch: 95 | Iteration: 24192 | Loss: 0.2876814172913087\n",
            "Epoch: 95 | Iteration: 24256 | Loss: 0.718684717302595\n",
            "Epoch: 95 | Iteration: 24320 | Loss: 0.39417327230584553\n",
            "Epoch: 95 | Iteration: 24384 | Loss: 0.009928703705591117\n",
            "Epoch: 95 | Iteration: 24448 | Loss: 0.23159874723541243\n",
            "Epoch: 95 | Iteration: 24512 | Loss: 0.20124944493116215\n",
            "Epoch: 95 | Iteration: 24576 | Loss: 0.38680404768391863\n",
            "Epoch: 95 | Iteration: 24640 | Loss: 0.19371369554947787\n",
            "Epoch: 95 | Iteration: 24704 | Loss: 0.11949741985154813\n",
            "Epoch: 95 | Iteration: 24768 | Loss: 0.6150257707781783\n",
            "Epoch: 95 | Iteration: 24832 | Loss: 0.05638546662248815\n",
            "Epoch: 95 | Iteration: 24896 | Loss: 0.2817142964875204\n",
            "Epoch: 95 | Iteration: 24960 | Loss: 0.011479507005669696\n",
            "Epoch: 95 | Iteration: 25024 | Loss: 0.04334790656889628\n",
            "Epoch: 95 | Iteration: 25088 | Loss: 0.32405372802737814\n",
            "Epoch: 95 | Iteration: 25152 | Loss: 0.7350334696003071\n",
            "Epoch: 95 | Iteration: 25216 | Loss: 0.1985621834815834\n",
            "Epoch: 95 | Iteration: 25280 | Loss: 0.15136973147700333\n",
            "Epoch: 95 | Iteration: 25344 | Loss: 0.02482366537680436\n",
            "Epoch: 95 | Iteration: 25408 | Loss: 0.031953150689729204\n",
            "Epoch: 95 | Iteration: 25472 | Loss: 0.5807288743534658\n",
            "Epoch: 95 | Iteration: 25536 | Loss: 1.1987092256593455\n",
            "Epoch: 95 | Iteration: 25600 | Loss: 0.07494250847793966\n",
            "Epoch: 95 | Iteration: 25664 | Loss: 0.6290197205530215\n",
            "Epoch: 95 | Iteration: 25728 | Loss: 0.5887368504288095\n",
            "Epoch: 95 | Iteration: 25792 | Loss: 0.884457396510433\n",
            "Epoch: 95 | Iteration: 25856 | Loss: 0.0815403799425511\n",
            "Epoch: 95 | Iteration: 25920 | Loss: 0.11634228751541521\n",
            "Epoch: 95 | Iteration: 25984 | Loss: 0.047914507833399324\n",
            "Epoch: 95 | Iteration: 26048 | Loss: 0.31372911921770075\n",
            "Epoch: 95 | Iteration: 26112 | Loss: 0.26791232106911494\n",
            "Epoch: 95 | Iteration: 26176 | Loss: 0.1958259523070424\n",
            "Epoch: 95 | Iteration: 26240 | Loss: 0.21106914397956836\n",
            "Epoch: 95 | Iteration: 26304 | Loss: 0.5473932365096484\n",
            "Epoch: 95 | Iteration: 26368 | Loss: 0.9009008160722565\n",
            "Epoch: 95 | Iteration: 26432 | Loss: 0.9556856937308065\n",
            "Epoch: 95 | Iteration: 26496 | Loss: 1.4665056036929243\n",
            "Epoch: 95 | Iteration: 26560 | Loss: 0.9281540276926239\n",
            "Epoch: 95 | Iteration: 26624 | Loss: 0.9167426204362679\n",
            "Epoch: 95 | Iteration: 26688 | Loss: 0.822725792692704\n",
            "Epoch: 95 | Iteration: 26752 | Loss: 0.9563338910633201\n",
            "Epoch: 95 | Iteration: 26816 | Loss: 0.2716433137676636\n",
            "Epoch: 95 | Iteration: 26880 | Loss: 1.0954677151349126\n",
            "Epoch: 95 | Iteration: 26944 | Loss: 0.0159675743631826\n",
            "Epoch: 95 | Iteration: 27008 | Loss: 0.1246244157124853\n",
            "Epoch: 95 | Iteration: 27072 | Loss: 0.07506998348675861\n",
            "Epoch: 95 | Iteration: 27136 | Loss: 1.6361408293158846\n",
            "Epoch: 95 | Iteration: 27200 | Loss: 0.9256503568221719\n",
            "Epoch: 95 | Iteration: 27264 | Loss: 0.11746726719625956\n",
            "Epoch: 95 | Iteration: 27328 | Loss: 0.020708825138501342\n",
            "Epoch: 95 | Iteration: 27392 | Loss: 0.1486090138763475\n",
            "Epoch: 95 | Iteration: 27456 | Loss: 0.5664368138229154\n",
            "Epoch: 95 | Iteration: 27520 | Loss: 0.6363657606954009\n",
            "Epoch: 95 | Iteration: 27584 | Loss: 0.7414400983726335\n",
            "Epoch: 95 | Iteration: 27648 | Loss: 0.585531364526603\n",
            "Epoch: 95 | Iteration: 27712 | Loss: 0.2304717490403554\n",
            "Epoch: 95 | Iteration: 27776 | Loss: 0.3121388746209087\n",
            "Epoch: 95 | Iteration: 27840 | Loss: 0.9195769732899945\n",
            "Epoch: 95 | Iteration: 27904 | Loss: 0.08899664973742245\n",
            "Epoch: 95 | Iteration: 27968 | Loss: 0.026152538219409376\n",
            "Epoch: 95 | Iteration: 28032 | Loss: 0.22054299751293038\n",
            "Epoch: 95 | Iteration: 28096 | Loss: 0.14157550872553934\n",
            "Epoch: 95 | Iteration: 28160 | Loss: 0.6302816760372759\n",
            "Epoch: 95 | Iteration: 28224 | Loss: 0.07411343414979936\n",
            "Epoch: 95 | Iteration: 28288 | Loss: 0.01241988497579222\n",
            "Epoch: 95 | Iteration: 28352 | Loss: 0.7332511087339424\n",
            "Epoch: 95 | Iteration: 28416 | Loss: 0.1034157929481766\n",
            "Epoch: 95 | Iteration: 28480 | Loss: 0.08854720710539712\n",
            "Epoch: 95 | Iteration: 28544 | Loss: 0.12420864844622843\n",
            "Epoch: 95 | Iteration: 28608 | Loss: 0.689674452859153\n",
            "Epoch: 95 | Iteration: 28672 | Loss: 0.2967873266975396\n",
            "Epoch: 95 | Iteration: 28736 | Loss: 0.11557759838999933\n",
            "Epoch: 95 | Iteration: 28800 | Loss: 0.1984909705730407\n",
            "Epoch: 95 | Iteration: 28864 | Loss: 0.10808767921095695\n",
            "Epoch: 95 | Iteration: 28928 | Loss: 0.49321723335121087\n",
            "Epoch: 95 | Iteration: 28992 | Loss: 0.2175879293080354\n",
            "Epoch: 95 | Iteration: 29056 | Loss: 0.1718771496510214\n",
            "Epoch: 95 | Iteration: 29120 | Loss: 0.31368476027768166\n",
            "Epoch: 95 | Iteration: 29184 | Loss: 0.6346147202822339\n",
            "Epoch: 95 | Iteration: 29248 | Loss: 0.38130440495226736\n",
            "Epoch: 95 | Iteration: 29312 | Loss: 0.12104278939690707\n",
            "Epoch: 95 | Iteration: 29376 | Loss: 0.17892029296449022\n",
            "Epoch: 95 | Iteration: 29440 | Loss: 0.47421350767319526\n",
            "Epoch: 95 | Iteration: 29504 | Loss: 0.20663016641647508\n",
            "Epoch: 95 | Iteration: 29568 | Loss: 0.22636154897608268\n",
            "Epoch: 95 | Iteration: 29632 | Loss: 0.14600639776937388\n",
            "Epoch: 95 | Iteration: 29696 | Loss: 0.5507812434512542\n",
            "Epoch: 95 | Iteration: 29760 | Loss: 0.04878821355188686\n",
            "Epoch: 95 | Iteration: 29824 | Loss: 0.274562224770968\n",
            "Epoch: 95 | Iteration: 29888 | Loss: 0.5011958430257055\n",
            "Epoch: 95 | Iteration: 29952 | Loss: 0.06632256888719518\n",
            "Epoch: 95 | Iteration: 30016 | Loss: 0.6825353057587029\n",
            "Epoch: 95 | Iteration: 30080 | Loss: 0.23672659546303987\n",
            "Epoch: 95 | Iteration: 30144 | Loss: 0.2954752197134208\n",
            "Epoch: 95 | Iteration: 30208 | Loss: 0.1110232045172759\n",
            "Epoch: 95 | Iteration: 30272 | Loss: 0.05722163237930203\n",
            "Epoch: 95 | Iteration: 30336 | Loss: 0.23612556815965471\n",
            "Epoch: 95 | Iteration: 30400 | Loss: 0.05380244817740209\n",
            "Epoch: 95 | Iteration: 30464 | Loss: 0.3050372519908003\n",
            "Epoch: 95 | Iteration: 30528 | Loss: 0.1475891773880448\n",
            "Epoch: 95 | Iteration: 30592 | Loss: 0.1011414860257871\n",
            "Epoch: 95 | Iteration: 30656 | Loss: 0.5972010971263395\n",
            "Epoch: 95 | Iteration: 30720 | Loss: 0.25488974384540924\n",
            "Epoch: 95 | Iteration: 30784 | Loss: 0.19270629596622701\n",
            "Epoch: 95 | Iteration: 30848 | Loss: 0.3607390532177852\n",
            "Epoch: 95 | Iteration: 30912 | Loss: 0.7810535559654206\n",
            "Epoch: 95 | Iteration: 30976 | Loss: 0.18088521521370216\n",
            "Epoch: 95 | Iteration: 31040 | Loss: 0.49230665218174835\n",
            "Epoch: 95 | Iteration: 31104 | Loss: 0.4639272359118123\n",
            "Epoch: 95 | Iteration: 31168 | Loss: 1.7206296900236757\n",
            "Epoch: 95 | Iteration: 31232 | Loss: 0.18521506607730753\n",
            "Epoch: 95 | Iteration: 31296 | Loss: 0.8564373446291678\n",
            "Epoch: 95 | Iteration: 31360 | Loss: 0.28684821405944594\n",
            "Epoch: 95 | Iteration: 31424 | Loss: 0.32017850349752874\n",
            "Epoch: 95 | Iteration: 31488 | Loss: 0.08349840283563849\n",
            "Epoch: 95 | Iteration: 31552 | Loss: 1.1164767687480928\n",
            "Epoch: 95 | Iteration: 31616 | Loss: 0.3571436362977294\n",
            "Epoch: 95 | Iteration: 31680 | Loss: 1.3906738252495976\n",
            "Epoch: 95 | Iteration: 31744 | Loss: 0.7629453117059273\n",
            "Epoch: 95 | Iteration: 31808 | Loss: 0.2137899801792013\n",
            "Epoch: 95 | Iteration: 31872 | Loss: 0.1877334458180571\n",
            "Epoch: 95 | Iteration: 31936 | Loss: 0.2568623074108236\n",
            "Epoch: 95 | Iteration: 32000 | Loss: 0.2984807643512794\n",
            "Epoch: 95 | Iteration: 32064 | Loss: 0.3160302130273917\n",
            "Epoch: 95 | Iteration: 32128 | Loss: 0.16502771902246238\n",
            "Epoch: 95 | Iteration: 32192 | Loss: 0.47316931427810877\n",
            "Epoch: 95 | Iteration: 32256 | Loss: 0.22015539330650474\n",
            "Epoch: 95 | Iteration: 32320 | Loss: 0.7604790058031525\n",
            "Epoch: 95 | Iteration: 32384 | Loss: 0.3316665334777178\n",
            "Epoch: 95 | Iteration: 32448 | Loss: 1.0500964067191534\n",
            "Epoch: 95 | Iteration: 32512 | Loss: 0.30920651495452645\n",
            "Epoch: 95 | Iteration: 32576 | Loss: 0.0007368093870540101\n",
            "Epoch: 95 | Iteration: 32640 | Loss: 0.3412076279181929\n",
            "Epoch: 95 | Iteration: 32704 | Loss: 0.3067989221342866\n",
            "Epoch: 95 | Iteration: 32768 | Loss: 0.9237531661398914\n",
            "Epoch: 95 | Iteration: 32832 | Loss: 0.6912668537257542\n",
            "Epoch: 95 | Iteration: 32896 | Loss: 0.7014869556876187\n",
            "Epoch: 95 | Iteration: 32960 | Loss: 0.015294139070369743\n",
            "Epoch: 95 | Iteration: 33024 | Loss: 0.29434048890334663\n",
            "Epoch: 95 | Iteration: 33088 | Loss: 0.03490313008232519\n",
            "Epoch: 95 | Iteration: 33152 | Loss: 0.08364380944246874\n",
            "Epoch: 95 | Iteration: 33216 | Loss: 0.23051427760246307\n",
            "Epoch: 95 | Iteration: 33280 | Loss: 0.2809762229633964\n",
            "Epoch: 95 | Iteration: 33344 | Loss: 0.3628545683582946\n",
            "Epoch: 95 | Iteration: 33408 | Loss: 0.6389984477739352\n",
            "Epoch: 95 | Iteration: 33472 | Loss: 0.5534185817750855\n",
            "Epoch: 95 | Iteration: 33536 | Loss: 0.4421684310934412\n",
            "Epoch: 95 | Iteration: 33600 | Loss: 0.07283591907067925\n",
            "Epoch: 95 | Iteration: 33664 | Loss: 0.14661200111692557\n",
            "Epoch: 95 | Iteration: 33728 | Loss: 0.5042166839440123\n",
            "Epoch: 95 | Iteration: 33792 | Loss: 0.12191410095534427\n",
            "Epoch: 95 | Iteration: 33856 | Loss: 0.011583538725473647\n",
            "Epoch: 95 | Iteration: 33920 | Loss: 0.013379570439638581\n",
            "Epoch: 95 | Iteration: 33984 | Loss: 0.5354824429613955\n",
            "Epoch: 95 | Iteration: 34048 | Loss: 0.5076617152226967\n",
            "Epoch: 95 | Iteration: 34112 | Loss: 0.012877269622378666\n",
            "Epoch: 95 | Iteration: 34176 | Loss: 0.06207437915499337\n",
            "Epoch: 95 | Iteration: 34240 | Loss: 0.0030582914954028926\n",
            "Epoch: 95 | Iteration: 34304 | Loss: 0.10822461294006333\n",
            "Epoch: 95 | Iteration: 34368 | Loss: 1.3644740849613943\n",
            "Epoch: 95 | Iteration: 34432 | Loss: 0.16274991734892819\n",
            "Epoch: 95 | Iteration: 34496 | Loss: 0.17632362303621507\n",
            "Epoch: 95 | Iteration: 34560 | Loss: 0.13193745896666753\n",
            "Epoch: 95 | Iteration: 34624 | Loss: 0.6275724974385134\n",
            "Epoch: 95 | Iteration: 34688 | Loss: 0.8238280057854029\n",
            "Epoch: 95 | Iteration: 34752 | Loss: 0.3868968303451056\n",
            "Epoch: 95 | Iteration: 34816 | Loss: 0.800764613290302\n",
            "Epoch: 95 | Iteration: 34880 | Loss: 0.4853229099190742\n",
            "Epoch: 95 | Iteration: 34944 | Loss: 0.04469851841848678\n",
            "Epoch: 95 | Iteration: 35008 | Loss: 1.3437161537973465\n",
            "Epoch: 95 | Iteration: 35072 | Loss: 0.033318010862106825\n",
            "Epoch: 95 | Iteration: 35136 | Loss: 1.0142637934826884\n",
            "Epoch: 95 | Iteration: 35200 | Loss: 1.199831612270648\n",
            "Epoch: 95 | Iteration: 35264 | Loss: 0.1205597697889089\n",
            "Epoch: 95 | Iteration: 35328 | Loss: 0.1637114896738849\n",
            "Epoch: 95 | Iteration: 35392 | Loss: 0.23134320139379255\n",
            "Epoch: 95 | Iteration: 35456 | Loss: 1.161828857538714\n",
            "Epoch: 95 | Iteration: 35520 | Loss: 0.14380813098437134\n",
            "Epoch: 95 | Iteration: 35584 | Loss: 0.9945557526947154\n",
            "Epoch: 95 | Iteration: 35648 | Loss: 0.25844081555819065\n",
            "Epoch: 95 | Iteration: 35712 | Loss: 0.10565943000994418\n",
            "Epoch: 95 | Iteration: 35776 | Loss: 0.04509827917247571\n",
            "Epoch: 95 | Iteration: 35840 | Loss: 0.24669385801714255\n",
            "Epoch: 95 | Iteration: 35904 | Loss: 0.30014471384400576\n",
            "Epoch: 95 | Iteration: 35968 | Loss: 0.06889065580003687\n",
            "Epoch: 95 | Iteration: 36032 | Loss: 0.9652844263101783\n",
            "Epoch: 95 | Iteration: 36096 | Loss: 1.333132816508081\n",
            "Epoch: 95 | Iteration: 36160 | Loss: 0.43203095756963983\n",
            "Epoch: 95 | Iteration: 36224 | Loss: 0.11220505531108212\n",
            "Epoch: 95 | Iteration: 36288 | Loss: 0.021660781335282493\n",
            "Epoch: 95 | Iteration: 36352 | Loss: 0.19422999652533615\n",
            "Epoch: 95 | Iteration: 36416 | Loss: 0.7578948794946863\n",
            "Epoch: 95 | Iteration: 36480 | Loss: 0.021144715772734315\n",
            "Epoch: 95 | Iteration: 36544 | Loss: 0.24976438740759566\n",
            "Epoch: 95 | Iteration: 36608 | Loss: 0.06501926142454534\n",
            "Epoch: 95 | Iteration: 36672 | Loss: 0.2262551722828624\n",
            "Epoch: 95 | Iteration: 36736 | Loss: 0.9529863940241344\n",
            "Epoch: 95 | Iteration: 36800 | Loss: 0.4012714304712537\n",
            "Epoch: 95 | Iteration: 36864 | Loss: 0.15665000334229356\n",
            "Epoch: 95 | Iteration: 36928 | Loss: 0.34105641840408674\n",
            "Epoch: 95 | Iteration: 36992 | Loss: 1.0829062387041684\n",
            "Epoch: 95 | Iteration: 37056 | Loss: 0.3133039305129437\n",
            "Epoch: 95 | Iteration: 37120 | Loss: 0.4835926174757586\n",
            "Epoch: 95 | Iteration: 37184 | Loss: 0.11054701414746695\n",
            "Epoch: 95 | Iteration: 37248 | Loss: 0.39031392279531607\n",
            "Epoch: 95 | Iteration: 37312 | Loss: 1.015610652250718\n",
            "Epoch: 95 | Iteration: 37376 | Loss: 0.9650833140041386\n",
            "Epoch: 95 | Iteration: 37440 | Loss: 0.4967419362183014\n",
            "Epoch: 95 | Iteration: 37504 | Loss: 0.223858103907176\n",
            "Epoch: 95 | Iteration: 37568 | Loss: 0.05655745977187889\n",
            "Epoch: 95 | Iteration: 37632 | Loss: 0.2104856735062619\n",
            "Epoch: 95 | Iteration: 37696 | Loss: 0.6697183151632247\n",
            "Epoch: 95 | Iteration: 37760 | Loss: 0.7630371828248771\n",
            "Epoch: 95 | Iteration: 37824 | Loss: 1.0088902766013685\n",
            "Epoch: 95 | Iteration: 37888 | Loss: 0.06670511765958323\n",
            "Epoch: 95 | Iteration: 37952 | Loss: 0.04688743065240951\n",
            "Epoch: 95 | Iteration: 38016 | Loss: 0.16081501605541226\n",
            "Epoch: 95 | Iteration: 38080 | Loss: 0.06999945388561166\n",
            "Epoch: 95 | Iteration: 38144 | Loss: 0.16360651730740006\n",
            "Epoch: 95 | Iteration: 38208 | Loss: 0.06682826720973292\n",
            "Epoch: 95 | Iteration: 38272 | Loss: 0.5237310724033125\n",
            "Epoch: 95 | Iteration: 38336 | Loss: 0.7172025488333662\n",
            "Epoch: 95 | Iteration: 38400 | Loss: 0.14848876382800685\n",
            "Epoch: 95 | Iteration: 38464 | Loss: 0.624885551050496\n",
            "Epoch: 95 | Iteration: 38528 | Loss: 0.5493201759486914\n",
            "Epoch: 95 | Iteration: 38592 | Loss: 0.30625416241557313\n",
            "Epoch: 95 | Iteration: 38656 | Loss: 0.5855803140856735\n",
            "Epoch: 95 | Iteration: 38720 | Loss: 0.2165979019141338\n",
            "Epoch: 95 | Iteration: 38784 | Loss: 0.007672564767686687\n",
            "Epoch: 95 | Iteration: 38848 | Loss: 0.08353846810244449\n",
            "Epoch: 95 | Iteration: 38912 | Loss: 0.26288101205989317\n",
            "Epoch: 95 | Iteration: 38976 | Loss: 0.21123226895370933\n",
            "Epoch: 95 | Iteration: 39040 | Loss: 0.006905385196542407\n",
            "Epoch: 95 | Iteration: 39104 | Loss: 0.10076539492104253\n",
            "Epoch: 95 | Iteration: 39168 | Loss: 0.7353988818828214\n",
            "Epoch: 95 | Iteration: 39232 | Loss: 0.39443619491823395\n",
            "Epoch: 95 | Iteration: 39296 | Loss: 0.9420393013861252\n",
            "Epoch: 95 | Iteration: 39360 | Loss: 1.0614758983364494\n",
            "Epoch: 95 | Iteration: 39424 | Loss: 0.1874507144233178\n",
            "Epoch: 95 | Iteration: 39488 | Loss: 0.053075385590254154\n",
            "Epoch: 95 | Iteration: 39552 | Loss: 0.43364321714290266\n",
            "Epoch: 95 | Iteration: 39616 | Loss: 0.4797644047124717\n",
            "Epoch: 95 | Iteration: 39680 | Loss: 0.38483892164308675\n",
            "Epoch: 95 | Iteration: 39744 | Loss: 0.6829874856186378\n",
            "Epoch: 95 | Iteration: 39808 | Loss: 0.7187669061740785\n",
            "Epoch: 95 | Iteration: 39872 | Loss: 0.15526450018694565\n",
            "Epoch: 95 | Iteration: 39936 | Loss: 0.0631669565091942\n",
            "Epoch: 95 | Iteration: 40000 | Loss: 0.24415754515395566\n",
            "Epoch: 95 | Iteration: 40064 | Loss: 0.23227792690352733\n",
            "Epoch: 95 | Iteration: 40128 | Loss: 0.15528232757422006\n",
            "Epoch: 95 | Iteration: 40192 | Loss: 0.3190930037273078\n",
            "Epoch: 95 | Iteration: 40256 | Loss: 0.44184727136920016\n",
            "Epoch: 95 | Iteration: 40320 | Loss: 0.2067548199020869\n",
            "Epoch: 95 | Iteration: 40384 | Loss: 0.2343528321527077\n",
            "Epoch: 95 | Iteration: 40448 | Loss: 0.06740304400852905\n",
            "Epoch: 95 | Iteration: 40512 | Loss: 0.09831832763696081\n",
            "Epoch: 95 | Iteration: 40576 | Loss: 0.6727424918930338\n",
            "Epoch: 95 | Iteration: 40640 | Loss: 0.648066985528766\n",
            "Epoch: 95 | Iteration: 40704 | Loss: 0.5025114464793152\n",
            "Epoch: 95 | Iteration: 40768 | Loss: 0.20679209768319246\n",
            "Epoch: 95 | Iteration: 40832 | Loss: 0.13677444625667554\n",
            "Epoch: 95 | Iteration: 40896 | Loss: 0.08199998453696289\n",
            "Epoch: 95 | Iteration: 40960 | Loss: 0.7979226228543006\n",
            "Epoch: 95 | Iteration: 41024 | Loss: 0.4127577099772647\n",
            "Epoch: 95 | Iteration: 41088 | Loss: 0.14987977080954196\n",
            "Epoch: 95 | Iteration: 41152 | Loss: 0.8450633501924687\n",
            "Epoch: 95 | Iteration: 41216 | Loss: 1.0992150579036137\n",
            "Epoch: 95 | Iteration: 41280 | Loss: 0.8376618535945929\n",
            "Epoch: 95 | Iteration: 41344 | Loss: 0.30637725746431543\n",
            "Epoch: 95 | Iteration: 41408 | Loss: 0.29580888515037407\n",
            "Epoch: 95 | Iteration: 41472 | Loss: 0.041175237168898704\n",
            "Epoch: 95 | Iteration: 41536 | Loss: 0.9918111557025164\n",
            "Epoch: 95 | Iteration: 41600 | Loss: 0.13094624308385838\n",
            "Epoch: 95 | Iteration: 41664 | Loss: 0.07609911373929792\n",
            "Epoch: 95 | Iteration: 41728 | Loss: 0.27411990185254764\n",
            "Epoch: 95 | Iteration: 41792 | Loss: 0.1501415786273151\n",
            "Epoch: 95 | Iteration: 41856 | Loss: 0.49522273934277905\n",
            "Epoch: 95 | Iteration: 41920 | Loss: 0.359221515578009\n",
            "Epoch: 95 | Iteration: 41984 | Loss: 0.09361884293904392\n",
            "Epoch: 95 | Iteration: 42048 | Loss: 0.03474656263285227\n",
            "Epoch: 95 | Iteration: 42112 | Loss: 0.18886511430980896\n",
            "Epoch: 95 | Iteration: 42176 | Loss: 0.2023284617306977\n",
            "Epoch: 95 | Iteration: 42240 | Loss: 0.06863858574953996\n",
            "Epoch: 95 | Iteration: 42304 | Loss: 0.745964325700751\n",
            "Epoch: 95 | Iteration: 42368 | Loss: 0.5412953151158311\n",
            "Epoch: 95 | Iteration: 42432 | Loss: 0.8178341275604368\n",
            "Epoch: 95 | Iteration: 42496 | Loss: 0.5972102391290495\n",
            "Epoch: 95 | Iteration: 42560 | Loss: 0.9144217067030802\n",
            "Epoch: 95 | Iteration: 42624 | Loss: 0.6214388730619842\n",
            "Epoch: 95 | Iteration: 42688 | Loss: 0.3650638733153584\n",
            "Epoch: 95 | Iteration: 42752 | Loss: 1.0473252519065812\n",
            "Epoch: 95 | Iteration: 42816 | Loss: 1.0544204744037937\n",
            "Epoch: 95 | Iteration: 42880 | Loss: 0.7630893145416507\n",
            "Epoch: 95 | Iteration: 42944 | Loss: 0.4498978277329708\n",
            "Epoch: 95 | Iteration: 43008 | Loss: 0.6436344899988699\n",
            "Epoch: 95 | Iteration: 43072 | Loss: 1.3444853576950029\n",
            "Epoch: 95 | Iteration: 43136 | Loss: 0.04100737901888943\n",
            "Epoch: 95 | Iteration: 43200 | Loss: 0.09238414383880983\n",
            "Epoch: 95 | Iteration: 43264 | Loss: 0.0028677275119240054\n",
            "Epoch: 95 | Iteration: 43328 | Loss: 0.01664497655151356\n",
            "Epoch: 95 | Iteration: 43392 | Loss: 0.4120598774004631\n",
            "Epoch: 95 | Iteration: 43456 | Loss: 0.0862655414876061\n",
            "Epoch: 95 | Iteration: 43520 | Loss: 0.40963450676131385\n",
            "Epoch: 95 | Iteration: 43584 | Loss: 0.20812072829411646\n",
            "Epoch: 95 | Iteration: 43648 | Loss: 1.2940352265151063\n",
            "Epoch: 95 | Iteration: 43712 | Loss: 0.006098180008464519\n",
            "Epoch: 95 | Iteration: 43776 | Loss: 0.048990719602867765\n",
            "Epoch: 95 | Iteration: 43840 | Loss: 0.8107122427728288\n",
            "Epoch: 95 | Iteration: 43904 | Loss: 0.26513763593154716\n",
            "Epoch: 95 | Iteration: 43968 | Loss: 0.2079637525508872\n",
            "Epoch: 95 | Iteration: 44032 | Loss: 0.2549917933708536\n",
            "Epoch: 95 | Iteration: 44096 | Loss: 0.28721801646742184\n",
            "Epoch: 95 | Iteration: 44160 | Loss: 0.10543730165103854\n",
            "Epoch: 95 | Iteration: 44224 | Loss: 0.1882281090020449\n",
            "Epoch: 95 | Iteration: 44288 | Loss: 0.3556985305121957\n",
            "Epoch: 95 | Iteration: 44352 | Loss: 0.3344368641913429\n",
            "Epoch: 95 | Iteration: 44416 | Loss: 1.102093141139004\n",
            "Epoch: 95 | Iteration: 44480 | Loss: 0.3177182519255141\n",
            "Epoch: 95 | Iteration: 44544 | Loss: 0.03218258290758748\n",
            "Epoch: 95 | Iteration: 44608 | Loss: 0.42898147377924906\n",
            "Epoch: 95 | Iteration: 44672 | Loss: 0.06918120464050442\n",
            "Epoch: 95 | Iteration: 44736 | Loss: 0.03851534842224904\n",
            "Epoch: 95 | Iteration: 44800 | Loss: 0.4280484485673915\n",
            "Epoch: 95 | Iteration: 44864 | Loss: 0.19738859305620016\n",
            "Epoch: 95 | Iteration: 44928 | Loss: 0.6650242352440409\n",
            "Epoch: 95 | Iteration: 44992 | Loss: 0.17167803364891182\n",
            "Epoch: 95 | Iteration: 45056 | Loss: 0.16536584189109135\n",
            "Epoch: 95 | Iteration: 45120 | Loss: 0.8424998013937334\n",
            "Epoch: 95 | Iteration: 45184 | Loss: 0.9993322558564235\n",
            "Epoch: 95 | Iteration: 45248 | Loss: 0.35614452184433004\n",
            "Epoch: 95 | Iteration: 45312 | Loss: 0.5718433321416039\n",
            "Epoch: 95 | Iteration: 45376 | Loss: 0.321986486925743\n",
            "Epoch: 95 | Iteration: 45440 | Loss: 0.8062344040091427\n",
            "Epoch: 95 | Iteration: 45504 | Loss: 0.37656021421662533\n",
            "Epoch: 95 | Iteration: 45568 | Loss: 0.6265636163067982\n",
            "Epoch: 95 | Iteration: 45632 | Loss: 0.15001906939837012\n",
            "Epoch: 95 | Iteration: 45696 | Loss: 0.007458256956408976\n",
            "Epoch: 95 | Iteration: 45760 | Loss: 0.5605347805893373\n",
            "Epoch: 95 | Iteration: 45824 | Loss: 0.3422237974893799\n",
            "Epoch: 95 | Iteration: 45888 | Loss: 0.6800554557172356\n",
            "Epoch: 95 | Iteration: 45952 | Loss: 0.52999758874381\n",
            "Epoch: 95 | Iteration: 46016 | Loss: 1.0283227144000255\n",
            "Epoch: 95 | Iteration: 46080 | Loss: 1.2093836641238762\n",
            "Epoch: 95 | Iteration: 46144 | Loss: 0.07549640913792231\n",
            "Epoch: 95 | Iteration: 46208 | Loss: 1.404928161360143\n",
            "Epoch: 95 | Iteration: 46272 | Loss: 1.3381361055966996\n",
            "Epoch: 95 | Iteration: 46336 | Loss: 0.47293511032647484\n",
            "Epoch: 95 | Iteration: 46400 | Loss: 0.32524423130100694\n",
            "Epoch: 95 | Iteration: 46464 | Loss: 0.0172976265275999\n",
            "Epoch: 95 | Iteration: 46528 | Loss: 0.03982435252933424\n",
            "Epoch: 95 | Iteration: 46592 | Loss: 0.04589104212075934\n",
            "Epoch: 95 | Iteration: 46656 | Loss: 0.22754481409155744\n",
            "Epoch: 95 | Iteration: 46720 | Loss: 0.880206143935748\n",
            "Epoch: 95 | Iteration: 46784 | Loss: 0.04389342232125988\n",
            "Epoch: 95 | Iteration: 46848 | Loss: 0.7363495069519863\n",
            "Epoch: 95 | Iteration: 46912 | Loss: 0.32180050632773044\n",
            "Epoch: 95 | Iteration: 46976 | Loss: 1.2892158964398779\n",
            "Epoch: 95 | Iteration: 47040 | Loss: 0.04216312654347044\n",
            "Epoch: 95 | Iteration: 47104 | Loss: 0.3063326262397861\n",
            "Epoch: 95 | Iteration: 47168 | Loss: 0.33641449539770196\n",
            "Epoch: 95 | Iteration: 47232 | Loss: 1.3867544852621503\n",
            "Epoch: 95 | Iteration: 47296 | Loss: 0.35844362075599323\n",
            "Epoch: 95 | Iteration: 47360 | Loss: 0.8835938289547377\n",
            "Epoch: 95 | Iteration: 47424 | Loss: 0.2787718276275807\n",
            "Epoch: 95 | Iteration: 47488 | Loss: 0.09362055519187445\n",
            "Epoch: 95 | Iteration: 47552 | Loss: 1.2299986690428697\n",
            "Epoch: 95 | Iteration: 47616 | Loss: 0.01427618799529094\n",
            "Epoch: 95 | Iteration: 47680 | Loss: 1.387813971767983\n",
            "Epoch: 95 | Iteration: 47744 | Loss: 0.8467582478798181\n",
            "Epoch: 95 | Iteration: 47808 | Loss: 0.08280229792558269\n",
            "Epoch: 95 | Iteration: 47872 | Loss: 0.7692825895986052\n",
            "Epoch: 95 | Iteration: 47936 | Loss: 0.4842336570728666\n",
            "Epoch: 96 | Iteration: 0 | Loss: 0.1523019614124465\n",
            "Epoch: 96 | Iteration: 64 | Loss: 0.5789350340853081\n",
            "Epoch: 96 | Iteration: 128 | Loss: 1.1155439142192325\n",
            "Epoch: 96 | Iteration: 192 | Loss: 0.26440631023347105\n",
            "Epoch: 96 | Iteration: 256 | Loss: 0.17098035247708948\n",
            "Epoch: 96 | Iteration: 320 | Loss: 0.01646368838828852\n",
            "Epoch: 96 | Iteration: 384 | Loss: 0.12855249406701097\n",
            "Epoch: 96 | Iteration: 448 | Loss: 1.7829821602011904\n",
            "Epoch: 96 | Iteration: 512 | Loss: 0.4441259876901542\n",
            "Epoch: 96 | Iteration: 576 | Loss: 0.39503956324678796\n",
            "Epoch: 96 | Iteration: 640 | Loss: 0.3758250474659631\n",
            "Epoch: 96 | Iteration: 704 | Loss: 0.17917859821666027\n",
            "Epoch: 96 | Iteration: 768 | Loss: 0.45605753049739334\n",
            "Epoch: 96 | Iteration: 832 | Loss: 0.43520165233376684\n",
            "Epoch: 96 | Iteration: 896 | Loss: 0.8757487044783594\n",
            "Epoch: 96 | Iteration: 960 | Loss: 0.4960354886639868\n",
            "Epoch: 96 | Iteration: 1024 | Loss: 0.955427069600275\n",
            "Epoch: 96 | Iteration: 1088 | Loss: 0.7262577216062956\n",
            "Epoch: 96 | Iteration: 1152 | Loss: 0.30973265450017623\n",
            "Epoch: 96 | Iteration: 1216 | Loss: 0.8744784734191751\n",
            "Epoch: 96 | Iteration: 1280 | Loss: 0.17602216911315352\n",
            "Epoch: 96 | Iteration: 1344 | Loss: 1.5842705326843192\n",
            "Epoch: 96 | Iteration: 1408 | Loss: 0.10330547114015043\n",
            "Epoch: 96 | Iteration: 1472 | Loss: 0.5475575825816594\n",
            "Epoch: 96 | Iteration: 1536 | Loss: 0.04813231135978743\n",
            "Epoch: 96 | Iteration: 1600 | Loss: 1.0266640622346688\n",
            "Epoch: 96 | Iteration: 1664 | Loss: 0.0992553204682045\n",
            "Epoch: 96 | Iteration: 1728 | Loss: 0.15984436526923484\n",
            "Epoch: 96 | Iteration: 1792 | Loss: 0.18847012346536904\n",
            "Epoch: 96 | Iteration: 1856 | Loss: 0.17838461407728579\n",
            "Epoch: 96 | Iteration: 1920 | Loss: 0.8735799955468393\n",
            "Epoch: 96 | Iteration: 1984 | Loss: 0.20056757127845862\n",
            "Epoch: 96 | Iteration: 2048 | Loss: 0.3383710644629755\n",
            "Epoch: 96 | Iteration: 2112 | Loss: 0.036153355449099096\n",
            "Epoch: 96 | Iteration: 2176 | Loss: 0.3676378086077805\n",
            "Epoch: 96 | Iteration: 2240 | Loss: 0.09924372930785984\n",
            "Epoch: 96 | Iteration: 2304 | Loss: 0.12948590322123446\n",
            "Epoch: 96 | Iteration: 2368 | Loss: 0.2017514664497081\n",
            "Epoch: 96 | Iteration: 2432 | Loss: 0.013013428547805067\n",
            "Epoch: 96 | Iteration: 2496 | Loss: 0.13291607718953302\n",
            "Epoch: 96 | Iteration: 2560 | Loss: 0.41306544199966155\n",
            "Epoch: 96 | Iteration: 2624 | Loss: 0.8611231326956148\n",
            "Epoch: 96 | Iteration: 2688 | Loss: 0.9020837123411873\n",
            "Epoch: 96 | Iteration: 2752 | Loss: 0.06685082094784778\n",
            "Epoch: 96 | Iteration: 2816 | Loss: 0.1299012767572603\n",
            "Epoch: 96 | Iteration: 2880 | Loss: 0.3270192946067241\n",
            "Epoch: 96 | Iteration: 2944 | Loss: 0.1010328897951249\n",
            "Epoch: 96 | Iteration: 3008 | Loss: 0.30622516180511444\n",
            "Epoch: 96 | Iteration: 3072 | Loss: 0.08510705166883993\n",
            "Epoch: 96 | Iteration: 3136 | Loss: 0.0014768738100424672\n",
            "Epoch: 96 | Iteration: 3200 | Loss: 0.09805135424053002\n",
            "Epoch: 96 | Iteration: 3264 | Loss: 0.07413289361466124\n",
            "Epoch: 96 | Iteration: 3328 | Loss: 0.0688312746662114\n",
            "Epoch: 96 | Iteration: 3392 | Loss: 0.05477814368537458\n",
            "Epoch: 96 | Iteration: 3456 | Loss: 0.23096363416744878\n",
            "Epoch: 96 | Iteration: 3520 | Loss: 0.15692881084112417\n",
            "Epoch: 96 | Iteration: 3584 | Loss: 0.07131680879111706\n",
            "Epoch: 96 | Iteration: 3648 | Loss: 0.5274843929141458\n",
            "Epoch: 96 | Iteration: 3712 | Loss: 0.10865622772728167\n",
            "Epoch: 96 | Iteration: 3776 | Loss: 0.3444653506097778\n",
            "Epoch: 96 | Iteration: 3840 | Loss: 0.006935055941281396\n",
            "Epoch: 96 | Iteration: 3904 | Loss: 0.20373368422175306\n",
            "Epoch: 96 | Iteration: 3968 | Loss: 0.07574078347516539\n",
            "Epoch: 96 | Iteration: 4032 | Loss: 0.15237281093855654\n",
            "Epoch: 96 | Iteration: 4096 | Loss: 0.2814696345539316\n",
            "Epoch: 96 | Iteration: 4160 | Loss: 0.20606514985612234\n",
            "Epoch: 96 | Iteration: 4224 | Loss: 0.5122189561665087\n",
            "Epoch: 96 | Iteration: 4288 | Loss: 0.09799209925548114\n",
            "Epoch: 96 | Iteration: 4352 | Loss: 0.29991754773440926\n",
            "Epoch: 96 | Iteration: 4416 | Loss: 0.9520447003825447\n",
            "Epoch: 96 | Iteration: 4480 | Loss: 0.06807701359632894\n",
            "Epoch: 96 | Iteration: 4544 | Loss: 0.04967357031139159\n",
            "Epoch: 96 | Iteration: 4608 | Loss: 0.7157297723278896\n",
            "Epoch: 96 | Iteration: 4672 | Loss: 0.060499605295416134\n",
            "Epoch: 96 | Iteration: 4736 | Loss: 0.09441537522922713\n",
            "Epoch: 96 | Iteration: 4800 | Loss: 0.16437874458943427\n",
            "Epoch: 96 | Iteration: 4864 | Loss: 0.02944577960674106\n",
            "Epoch: 96 | Iteration: 4928 | Loss: 0.39157012970833277\n",
            "Epoch: 96 | Iteration: 4992 | Loss: 0.7495083573794573\n",
            "Epoch: 96 | Iteration: 5056 | Loss: 0.5229789281168243\n",
            "Epoch: 96 | Iteration: 5120 | Loss: 0.7786367587955418\n",
            "Epoch: 96 | Iteration: 5184 | Loss: 0.2904098485896874\n",
            "Epoch: 96 | Iteration: 5248 | Loss: 0.34029226191960765\n",
            "Epoch: 96 | Iteration: 5312 | Loss: 0.3729303579778621\n",
            "Epoch: 96 | Iteration: 5376 | Loss: 0.1105002131862419\n",
            "Epoch: 96 | Iteration: 5440 | Loss: 0.11947711875982091\n",
            "Epoch: 96 | Iteration: 5504 | Loss: 0.31284034376334546\n",
            "Epoch: 96 | Iteration: 5568 | Loss: 0.16720731814565887\n",
            "Epoch: 96 | Iteration: 5632 | Loss: 0.16282505645076784\n",
            "Epoch: 96 | Iteration: 5696 | Loss: 0.8866639527810005\n",
            "Epoch: 96 | Iteration: 5760 | Loss: 1.2024934999349968\n",
            "Epoch: 96 | Iteration: 5824 | Loss: 0.1725088416519824\n",
            "Epoch: 96 | Iteration: 5888 | Loss: 1.0155044994499205\n",
            "Epoch: 96 | Iteration: 5952 | Loss: 0.04986670894737426\n",
            "Epoch: 96 | Iteration: 6016 | Loss: 0.07700198253435994\n",
            "Epoch: 96 | Iteration: 6080 | Loss: 0.47707123452834443\n",
            "Epoch: 96 | Iteration: 6144 | Loss: 0.39718126666385783\n",
            "Epoch: 96 | Iteration: 6208 | Loss: 0.48811549957632183\n",
            "Epoch: 96 | Iteration: 6272 | Loss: 0.03570667148589423\n",
            "Epoch: 96 | Iteration: 6336 | Loss: 0.18313299087612844\n",
            "Epoch: 96 | Iteration: 6400 | Loss: 1.0310098441502993\n",
            "Epoch: 96 | Iteration: 6464 | Loss: 0.061752341879373326\n",
            "Epoch: 96 | Iteration: 6528 | Loss: 0.25619340997123585\n",
            "Epoch: 96 | Iteration: 6592 | Loss: 0.02377255371073972\n",
            "Epoch: 96 | Iteration: 6656 | Loss: 0.26877741875779126\n",
            "Epoch: 96 | Iteration: 6720 | Loss: 0.05395703021165668\n",
            "Epoch: 96 | Iteration: 6784 | Loss: 0.921683141323552\n",
            "Epoch: 96 | Iteration: 6848 | Loss: 1.7441144683919116\n",
            "Epoch: 96 | Iteration: 6912 | Loss: 0.44045668137013727\n",
            "Epoch: 96 | Iteration: 6976 | Loss: 0.5565812667668317\n",
            "Epoch: 96 | Iteration: 7040 | Loss: 1.0763421807168028\n",
            "Epoch: 96 | Iteration: 7104 | Loss: 0.22669301139239278\n",
            "Epoch: 96 | Iteration: 7168 | Loss: 0.1253825618508585\n",
            "Epoch: 96 | Iteration: 7232 | Loss: 0.878645836376259\n",
            "Epoch: 96 | Iteration: 7296 | Loss: 0.760093578678116\n",
            "Epoch: 96 | Iteration: 7360 | Loss: 0.0459776739118942\n",
            "Epoch: 96 | Iteration: 7424 | Loss: 0.08803652531862217\n",
            "Epoch: 96 | Iteration: 7488 | Loss: 0.5342349410281433\n",
            "Epoch: 96 | Iteration: 7552 | Loss: 0.6877045439024858\n",
            "Epoch: 96 | Iteration: 7616 | Loss: 0.19128176251604773\n",
            "Epoch: 96 | Iteration: 7680 | Loss: 0.5774909250589056\n",
            "Epoch: 96 | Iteration: 7744 | Loss: 0.3351187578795094\n",
            "Epoch: 96 | Iteration: 7808 | Loss: 0.4971836768433895\n",
            "Epoch: 96 | Iteration: 7872 | Loss: 0.5833302310239542\n",
            "Epoch: 96 | Iteration: 7936 | Loss: 0.2929509356770504\n",
            "Epoch: 96 | Iteration: 8000 | Loss: 0.10730076620557917\n",
            "Epoch: 96 | Iteration: 8064 | Loss: 0.3177847212739593\n",
            "Epoch: 96 | Iteration: 8128 | Loss: 0.10253593368797545\n",
            "Epoch: 96 | Iteration: 8192 | Loss: 1.8458080933375767\n",
            "Epoch: 96 | Iteration: 8256 | Loss: 0.1445011116354288\n",
            "Epoch: 96 | Iteration: 8320 | Loss: 0.33939280123448884\n",
            "Epoch: 96 | Iteration: 8384 | Loss: 0.35290723135326485\n",
            "Epoch: 96 | Iteration: 8448 | Loss: 0.6172237407884795\n",
            "Epoch: 96 | Iteration: 8512 | Loss: 0.06432589255063424\n",
            "Epoch: 96 | Iteration: 8576 | Loss: 0.15828069171995718\n",
            "Epoch: 96 | Iteration: 8640 | Loss: 0.2003633430649548\n",
            "Epoch: 96 | Iteration: 8704 | Loss: 1.0944339788818886\n",
            "Epoch: 96 | Iteration: 8768 | Loss: 0.5656087514469877\n",
            "Epoch: 96 | Iteration: 8832 | Loss: 1.0918684919418353\n",
            "Epoch: 96 | Iteration: 8896 | Loss: 0.5622610027356013\n",
            "Epoch: 96 | Iteration: 8960 | Loss: 0.2160463785678911\n",
            "Epoch: 96 | Iteration: 9024 | Loss: 0.19283201616469509\n",
            "Epoch: 96 | Iteration: 9088 | Loss: 0.23907391516243323\n",
            "Epoch: 96 | Iteration: 9152 | Loss: 0.087611962549765\n",
            "Epoch: 96 | Iteration: 9216 | Loss: 0.2596626335820875\n",
            "Epoch: 96 | Iteration: 9280 | Loss: 0.8102194233755633\n",
            "Epoch: 96 | Iteration: 9344 | Loss: 0.21370407453652235\n",
            "Epoch: 96 | Iteration: 9408 | Loss: 0.616256313410189\n",
            "Epoch: 96 | Iteration: 9472 | Loss: 0.5440864905215034\n",
            "Epoch: 96 | Iteration: 9536 | Loss: 0.08336320136403659\n",
            "Epoch: 96 | Iteration: 9600 | Loss: 0.15047143084979336\n",
            "Epoch: 96 | Iteration: 9664 | Loss: 0.0031786077650928788\n",
            "Epoch: 96 | Iteration: 9728 | Loss: 0.4325173602659033\n",
            "Epoch: 96 | Iteration: 9792 | Loss: 0.0895173995754031\n",
            "Epoch: 96 | Iteration: 9856 | Loss: 0.023061713731385297\n",
            "Epoch: 96 | Iteration: 9920 | Loss: 0.3235675218893067\n",
            "Epoch: 96 | Iteration: 9984 | Loss: 0.14326149738735036\n",
            "Epoch: 96 | Iteration: 10048 | Loss: 0.34117747642376184\n",
            "Epoch: 96 | Iteration: 10112 | Loss: 0.5349643566251956\n",
            "Epoch: 96 | Iteration: 10176 | Loss: 0.636186895863159\n",
            "Epoch: 96 | Iteration: 10240 | Loss: 0.5219434940222059\n",
            "Epoch: 96 | Iteration: 10304 | Loss: 0.02668485880930327\n",
            "Epoch: 96 | Iteration: 10368 | Loss: 0.12496493190241287\n",
            "Epoch: 96 | Iteration: 10432 | Loss: 0.0423271675860821\n",
            "Epoch: 96 | Iteration: 10496 | Loss: 0.02687880262245662\n",
            "Epoch: 96 | Iteration: 10560 | Loss: 0.03430248594087731\n",
            "Epoch: 96 | Iteration: 10624 | Loss: 0.14457727690822142\n",
            "Epoch: 96 | Iteration: 10688 | Loss: 0.10013447081887758\n",
            "Epoch: 96 | Iteration: 10752 | Loss: 0.37378826320497865\n",
            "Epoch: 96 | Iteration: 10816 | Loss: 0.18496253565794096\n",
            "Epoch: 96 | Iteration: 10880 | Loss: 0.16410947860513236\n",
            "Epoch: 96 | Iteration: 10944 | Loss: 1.1288300850742368\n",
            "Epoch: 96 | Iteration: 11008 | Loss: 0.22515389140979294\n",
            "Epoch: 96 | Iteration: 11072 | Loss: 0.3208274588982677\n",
            "Epoch: 96 | Iteration: 11136 | Loss: 0.08392005769110997\n",
            "Epoch: 96 | Iteration: 11200 | Loss: 0.25849667750617833\n",
            "Epoch: 96 | Iteration: 11264 | Loss: 0.02856592670307868\n",
            "Epoch: 96 | Iteration: 11328 | Loss: 0.09039018948776595\n",
            "Epoch: 96 | Iteration: 11392 | Loss: 0.026957962910760032\n",
            "Epoch: 96 | Iteration: 11456 | Loss: 0.0903641454319965\n",
            "Epoch: 96 | Iteration: 11520 | Loss: 0.47812649209802194\n",
            "Epoch: 96 | Iteration: 11584 | Loss: 0.48760513110503356\n",
            "Epoch: 96 | Iteration: 11648 | Loss: 0.6704621670321365\n",
            "Epoch: 96 | Iteration: 11712 | Loss: 0.7529199615166343\n",
            "Epoch: 96 | Iteration: 11776 | Loss: 0.18340920941942904\n",
            "Epoch: 96 | Iteration: 11840 | Loss: 0.4745798204725124\n",
            "Epoch: 96 | Iteration: 11904 | Loss: 0.5068200554192989\n",
            "Epoch: 96 | Iteration: 11968 | Loss: 0.1690045414103254\n",
            "Epoch: 96 | Iteration: 12032 | Loss: 0.08654914659674526\n",
            "Epoch: 96 | Iteration: 12096 | Loss: 0.03280259862714453\n",
            "Epoch: 96 | Iteration: 12160 | Loss: 0.11986494741471573\n",
            "Epoch: 96 | Iteration: 12224 | Loss: 0.13130153719251078\n",
            "Epoch: 96 | Iteration: 12288 | Loss: 0.22885819047566652\n",
            "Epoch: 96 | Iteration: 12352 | Loss: 0.1684401201833277\n",
            "Epoch: 96 | Iteration: 12416 | Loss: 0.09306845014907637\n",
            "Epoch: 96 | Iteration: 12480 | Loss: 0.05806841599998409\n",
            "Epoch: 96 | Iteration: 12544 | Loss: 0.6812974534501692\n",
            "Epoch: 96 | Iteration: 12608 | Loss: 0.14998601497306727\n",
            "Epoch: 96 | Iteration: 12672 | Loss: 0.626355831910282\n",
            "Epoch: 96 | Iteration: 12736 | Loss: 0.0465632294047057\n",
            "Epoch: 96 | Iteration: 12800 | Loss: 0.6190001903811146\n",
            "Epoch: 96 | Iteration: 12864 | Loss: 0.14766279270839153\n",
            "Epoch: 96 | Iteration: 12928 | Loss: 0.22803342986022862\n",
            "Epoch: 96 | Iteration: 12992 | Loss: 0.1202938159290193\n",
            "Epoch: 96 | Iteration: 13056 | Loss: 0.09974996857451848\n",
            "Epoch: 96 | Iteration: 13120 | Loss: 0.06543979420247065\n",
            "Epoch: 96 | Iteration: 13184 | Loss: 0.4759570659691098\n",
            "Epoch: 96 | Iteration: 13248 | Loss: 0.05951342805966293\n",
            "Epoch: 96 | Iteration: 13312 | Loss: 0.015347437475894206\n",
            "Epoch: 96 | Iteration: 13376 | Loss: 0.48042031439100075\n",
            "Epoch: 96 | Iteration: 13440 | Loss: 0.06421086430696796\n",
            "Epoch: 96 | Iteration: 13504 | Loss: 0.23065815533349435\n",
            "Epoch: 96 | Iteration: 13568 | Loss: 0.09339331047012499\n",
            "Epoch: 96 | Iteration: 13632 | Loss: 0.35390530858943026\n",
            "Epoch: 96 | Iteration: 13696 | Loss: 0.4823225200788781\n",
            "Epoch: 96 | Iteration: 13760 | Loss: 0.024053143726673394\n",
            "Epoch: 96 | Iteration: 13824 | Loss: 0.25779715371339396\n",
            "Epoch: 96 | Iteration: 13888 | Loss: 0.12946023921255706\n",
            "Epoch: 96 | Iteration: 13952 | Loss: 0.8173512622088548\n",
            "Epoch: 96 | Iteration: 14016 | Loss: 0.24177147015730596\n",
            "Epoch: 96 | Iteration: 14080 | Loss: 0.24712666274814515\n",
            "Epoch: 96 | Iteration: 14144 | Loss: 0.2195332095213355\n",
            "Epoch: 96 | Iteration: 14208 | Loss: 0.5914601974546544\n",
            "Epoch: 96 | Iteration: 14272 | Loss: 0.4393647437045314\n",
            "Epoch: 96 | Iteration: 14336 | Loss: 0.2573932128458275\n",
            "Epoch: 96 | Iteration: 14400 | Loss: 0.1572944609857477\n",
            "Epoch: 96 | Iteration: 14464 | Loss: 0.08467654154172907\n",
            "Epoch: 96 | Iteration: 14528 | Loss: 0.31126077406014224\n",
            "Epoch: 96 | Iteration: 14592 | Loss: 0.18128499063431697\n",
            "Epoch: 96 | Iteration: 14656 | Loss: 0.0898720198608246\n",
            "Epoch: 96 | Iteration: 14720 | Loss: 0.7526735260059518\n",
            "Epoch: 96 | Iteration: 14784 | Loss: 0.7761393961155805\n",
            "Epoch: 96 | Iteration: 14848 | Loss: 0.22497788373269717\n",
            "Epoch: 96 | Iteration: 14912 | Loss: 0.20589561278109275\n",
            "Epoch: 96 | Iteration: 14976 | Loss: 0.03936994324479842\n",
            "Epoch: 96 | Iteration: 15040 | Loss: 0.1397946667265157\n",
            "Epoch: 96 | Iteration: 15104 | Loss: 0.6384633888248983\n",
            "Epoch: 96 | Iteration: 15168 | Loss: 0.1346159716169173\n",
            "Epoch: 96 | Iteration: 15232 | Loss: 0.1327560255513072\n",
            "Epoch: 96 | Iteration: 15296 | Loss: 0.46918042701127105\n",
            "Epoch: 96 | Iteration: 15360 | Loss: 0.36670759830331734\n",
            "Epoch: 96 | Iteration: 15424 | Loss: 0.06903485408077009\n",
            "Epoch: 96 | Iteration: 15488 | Loss: 0.09774011730863741\n",
            "Epoch: 96 | Iteration: 15552 | Loss: 0.26722237889625244\n",
            "Epoch: 96 | Iteration: 15616 | Loss: 0.05265085361452273\n",
            "Epoch: 96 | Iteration: 15680 | Loss: 0.21567921005981\n",
            "Epoch: 96 | Iteration: 15744 | Loss: 0.4192453056586903\n",
            "Epoch: 96 | Iteration: 15808 | Loss: 0.5040200551955658\n",
            "Epoch: 96 | Iteration: 15872 | Loss: 0.25172157964222974\n",
            "Epoch: 96 | Iteration: 15936 | Loss: 1.0289051276695864\n",
            "Epoch: 96 | Iteration: 16000 | Loss: 0.502936873267052\n",
            "Epoch: 96 | Iteration: 16064 | Loss: 0.06103868018528096\n",
            "Epoch: 96 | Iteration: 16128 | Loss: 0.3363249642097024\n",
            "Epoch: 96 | Iteration: 16192 | Loss: 0.011294112365282296\n",
            "Epoch: 96 | Iteration: 16256 | Loss: 0.22004111138938381\n",
            "Epoch: 96 | Iteration: 16320 | Loss: 0.16572842415765907\n",
            "Epoch: 96 | Iteration: 16384 | Loss: 0.4114624071016778\n",
            "Epoch: 96 | Iteration: 16448 | Loss: 0.1651434616808281\n",
            "Epoch: 96 | Iteration: 16512 | Loss: 0.7548551825343183\n",
            "Epoch: 96 | Iteration: 16576 | Loss: 0.05724262076506817\n",
            "Epoch: 96 | Iteration: 16640 | Loss: 0.5197550961138659\n",
            "Epoch: 96 | Iteration: 16704 | Loss: 0.31057639367039774\n",
            "Epoch: 96 | Iteration: 16768 | Loss: 0.3602439475169724\n",
            "Epoch: 96 | Iteration: 16832 | Loss: 0.43418673309938305\n",
            "Epoch: 96 | Iteration: 16896 | Loss: 0.36793762771216776\n",
            "Epoch: 96 | Iteration: 16960 | Loss: 0.2983484999006704\n",
            "Epoch: 96 | Iteration: 17024 | Loss: 0.2987795256219864\n",
            "Epoch: 96 | Iteration: 17088 | Loss: 0.5454550941516048\n",
            "Epoch: 96 | Iteration: 17152 | Loss: 0.4148658848044612\n",
            "Epoch: 96 | Iteration: 17216 | Loss: 0.4976931505052097\n",
            "Epoch: 96 | Iteration: 17280 | Loss: 0.012905647462575763\n",
            "Epoch: 96 | Iteration: 17344 | Loss: 0.42326583397081086\n",
            "Epoch: 96 | Iteration: 17408 | Loss: 0.1301581991376037\n",
            "Epoch: 96 | Iteration: 17472 | Loss: 0.19680137262419323\n",
            "Epoch: 96 | Iteration: 17536 | Loss: 0.63586806576361\n",
            "Epoch: 96 | Iteration: 17600 | Loss: 0.19932778447679458\n",
            "Epoch: 96 | Iteration: 17664 | Loss: 0.3826245283349094\n",
            "Epoch: 96 | Iteration: 17728 | Loss: 1.4989666025800554\n",
            "Epoch: 96 | Iteration: 17792 | Loss: 0.21467750011128164\n",
            "Epoch: 96 | Iteration: 17856 | Loss: 0.40328702266526517\n",
            "Epoch: 96 | Iteration: 17920 | Loss: 0.04213136375243853\n",
            "Epoch: 96 | Iteration: 17984 | Loss: 0.08084584034308903\n",
            "Epoch: 96 | Iteration: 18048 | Loss: 0.469219076510938\n",
            "Epoch: 96 | Iteration: 18112 | Loss: 0.21866976865162435\n",
            "Epoch: 96 | Iteration: 18176 | Loss: 0.10446094166134387\n",
            "Epoch: 96 | Iteration: 18240 | Loss: 0.6060385146145886\n",
            "Epoch: 96 | Iteration: 18304 | Loss: 0.4407665960244964\n",
            "Epoch: 96 | Iteration: 18368 | Loss: 0.3502400799281744\n",
            "Epoch: 96 | Iteration: 18432 | Loss: 0.7124435932200288\n",
            "Epoch: 96 | Iteration: 18496 | Loss: 0.16006199123926346\n",
            "Epoch: 96 | Iteration: 18560 | Loss: 0.19855809246509026\n",
            "Epoch: 96 | Iteration: 18624 | Loss: 0.07228653552965933\n",
            "Epoch: 96 | Iteration: 18688 | Loss: 0.35530929754868223\n",
            "Epoch: 96 | Iteration: 18752 | Loss: 0.07793314010779749\n",
            "Epoch: 96 | Iteration: 18816 | Loss: 0.08491233895185286\n",
            "Epoch: 96 | Iteration: 18880 | Loss: 0.21564786372799763\n",
            "Epoch: 96 | Iteration: 18944 | Loss: 0.1980375510067849\n",
            "Epoch: 96 | Iteration: 19008 | Loss: 0.1407993253201037\n",
            "Epoch: 96 | Iteration: 19072 | Loss: 1.337617337406123\n",
            "Epoch: 96 | Iteration: 19136 | Loss: 0.15643181694601727\n",
            "Epoch: 96 | Iteration: 19200 | Loss: 0.18078665343297123\n",
            "Epoch: 96 | Iteration: 19264 | Loss: 0.501225261350689\n",
            "Epoch: 96 | Iteration: 19328 | Loss: 1.585570338902698\n",
            "Epoch: 96 | Iteration: 19392 | Loss: 0.31746776896845286\n",
            "Epoch: 96 | Iteration: 19456 | Loss: 0.24438952301369832\n",
            "Epoch: 96 | Iteration: 19520 | Loss: 0.054819768544726345\n",
            "Epoch: 96 | Iteration: 19584 | Loss: 0.11804795095854985\n",
            "Epoch: 96 | Iteration: 19648 | Loss: 0.10816627392335945\n",
            "Epoch: 96 | Iteration: 19712 | Loss: 0.014153199003263946\n",
            "Epoch: 96 | Iteration: 19776 | Loss: 0.8621926889302631\n",
            "Epoch: 96 | Iteration: 19840 | Loss: 0.39970395029020006\n",
            "Epoch: 96 | Iteration: 19904 | Loss: 0.372625051080713\n",
            "Epoch: 96 | Iteration: 19968 | Loss: 0.3005388174378877\n",
            "Epoch: 96 | Iteration: 20032 | Loss: 0.2639604768327374\n",
            "Epoch: 96 | Iteration: 20096 | Loss: 0.09247985717084911\n",
            "Epoch: 96 | Iteration: 20160 | Loss: 0.8734121197443777\n",
            "Epoch: 96 | Iteration: 20224 | Loss: 0.4435830981165579\n",
            "Epoch: 96 | Iteration: 20288 | Loss: 0.8164439207540937\n",
            "Epoch: 96 | Iteration: 20352 | Loss: 0.0066172295556905705\n",
            "Epoch: 96 | Iteration: 20416 | Loss: 0.015354698002819371\n",
            "Epoch: 96 | Iteration: 20480 | Loss: 0.1020425391900179\n",
            "Epoch: 96 | Iteration: 20544 | Loss: 0.20570522848237183\n",
            "Epoch: 96 | Iteration: 20608 | Loss: 0.1273161019866664\n",
            "Epoch: 96 | Iteration: 20672 | Loss: 1.772058708551282\n",
            "Epoch: 96 | Iteration: 20736 | Loss: 1.1281703577678353\n",
            "Epoch: 96 | Iteration: 20800 | Loss: 0.3850230037846732\n",
            "Epoch: 96 | Iteration: 20864 | Loss: 0.34454658235466656\n",
            "Epoch: 96 | Iteration: 20928 | Loss: 0.2870740871020932\n",
            "Epoch: 96 | Iteration: 20992 | Loss: 0.3949005301150731\n",
            "Epoch: 96 | Iteration: 21056 | Loss: 0.27621081323984614\n",
            "Epoch: 96 | Iteration: 21120 | Loss: 0.3465490596033214\n",
            "Epoch: 96 | Iteration: 21184 | Loss: 0.21431489764140524\n",
            "Epoch: 96 | Iteration: 21248 | Loss: 0.20245278575689374\n",
            "Epoch: 96 | Iteration: 21312 | Loss: 0.435115715888332\n",
            "Epoch: 96 | Iteration: 21376 | Loss: 0.17611938552664877\n",
            "Epoch: 96 | Iteration: 21440 | Loss: 0.5952102269003647\n",
            "Epoch: 96 | Iteration: 21504 | Loss: 0.19545740328225367\n",
            "Epoch: 96 | Iteration: 21568 | Loss: 1.4011298754201933\n",
            "Epoch: 96 | Iteration: 21632 | Loss: 0.2398277654234435\n",
            "Epoch: 96 | Iteration: 21696 | Loss: 0.10469449075103948\n",
            "Epoch: 96 | Iteration: 21760 | Loss: 0.006051115813466656\n",
            "Epoch: 96 | Iteration: 21824 | Loss: 0.0035882515770964685\n",
            "Epoch: 96 | Iteration: 21888 | Loss: 0.5476403155302005\n",
            "Epoch: 96 | Iteration: 21952 | Loss: 0.3702158635226537\n",
            "Epoch: 96 | Iteration: 22016 | Loss: 0.07489002800940225\n",
            "Epoch: 96 | Iteration: 22080 | Loss: 0.4232759187850771\n",
            "Epoch: 96 | Iteration: 22144 | Loss: 0.35655741759260057\n",
            "Epoch: 96 | Iteration: 22208 | Loss: 0.23871344146095758\n",
            "Epoch: 96 | Iteration: 22272 | Loss: 0.728141402998265\n",
            "Epoch: 96 | Iteration: 22336 | Loss: 0.03935654672163447\n",
            "Epoch: 96 | Iteration: 22400 | Loss: 0.19599447985884588\n",
            "Epoch: 96 | Iteration: 22464 | Loss: 0.6092252748708255\n",
            "Epoch: 96 | Iteration: 22528 | Loss: 0.9432748037496693\n",
            "Epoch: 96 | Iteration: 22592 | Loss: 0.47448577251744806\n",
            "Epoch: 96 | Iteration: 22656 | Loss: 0.12651269284162964\n",
            "Epoch: 96 | Iteration: 22720 | Loss: 0.5673337144471248\n",
            "Epoch: 96 | Iteration: 22784 | Loss: 0.16022098962929474\n",
            "Epoch: 96 | Iteration: 22848 | Loss: 0.2675037570743362\n",
            "Epoch: 96 | Iteration: 22912 | Loss: 0.0017342681664203516\n",
            "Epoch: 96 | Iteration: 22976 | Loss: 0.2688831781477133\n",
            "Epoch: 96 | Iteration: 23040 | Loss: 0.22892816699779833\n",
            "Epoch: 96 | Iteration: 23104 | Loss: 0.122262498571284\n",
            "Epoch: 96 | Iteration: 23168 | Loss: 0.5007005181121259\n",
            "Epoch: 96 | Iteration: 23232 | Loss: 0.024682596189574423\n",
            "Epoch: 96 | Iteration: 23296 | Loss: 0.11732568646124612\n",
            "Epoch: 96 | Iteration: 23360 | Loss: 0.3102768551165178\n",
            "Epoch: 96 | Iteration: 23424 | Loss: 0.08796741241657689\n",
            "Epoch: 96 | Iteration: 23488 | Loss: 0.08845535953072325\n",
            "Epoch: 96 | Iteration: 23552 | Loss: 0.6350350965879585\n",
            "Epoch: 96 | Iteration: 23616 | Loss: 0.7074122638065967\n",
            "Epoch: 96 | Iteration: 23680 | Loss: 0.8425365924329501\n",
            "Epoch: 96 | Iteration: 23744 | Loss: 0.012556070101774196\n",
            "Epoch: 96 | Iteration: 23808 | Loss: 0.3780034947890975\n",
            "Epoch: 96 | Iteration: 23872 | Loss: 0.6080968610055594\n",
            "Epoch: 96 | Iteration: 23936 | Loss: 1.0191201515339667\n",
            "Epoch: 96 | Iteration: 24000 | Loss: 0.16239653926668077\n",
            "Epoch: 96 | Iteration: 24064 | Loss: 0.07669892230516161\n",
            "Epoch: 96 | Iteration: 24128 | Loss: 0.15055648139302047\n",
            "Epoch: 96 | Iteration: 24192 | Loss: 0.2771393998266827\n",
            "Epoch: 96 | Iteration: 24256 | Loss: 0.7084060114211509\n",
            "Epoch: 96 | Iteration: 24320 | Loss: 0.38956289537067124\n",
            "Epoch: 96 | Iteration: 24384 | Loss: 0.009639184635132093\n",
            "Epoch: 96 | Iteration: 24448 | Loss: 0.22528643523647526\n",
            "Epoch: 96 | Iteration: 24512 | Loss: 0.19369421845536894\n",
            "Epoch: 96 | Iteration: 24576 | Loss: 0.3747730522558031\n",
            "Epoch: 96 | Iteration: 24640 | Loss: 0.1890151647867181\n",
            "Epoch: 96 | Iteration: 24704 | Loss: 0.11552016019097407\n",
            "Epoch: 96 | Iteration: 24768 | Loss: 0.6023521730967514\n",
            "Epoch: 96 | Iteration: 24832 | Loss: 0.05499060824531428\n",
            "Epoch: 96 | Iteration: 24896 | Loss: 0.27408861939565626\n",
            "Epoch: 96 | Iteration: 24960 | Loss: 0.011204687469348565\n",
            "Epoch: 96 | Iteration: 25024 | Loss: 0.04242556657655535\n",
            "Epoch: 96 | Iteration: 25088 | Loss: 0.3157576556780039\n",
            "Epoch: 96 | Iteration: 25152 | Loss: 0.7208053576753557\n",
            "Epoch: 96 | Iteration: 25216 | Loss: 0.19208611861515967\n",
            "Epoch: 96 | Iteration: 25280 | Loss: 0.14755368640170452\n",
            "Epoch: 96 | Iteration: 25344 | Loss: 0.02435648014045896\n",
            "Epoch: 96 | Iteration: 25408 | Loss: 0.03112819826799306\n",
            "Epoch: 96 | Iteration: 25472 | Loss: 0.5707811774939209\n",
            "Epoch: 96 | Iteration: 25536 | Loss: 1.1934781433864123\n",
            "Epoch: 96 | Iteration: 25600 | Loss: 0.07189050772182334\n",
            "Epoch: 96 | Iteration: 25664 | Loss: 0.6148428241515554\n",
            "Epoch: 96 | Iteration: 25728 | Loss: 0.5806199273723224\n",
            "Epoch: 96 | Iteration: 25792 | Loss: 0.8596274221712403\n",
            "Epoch: 96 | Iteration: 25856 | Loss: 0.0799806666987691\n",
            "Epoch: 96 | Iteration: 25920 | Loss: 0.11327448290860112\n",
            "Epoch: 96 | Iteration: 25984 | Loss: 0.046714875123015726\n",
            "Epoch: 96 | Iteration: 26048 | Loss: 0.30775439528615983\n",
            "Epoch: 96 | Iteration: 26112 | Loss: 0.26339988596868497\n",
            "Epoch: 96 | Iteration: 26176 | Loss: 0.19133607772132433\n",
            "Epoch: 96 | Iteration: 26240 | Loss: 0.20652269660991324\n",
            "Epoch: 96 | Iteration: 26304 | Loss: 0.5387948860567366\n",
            "Epoch: 96 | Iteration: 26368 | Loss: 0.8730944617736921\n",
            "Epoch: 96 | Iteration: 26432 | Loss: 0.9425078692361636\n",
            "Epoch: 96 | Iteration: 26496 | Loss: 1.4484605067905258\n",
            "Epoch: 96 | Iteration: 26560 | Loss: 0.9280103925263514\n",
            "Epoch: 96 | Iteration: 26624 | Loss: 0.9097788087618751\n",
            "Epoch: 96 | Iteration: 26688 | Loss: 0.8084884506792169\n",
            "Epoch: 96 | Iteration: 26752 | Loss: 0.9484502136866504\n",
            "Epoch: 96 | Iteration: 26816 | Loss: 0.2675395751921846\n",
            "Epoch: 96 | Iteration: 26880 | Loss: 1.0912740892595492\n",
            "Epoch: 96 | Iteration: 26944 | Loss: 0.01562302068595206\n",
            "Epoch: 96 | Iteration: 27008 | Loss: 0.12242499680158264\n",
            "Epoch: 96 | Iteration: 27072 | Loss: 0.07287378622441866\n",
            "Epoch: 96 | Iteration: 27136 | Loss: 1.617447833195123\n",
            "Epoch: 96 | Iteration: 27200 | Loss: 0.909808990804587\n",
            "Epoch: 96 | Iteration: 27264 | Loss: 0.116047340022887\n",
            "Epoch: 96 | Iteration: 27328 | Loss: 0.0201337504089094\n",
            "Epoch: 96 | Iteration: 27392 | Loss: 0.1442474253309044\n",
            "Epoch: 96 | Iteration: 27456 | Loss: 0.556461516508122\n",
            "Epoch: 96 | Iteration: 27520 | Loss: 0.6294867929326708\n",
            "Epoch: 96 | Iteration: 27584 | Loss: 0.7274592564956066\n",
            "Epoch: 96 | Iteration: 27648 | Loss: 0.5761833849327308\n",
            "Epoch: 96 | Iteration: 27712 | Loss: 0.22115714427702848\n",
            "Epoch: 96 | Iteration: 27776 | Loss: 0.3055856074339983\n",
            "Epoch: 96 | Iteration: 27840 | Loss: 0.9110117833560968\n",
            "Epoch: 96 | Iteration: 27904 | Loss: 0.08669796184781012\n",
            "Epoch: 96 | Iteration: 27968 | Loss: 0.025478911352340896\n",
            "Epoch: 96 | Iteration: 28032 | Loss: 0.21614818977048197\n",
            "Epoch: 96 | Iteration: 28096 | Loss: 0.1389505556064886\n",
            "Epoch: 96 | Iteration: 28160 | Loss: 0.6248943106995644\n",
            "Epoch: 96 | Iteration: 28224 | Loss: 0.07321663382749842\n",
            "Epoch: 96 | Iteration: 28288 | Loss: 0.012020330334447313\n",
            "Epoch: 96 | Iteration: 28352 | Loss: 0.7082664271553967\n",
            "Epoch: 96 | Iteration: 28416 | Loss: 0.1003870449749863\n",
            "Epoch: 96 | Iteration: 28480 | Loss: 0.08604968969826068\n",
            "Epoch: 96 | Iteration: 28544 | Loss: 0.12120632277233236\n",
            "Epoch: 96 | Iteration: 28608 | Loss: 0.6731966350882878\n",
            "Epoch: 96 | Iteration: 28672 | Loss: 0.2886300411038752\n",
            "Epoch: 96 | Iteration: 28736 | Loss: 0.11101955142408944\n",
            "Epoch: 96 | Iteration: 28800 | Loss: 0.19334530008725342\n",
            "Epoch: 96 | Iteration: 28864 | Loss: 0.10746996287801626\n",
            "Epoch: 96 | Iteration: 28928 | Loss: 0.4793876605371017\n",
            "Epoch: 96 | Iteration: 28992 | Loss: 0.21357591919790372\n",
            "Epoch: 96 | Iteration: 29056 | Loss: 0.16617104298962032\n",
            "Epoch: 96 | Iteration: 29120 | Loss: 0.30450110437097\n",
            "Epoch: 96 | Iteration: 29184 | Loss: 0.6208217964706588\n",
            "Epoch: 96 | Iteration: 29248 | Loss: 0.3756888574258461\n",
            "Epoch: 96 | Iteration: 29312 | Loss: 0.11764259726505492\n",
            "Epoch: 96 | Iteration: 29376 | Loss: 0.17202969192647033\n",
            "Epoch: 96 | Iteration: 29440 | Loss: 0.4661576650986383\n",
            "Epoch: 96 | Iteration: 29504 | Loss: 0.20118198219128042\n",
            "Epoch: 96 | Iteration: 29568 | Loss: 0.21951028123470429\n",
            "Epoch: 96 | Iteration: 29632 | Loss: 0.14287725618682795\n",
            "Epoch: 96 | Iteration: 29696 | Loss: 0.5368410989312243\n",
            "Epoch: 96 | Iteration: 29760 | Loss: 0.04761643421625476\n",
            "Epoch: 96 | Iteration: 29824 | Loss: 0.2664781037353097\n",
            "Epoch: 96 | Iteration: 29888 | Loss: 0.4921206844918352\n",
            "Epoch: 96 | Iteration: 29952 | Loss: 0.06435399541602124\n",
            "Epoch: 96 | Iteration: 30016 | Loss: 0.6669271455066137\n",
            "Epoch: 96 | Iteration: 30080 | Loss: 0.23151550195364073\n",
            "Epoch: 96 | Iteration: 30144 | Loss: 0.2879241937719188\n",
            "Epoch: 96 | Iteration: 30208 | Loss: 0.10721362974838311\n",
            "Epoch: 96 | Iteration: 30272 | Loss: 0.055526138083460505\n",
            "Epoch: 96 | Iteration: 30336 | Loss: 0.23077203245530792\n",
            "Epoch: 96 | Iteration: 30400 | Loss: 0.052587885588412674\n",
            "Epoch: 96 | Iteration: 30464 | Loss: 0.2949781283903778\n",
            "Epoch: 96 | Iteration: 30528 | Loss: 0.14461513362364897\n",
            "Epoch: 96 | Iteration: 30592 | Loss: 0.09714873903426205\n",
            "Epoch: 96 | Iteration: 30656 | Loss: 0.5865415374875604\n",
            "Epoch: 96 | Iteration: 30720 | Loss: 0.24669800068517447\n",
            "Epoch: 96 | Iteration: 30784 | Loss: 0.18789976420428972\n",
            "Epoch: 96 | Iteration: 30848 | Loss: 0.3495313617750232\n",
            "Epoch: 96 | Iteration: 30912 | Loss: 0.759479872678524\n",
            "Epoch: 96 | Iteration: 30976 | Loss: 0.17980802575036142\n",
            "Epoch: 96 | Iteration: 31040 | Loss: 0.48383271315912246\n",
            "Epoch: 96 | Iteration: 31104 | Loss: 0.450687899787056\n",
            "Epoch: 96 | Iteration: 31168 | Loss: 1.7072876007678122\n",
            "Epoch: 96 | Iteration: 31232 | Loss: 0.18029398970701846\n",
            "Epoch: 96 | Iteration: 31296 | Loss: 0.8380658958791949\n",
            "Epoch: 96 | Iteration: 31360 | Loss: 0.2784864442803489\n",
            "Epoch: 96 | Iteration: 31424 | Loss: 0.31270332071990203\n",
            "Epoch: 96 | Iteration: 31488 | Loss: 0.08263182707825573\n",
            "Epoch: 96 | Iteration: 31552 | Loss: 1.1071668556006764\n",
            "Epoch: 96 | Iteration: 31616 | Loss: 0.3484108862725161\n",
            "Epoch: 96 | Iteration: 31680 | Loss: 1.3756352757673942\n",
            "Epoch: 96 | Iteration: 31744 | Loss: 0.7568388632697567\n",
            "Epoch: 96 | Iteration: 31808 | Loss: 0.207198421953178\n",
            "Epoch: 96 | Iteration: 31872 | Loss: 0.18267321585655247\n",
            "Epoch: 96 | Iteration: 31936 | Loss: 0.25112385104235835\n",
            "Epoch: 96 | Iteration: 32000 | Loss: 0.2914831762875257\n",
            "Epoch: 96 | Iteration: 32064 | Loss: 0.31108329408639623\n",
            "Epoch: 96 | Iteration: 32128 | Loss: 0.16059967785169138\n",
            "Epoch: 96 | Iteration: 32192 | Loss: 0.4626667335158303\n",
            "Epoch: 96 | Iteration: 32256 | Loss: 0.21246823608184007\n",
            "Epoch: 96 | Iteration: 32320 | Loss: 0.7392524214946887\n",
            "Epoch: 96 | Iteration: 32384 | Loss: 0.3238723099324473\n",
            "Epoch: 96 | Iteration: 32448 | Loss: 1.0432852591722606\n",
            "Epoch: 96 | Iteration: 32512 | Loss: 0.3020371857373532\n",
            "Epoch: 96 | Iteration: 32576 | Loss: 0.0007109184536380808\n",
            "Epoch: 96 | Iteration: 32640 | Loss: 0.329137272973438\n",
            "Epoch: 96 | Iteration: 32704 | Loss: 0.30254016803763595\n",
            "Epoch: 96 | Iteration: 32768 | Loss: 0.8959735275423039\n",
            "Epoch: 96 | Iteration: 32832 | Loss: 0.6745700496030572\n",
            "Epoch: 96 | Iteration: 32896 | Loss: 0.691102271794507\n",
            "Epoch: 96 | Iteration: 32960 | Loss: 0.014743385935293134\n",
            "Epoch: 96 | Iteration: 33024 | Loss: 0.2819838447316062\n",
            "Epoch: 96 | Iteration: 33088 | Loss: 0.034116579271372315\n",
            "Epoch: 96 | Iteration: 33152 | Loss: 0.0806790873446913\n",
            "Epoch: 96 | Iteration: 33216 | Loss: 0.22238672255197606\n",
            "Epoch: 96 | Iteration: 33280 | Loss: 0.27195800693744787\n",
            "Epoch: 96 | Iteration: 33344 | Loss: 0.3523859571586339\n",
            "Epoch: 96 | Iteration: 33408 | Loss: 0.6243566305971491\n",
            "Epoch: 96 | Iteration: 33472 | Loss: 0.5390452381532111\n",
            "Epoch: 96 | Iteration: 33536 | Loss: 0.4306969796965485\n",
            "Epoch: 96 | Iteration: 33600 | Loss: 0.07111229734525613\n",
            "Epoch: 96 | Iteration: 33664 | Loss: 0.14048806006098122\n",
            "Epoch: 96 | Iteration: 33728 | Loss: 0.4827738863089126\n",
            "Epoch: 96 | Iteration: 33792 | Loss: 0.11926853116825012\n",
            "Epoch: 96 | Iteration: 33856 | Loss: 0.011264770338267241\n",
            "Epoch: 96 | Iteration: 33920 | Loss: 0.013086440073745832\n",
            "Epoch: 96 | Iteration: 33984 | Loss: 0.5315259872052839\n",
            "Epoch: 96 | Iteration: 34048 | Loss: 0.4959848875456944\n",
            "Epoch: 96 | Iteration: 34112 | Loss: 0.012688775800020126\n",
            "Epoch: 96 | Iteration: 34176 | Loss: 0.060305435732590666\n",
            "Epoch: 96 | Iteration: 34240 | Loss: 0.002957625499807841\n",
            "Epoch: 96 | Iteration: 34304 | Loss: 0.10714348252932074\n",
            "Epoch: 96 | Iteration: 34368 | Loss: 1.358574612220489\n",
            "Epoch: 96 | Iteration: 34432 | Loss: 0.158819159804912\n",
            "Epoch: 96 | Iteration: 34496 | Loss: 0.17279619133785334\n",
            "Epoch: 96 | Iteration: 34560 | Loss: 0.1281931562315578\n",
            "Epoch: 96 | Iteration: 34624 | Loss: 0.6146065970390424\n",
            "Epoch: 96 | Iteration: 34688 | Loss: 0.8138284547873463\n",
            "Epoch: 96 | Iteration: 34752 | Loss: 0.3780188849500936\n",
            "Epoch: 96 | Iteration: 34816 | Loss: 0.7828967105066875\n",
            "Epoch: 96 | Iteration: 34880 | Loss: 0.47423457834220967\n",
            "Epoch: 96 | Iteration: 34944 | Loss: 0.04363069360720119\n",
            "Epoch: 96 | Iteration: 35008 | Loss: 1.3211050308514678\n",
            "Epoch: 96 | Iteration: 35072 | Loss: 0.031720231739154496\n",
            "Epoch: 96 | Iteration: 35136 | Loss: 1.0082547323971356\n",
            "Epoch: 96 | Iteration: 35200 | Loss: 1.1916962992333413\n",
            "Epoch: 96 | Iteration: 35264 | Loss: 0.11794990180208684\n",
            "Epoch: 96 | Iteration: 35328 | Loss: 0.1607493037597129\n",
            "Epoch: 96 | Iteration: 35392 | Loss: 0.22754155945912585\n",
            "Epoch: 96 | Iteration: 35456 | Loss: 1.1532108189880157\n",
            "Epoch: 96 | Iteration: 35520 | Loss: 0.14187357050948257\n",
            "Epoch: 96 | Iteration: 35584 | Loss: 0.9916676171868544\n",
            "Epoch: 96 | Iteration: 35648 | Loss: 0.2503085038545893\n",
            "Epoch: 96 | Iteration: 35712 | Loss: 0.10387799122923014\n",
            "Epoch: 96 | Iteration: 35776 | Loss: 0.043693506641229986\n",
            "Epoch: 96 | Iteration: 35840 | Loss: 0.2406740038559901\n",
            "Epoch: 96 | Iteration: 35904 | Loss: 0.28978213800313524\n",
            "Epoch: 96 | Iteration: 35968 | Loss: 0.06711336359463146\n",
            "Epoch: 96 | Iteration: 36032 | Loss: 0.9444484879140551\n",
            "Epoch: 96 | Iteration: 36096 | Loss: 1.326558150135226\n",
            "Epoch: 96 | Iteration: 36160 | Loss: 0.42015271714829094\n",
            "Epoch: 96 | Iteration: 36224 | Loss: 0.10909236042574982\n",
            "Epoch: 96 | Iteration: 36288 | Loss: 0.021102419716241105\n",
            "Epoch: 96 | Iteration: 36352 | Loss: 0.18800845484525147\n",
            "Epoch: 96 | Iteration: 36416 | Loss: 0.7451537199885985\n",
            "Epoch: 96 | Iteration: 36480 | Loss: 0.020492823080089213\n",
            "Epoch: 96 | Iteration: 36544 | Loss: 0.24478930625006357\n",
            "Epoch: 96 | Iteration: 36608 | Loss: 0.06322247516601132\n",
            "Epoch: 96 | Iteration: 36672 | Loss: 0.22122427634582897\n",
            "Epoch: 96 | Iteration: 36736 | Loss: 0.941121188755635\n",
            "Epoch: 96 | Iteration: 36800 | Loss: 0.3949493989239563\n",
            "Epoch: 96 | Iteration: 36864 | Loss: 0.15394680032164626\n",
            "Epoch: 96 | Iteration: 36928 | Loss: 0.33492693557767417\n",
            "Epoch: 96 | Iteration: 36992 | Loss: 1.0751914250057257\n",
            "Epoch: 96 | Iteration: 37056 | Loss: 0.30361268483898207\n",
            "Epoch: 96 | Iteration: 37120 | Loss: 0.46904660710611656\n",
            "Epoch: 96 | Iteration: 37184 | Loss: 0.10834442209373629\n",
            "Epoch: 96 | Iteration: 37248 | Loss: 0.382201926732818\n",
            "Epoch: 96 | Iteration: 37312 | Loss: 0.9887911878529647\n",
            "Epoch: 96 | Iteration: 37376 | Loss: 0.9565205523718285\n",
            "Epoch: 96 | Iteration: 37440 | Loss: 0.4807798891632195\n",
            "Epoch: 96 | Iteration: 37504 | Loss: 0.21857258916655006\n",
            "Epoch: 96 | Iteration: 37568 | Loss: 0.05489068757422845\n",
            "Epoch: 96 | Iteration: 37632 | Loss: 0.2063484913498935\n",
            "Epoch: 96 | Iteration: 37696 | Loss: 0.6541726497541223\n",
            "Epoch: 96 | Iteration: 37760 | Loss: 0.7582563284343601\n",
            "Epoch: 96 | Iteration: 37824 | Loss: 1.0040263860548178\n",
            "Epoch: 96 | Iteration: 37888 | Loss: 0.06566568941353519\n",
            "Epoch: 96 | Iteration: 37952 | Loss: 0.04501095520824553\n",
            "Epoch: 96 | Iteration: 38016 | Loss: 0.15760196838559218\n",
            "Epoch: 96 | Iteration: 38080 | Loss: 0.0677259565688558\n",
            "Epoch: 96 | Iteration: 38144 | Loss: 0.1603314606083586\n",
            "Epoch: 96 | Iteration: 38208 | Loss: 0.0655938829251617\n",
            "Epoch: 96 | Iteration: 38272 | Loss: 0.5098715653171934\n",
            "Epoch: 96 | Iteration: 38336 | Loss: 0.7080274846042052\n",
            "Epoch: 96 | Iteration: 38400 | Loss: 0.1438474820759928\n",
            "Epoch: 96 | Iteration: 38464 | Loss: 0.6162597573739883\n",
            "Epoch: 96 | Iteration: 38528 | Loss: 0.5379790510387781\n",
            "Epoch: 96 | Iteration: 38592 | Loss: 0.3004041899116389\n",
            "Epoch: 96 | Iteration: 38656 | Loss: 0.58369866849006\n",
            "Epoch: 96 | Iteration: 38720 | Loss: 0.21279633673498266\n",
            "Epoch: 96 | Iteration: 38784 | Loss: 0.0074096220197089075\n",
            "Epoch: 96 | Iteration: 38848 | Loss: 0.08231161937445947\n",
            "Epoch: 96 | Iteration: 38912 | Loss: 0.25542495566147966\n",
            "Epoch: 96 | Iteration: 38976 | Loss: 0.2057871556453865\n",
            "Epoch: 96 | Iteration: 39040 | Loss: 0.006676761434273846\n",
            "Epoch: 96 | Iteration: 39104 | Loss: 0.09832551687567133\n",
            "Epoch: 96 | Iteration: 39168 | Loss: 0.7175204756177915\n",
            "Epoch: 96 | Iteration: 39232 | Loss: 0.3872085395194501\n",
            "Epoch: 96 | Iteration: 39296 | Loss: 0.9175447334699287\n",
            "Epoch: 96 | Iteration: 39360 | Loss: 1.045035463472102\n",
            "Epoch: 96 | Iteration: 39424 | Loss: 0.18164740816969402\n",
            "Epoch: 96 | Iteration: 39488 | Loss: 0.051720557381984375\n",
            "Epoch: 96 | Iteration: 39552 | Loss: 0.4261123057810229\n",
            "Epoch: 96 | Iteration: 39616 | Loss: 0.47031836216850464\n",
            "Epoch: 96 | Iteration: 39680 | Loss: 0.3774036771956426\n",
            "Epoch: 96 | Iteration: 39744 | Loss: 0.6799117437327377\n",
            "Epoch: 96 | Iteration: 39808 | Loss: 0.7032227753252756\n",
            "Epoch: 96 | Iteration: 39872 | Loss: 0.15335156649534076\n",
            "Epoch: 96 | Iteration: 39936 | Loss: 0.06093973943787152\n",
            "Epoch: 96 | Iteration: 40000 | Loss: 0.24017945324877493\n",
            "Epoch: 96 | Iteration: 40064 | Loss: 0.228086551860322\n",
            "Epoch: 96 | Iteration: 40128 | Loss: 0.15117585211934167\n",
            "Epoch: 96 | Iteration: 40192 | Loss: 0.31400872171248523\n",
            "Epoch: 96 | Iteration: 40256 | Loss: 0.4323188723516658\n",
            "Epoch: 96 | Iteration: 40320 | Loss: 0.19998259362811494\n",
            "Epoch: 96 | Iteration: 40384 | Loss: 0.22919021331071077\n",
            "Epoch: 96 | Iteration: 40448 | Loss: 0.06460896835925752\n",
            "Epoch: 96 | Iteration: 40512 | Loss: 0.09568616418614223\n",
            "Epoch: 96 | Iteration: 40576 | Loss: 0.6630888231960936\n",
            "Epoch: 96 | Iteration: 40640 | Loss: 0.634159191791539\n",
            "Epoch: 96 | Iteration: 40704 | Loss: 0.4900519216012418\n",
            "Epoch: 96 | Iteration: 40768 | Loss: 0.20391132256947023\n",
            "Epoch: 96 | Iteration: 40832 | Loss: 0.13338097482513075\n",
            "Epoch: 96 | Iteration: 40896 | Loss: 0.07959047245474202\n",
            "Epoch: 96 | Iteration: 40960 | Loss: 0.7887377843176462\n",
            "Epoch: 96 | Iteration: 41024 | Loss: 0.4055662947332197\n",
            "Epoch: 96 | Iteration: 41088 | Loss: 0.14508592526504827\n",
            "Epoch: 96 | Iteration: 41152 | Loss: 0.8363020954288585\n",
            "Epoch: 96 | Iteration: 41216 | Loss: 1.0774172492488834\n",
            "Epoch: 96 | Iteration: 41280 | Loss: 0.8170600549945923\n",
            "Epoch: 96 | Iteration: 41344 | Loss: 0.29774309353271866\n",
            "Epoch: 96 | Iteration: 41408 | Loss: 0.2863914831672496\n",
            "Epoch: 96 | Iteration: 41472 | Loss: 0.040240760653191765\n",
            "Epoch: 96 | Iteration: 41536 | Loss: 0.9844845297346794\n",
            "Epoch: 96 | Iteration: 41600 | Loss: 0.128573072013354\n",
            "Epoch: 96 | Iteration: 41664 | Loss: 0.0737590979097174\n",
            "Epoch: 96 | Iteration: 41728 | Loss: 0.26479534695592344\n",
            "Epoch: 96 | Iteration: 41792 | Loss: 0.14698511132898248\n",
            "Epoch: 96 | Iteration: 41856 | Loss: 0.48678827452100426\n",
            "Epoch: 96 | Iteration: 41920 | Loss: 0.3508614255816366\n",
            "Epoch: 96 | Iteration: 41984 | Loss: 0.09027143725146106\n",
            "Epoch: 96 | Iteration: 42048 | Loss: 0.03345031436990456\n",
            "Epoch: 96 | Iteration: 42112 | Loss: 0.18574276460178318\n",
            "Epoch: 96 | Iteration: 42176 | Loss: 0.19771818993780038\n",
            "Epoch: 96 | Iteration: 42240 | Loss: 0.06723224849912117\n",
            "Epoch: 96 | Iteration: 42304 | Loss: 0.7199540283677054\n",
            "Epoch: 96 | Iteration: 42368 | Loss: 0.5324703530635722\n",
            "Epoch: 96 | Iteration: 42432 | Loss: 0.8018379781787368\n",
            "Epoch: 96 | Iteration: 42496 | Loss: 0.582530985672577\n",
            "Epoch: 96 | Iteration: 42560 | Loss: 0.9091844913246343\n",
            "Epoch: 96 | Iteration: 42624 | Loss: 0.6149753118720385\n",
            "Epoch: 96 | Iteration: 42688 | Loss: 0.3619546270606127\n",
            "Epoch: 96 | Iteration: 42752 | Loss: 1.0407139375579404\n",
            "Epoch: 96 | Iteration: 42816 | Loss: 1.042868876250302\n",
            "Epoch: 96 | Iteration: 42880 | Loss: 0.7508827939495984\n",
            "Epoch: 96 | Iteration: 42944 | Loss: 0.4393036462674384\n",
            "Epoch: 96 | Iteration: 43008 | Loss: 0.6300095678914354\n",
            "Epoch: 96 | Iteration: 43072 | Loss: 1.3369506484822797\n",
            "Epoch: 96 | Iteration: 43136 | Loss: 0.03964408430724122\n",
            "Epoch: 96 | Iteration: 43200 | Loss: 0.09005424080373736\n",
            "Epoch: 96 | Iteration: 43264 | Loss: 0.0027867755507357574\n",
            "Epoch: 96 | Iteration: 43328 | Loss: 0.01611969162308957\n",
            "Epoch: 96 | Iteration: 43392 | Loss: 0.40373729800083935\n",
            "Epoch: 96 | Iteration: 43456 | Loss: 0.08576691819306691\n",
            "Epoch: 96 | Iteration: 43520 | Loss: 0.397486929724906\n",
            "Epoch: 96 | Iteration: 43584 | Loss: 0.20216276432051775\n",
            "Epoch: 96 | Iteration: 43648 | Loss: 1.2692323983982996\n",
            "Epoch: 96 | Iteration: 43712 | Loss: 0.006096431422024434\n",
            "Epoch: 96 | Iteration: 43776 | Loss: 0.04841158773912453\n",
            "Epoch: 96 | Iteration: 43840 | Loss: 0.8033511122174977\n",
            "Epoch: 96 | Iteration: 43904 | Loss: 0.25863314677930926\n",
            "Epoch: 96 | Iteration: 43968 | Loss: 0.20345618072188976\n",
            "Epoch: 96 | Iteration: 44032 | Loss: 0.25092023343391917\n",
            "Epoch: 96 | Iteration: 44096 | Loss: 0.2807175451453066\n",
            "Epoch: 96 | Iteration: 44160 | Loss: 0.10324485156249757\n",
            "Epoch: 96 | Iteration: 44224 | Loss: 0.18438585090911663\n",
            "Epoch: 96 | Iteration: 44288 | Loss: 0.3457971480407793\n",
            "Epoch: 96 | Iteration: 44352 | Loss: 0.3259962479234756\n",
            "Epoch: 96 | Iteration: 44416 | Loss: 1.0879344585259154\n",
            "Epoch: 96 | Iteration: 44480 | Loss: 0.31234655104210557\n",
            "Epoch: 96 | Iteration: 44544 | Loss: 0.03149819527133742\n",
            "Epoch: 96 | Iteration: 44608 | Loss: 0.41607267323122377\n",
            "Epoch: 96 | Iteration: 44672 | Loss: 0.06689673635901702\n",
            "Epoch: 96 | Iteration: 44736 | Loss: 0.03782092815528226\n",
            "Epoch: 96 | Iteration: 44800 | Loss: 0.4129332253242398\n",
            "Epoch: 96 | Iteration: 44864 | Loss: 0.19198547648219566\n",
            "Epoch: 96 | Iteration: 44928 | Loss: 0.6505331384539742\n",
            "Epoch: 96 | Iteration: 44992 | Loss: 0.16589093148208267\n",
            "Epoch: 96 | Iteration: 45056 | Loss: 0.16310733709570024\n",
            "Epoch: 96 | Iteration: 45120 | Loss: 0.8409304137616422\n",
            "Epoch: 96 | Iteration: 45184 | Loss: 0.9868163663725102\n",
            "Epoch: 96 | Iteration: 45248 | Loss: 0.3495495097840754\n",
            "Epoch: 96 | Iteration: 45312 | Loss: 0.5687932053751776\n",
            "Epoch: 96 | Iteration: 45376 | Loss: 0.31384752941080674\n",
            "Epoch: 96 | Iteration: 45440 | Loss: 0.7950508372409841\n",
            "Epoch: 96 | Iteration: 45504 | Loss: 0.36794492489925545\n",
            "Epoch: 96 | Iteration: 45568 | Loss: 0.6081902978580951\n",
            "Epoch: 96 | Iteration: 45632 | Loss: 0.14801640073035335\n",
            "Epoch: 96 | Iteration: 45696 | Loss: 0.007347297918031101\n",
            "Epoch: 96 | Iteration: 45760 | Loss: 0.545714479447122\n",
            "Epoch: 96 | Iteration: 45824 | Loss: 0.3334173458623795\n",
            "Epoch: 96 | Iteration: 45888 | Loss: 0.6717183776790133\n",
            "Epoch: 96 | Iteration: 45952 | Loss: 0.5200569826983328\n",
            "Epoch: 96 | Iteration: 46016 | Loss: 1.0206997717293207\n",
            "Epoch: 96 | Iteration: 46080 | Loss: 1.1932282275099766\n",
            "Epoch: 96 | Iteration: 46144 | Loss: 0.07332404565102335\n",
            "Epoch: 96 | Iteration: 46208 | Loss: 1.3786562838050165\n",
            "Epoch: 96 | Iteration: 46272 | Loss: 1.309934406011703\n",
            "Epoch: 96 | Iteration: 46336 | Loss: 0.4625165899628768\n",
            "Epoch: 96 | Iteration: 46400 | Loss: 0.3149309316284397\n",
            "Epoch: 96 | Iteration: 46464 | Loss: 0.016771844792198877\n",
            "Epoch: 96 | Iteration: 46528 | Loss: 0.03874414656553726\n",
            "Epoch: 96 | Iteration: 46592 | Loss: 0.04444125178481109\n",
            "Epoch: 96 | Iteration: 46656 | Loss: 0.2251953031578227\n",
            "Epoch: 96 | Iteration: 46720 | Loss: 0.8651685084036228\n",
            "Epoch: 96 | Iteration: 46784 | Loss: 0.04249085808681392\n",
            "Epoch: 96 | Iteration: 46848 | Loss: 0.7270956912737234\n",
            "Epoch: 96 | Iteration: 46912 | Loss: 0.3181163018326628\n",
            "Epoch: 96 | Iteration: 46976 | Loss: 1.2760893609843444\n",
            "Epoch: 96 | Iteration: 47040 | Loss: 0.04062868802135738\n",
            "Epoch: 96 | Iteration: 47104 | Loss: 0.2989766383235197\n",
            "Epoch: 96 | Iteration: 47168 | Loss: 0.3259918964526634\n",
            "Epoch: 96 | Iteration: 47232 | Loss: 1.3659868870316485\n",
            "Epoch: 96 | Iteration: 47296 | Loss: 0.35146974339640763\n",
            "Epoch: 96 | Iteration: 47360 | Loss: 0.8730095066153899\n",
            "Epoch: 96 | Iteration: 47424 | Loss: 0.27245489722738425\n",
            "Epoch: 96 | Iteration: 47488 | Loss: 0.09128047912322239\n",
            "Epoch: 96 | Iteration: 47552 | Loss: 1.2168308663373109\n",
            "Epoch: 96 | Iteration: 47616 | Loss: 0.014001502946375623\n",
            "Epoch: 96 | Iteration: 47680 | Loss: 1.3739717782385776\n",
            "Epoch: 96 | Iteration: 47744 | Loss: 0.8378850618307787\n",
            "Epoch: 96 | Iteration: 47808 | Loss: 0.0806976138132188\n",
            "Epoch: 96 | Iteration: 47872 | Loss: 0.7563161449922806\n",
            "Epoch: 96 | Iteration: 47936 | Loss: 0.472909649367715\n",
            "Epoch: 97 | Iteration: 0 | Loss: 0.15135139653441076\n",
            "Epoch: 97 | Iteration: 64 | Loss: 0.572752686665741\n",
            "Epoch: 97 | Iteration: 128 | Loss: 1.1016742913464124\n",
            "Epoch: 97 | Iteration: 192 | Loss: 0.2585605649365924\n",
            "Epoch: 97 | Iteration: 256 | Loss: 0.16803498440723968\n",
            "Epoch: 97 | Iteration: 320 | Loss: 0.016063614463216302\n",
            "Epoch: 97 | Iteration: 384 | Loss: 0.12488728505704332\n",
            "Epoch: 97 | Iteration: 448 | Loss: 1.7738197083054805\n",
            "Epoch: 97 | Iteration: 512 | Loss: 0.4338758777668188\n",
            "Epoch: 97 | Iteration: 576 | Loss: 0.3848235421862367\n",
            "Epoch: 97 | Iteration: 640 | Loss: 0.3664945155297217\n",
            "Epoch: 97 | Iteration: 704 | Loss: 0.17372930843655643\n",
            "Epoch: 97 | Iteration: 768 | Loss: 0.4470604604612086\n",
            "Epoch: 97 | Iteration: 832 | Loss: 0.4247818330898789\n",
            "Epoch: 97 | Iteration: 896 | Loss: 0.8707193732086105\n",
            "Epoch: 97 | Iteration: 960 | Loss: 0.48725368507839617\n",
            "Epoch: 97 | Iteration: 1024 | Loss: 0.928533192629681\n",
            "Epoch: 97 | Iteration: 1088 | Loss: 0.7146071655250064\n",
            "Epoch: 97 | Iteration: 1152 | Loss: 0.29926014155520486\n",
            "Epoch: 97 | Iteration: 1216 | Loss: 0.8618177259569538\n",
            "Epoch: 97 | Iteration: 1280 | Loss: 0.1717282534854228\n",
            "Epoch: 97 | Iteration: 1344 | Loss: 1.5591993938296946\n",
            "Epoch: 97 | Iteration: 1408 | Loss: 0.0995982557066113\n",
            "Epoch: 97 | Iteration: 1472 | Loss: 0.5335426104236416\n",
            "Epoch: 97 | Iteration: 1536 | Loss: 0.047144781393959244\n",
            "Epoch: 97 | Iteration: 1600 | Loss: 1.0075273803218292\n",
            "Epoch: 97 | Iteration: 1664 | Loss: 0.09787744593499971\n",
            "Epoch: 97 | Iteration: 1728 | Loss: 0.1552685252875306\n",
            "Epoch: 97 | Iteration: 1792 | Loss: 0.18458284917328593\n",
            "Epoch: 97 | Iteration: 1856 | Loss: 0.1753429120534284\n",
            "Epoch: 97 | Iteration: 1920 | Loss: 0.8565711175474429\n",
            "Epoch: 97 | Iteration: 1984 | Loss: 0.1986714403749847\n",
            "Epoch: 97 | Iteration: 2048 | Loss: 0.3278634352230454\n",
            "Epoch: 97 | Iteration: 2112 | Loss: 0.035389071659368\n",
            "Epoch: 97 | Iteration: 2176 | Loss: 0.3641928173697607\n",
            "Epoch: 97 | Iteration: 2240 | Loss: 0.09713847473401652\n",
            "Epoch: 97 | Iteration: 2304 | Loss: 0.1286220427351737\n",
            "Epoch: 97 | Iteration: 2368 | Loss: 0.1981611233050594\n",
            "Epoch: 97 | Iteration: 2432 | Loss: 0.012834807528653833\n",
            "Epoch: 97 | Iteration: 2496 | Loss: 0.13199975298352318\n",
            "Epoch: 97 | Iteration: 2560 | Loss: 0.405606983456774\n",
            "Epoch: 97 | Iteration: 2624 | Loss: 0.8541345581247013\n",
            "Epoch: 97 | Iteration: 2688 | Loss: 0.8970803255121027\n",
            "Epoch: 97 | Iteration: 2752 | Loss: 0.06507132355115314\n",
            "Epoch: 97 | Iteration: 2816 | Loss: 0.12851999538778772\n",
            "Epoch: 97 | Iteration: 2880 | Loss: 0.3205313507175373\n",
            "Epoch: 97 | Iteration: 2944 | Loss: 0.10043441678970054\n",
            "Epoch: 97 | Iteration: 3008 | Loss: 0.29854742512828547\n",
            "Epoch: 97 | Iteration: 3072 | Loss: 0.08363713664784952\n",
            "Epoch: 97 | Iteration: 3136 | Loss: 0.0014383052277156696\n",
            "Epoch: 97 | Iteration: 3200 | Loss: 0.09700085748883956\n",
            "Epoch: 97 | Iteration: 3264 | Loss: 0.07237535571227084\n",
            "Epoch: 97 | Iteration: 3328 | Loss: 0.06792236500527944\n",
            "Epoch: 97 | Iteration: 3392 | Loss: 0.05314603033208888\n",
            "Epoch: 97 | Iteration: 3456 | Loss: 0.2240040312015077\n",
            "Epoch: 97 | Iteration: 3520 | Loss: 0.15480627744023334\n",
            "Epoch: 97 | Iteration: 3584 | Loss: 0.07027513808233969\n",
            "Epoch: 97 | Iteration: 3648 | Loss: 0.5148902763167735\n",
            "Epoch: 97 | Iteration: 3712 | Loss: 0.10672383991142495\n",
            "Epoch: 97 | Iteration: 3776 | Loss: 0.3383341187407577\n",
            "Epoch: 97 | Iteration: 3840 | Loss: 0.00671817316331005\n",
            "Epoch: 97 | Iteration: 3904 | Loss: 0.20077499676142946\n",
            "Epoch: 97 | Iteration: 3968 | Loss: 0.07373631062946921\n",
            "Epoch: 97 | Iteration: 4032 | Loss: 0.14824968853225268\n",
            "Epoch: 97 | Iteration: 4096 | Loss: 0.27310082908964356\n",
            "Epoch: 97 | Iteration: 4160 | Loss: 0.20059803919610905\n",
            "Epoch: 97 | Iteration: 4224 | Loss: 0.5037093828281403\n",
            "Epoch: 97 | Iteration: 4288 | Loss: 0.0948315980756658\n",
            "Epoch: 97 | Iteration: 4352 | Loss: 0.2930426737503817\n",
            "Epoch: 97 | Iteration: 4416 | Loss: 0.9498415601299482\n",
            "Epoch: 97 | Iteration: 4480 | Loss: 0.06529383976987789\n",
            "Epoch: 97 | Iteration: 4544 | Loss: 0.04882910946286346\n",
            "Epoch: 97 | Iteration: 4608 | Loss: 0.7003889737580388\n",
            "Epoch: 97 | Iteration: 4672 | Loss: 0.05902623280859111\n",
            "Epoch: 97 | Iteration: 4736 | Loss: 0.09250414415010025\n",
            "Epoch: 97 | Iteration: 4800 | Loss: 0.1612675771029262\n",
            "Epoch: 97 | Iteration: 4864 | Loss: 0.028413700844401254\n",
            "Epoch: 97 | Iteration: 4928 | Loss: 0.3787927662950548\n",
            "Epoch: 97 | Iteration: 4992 | Loss: 0.7381898276622892\n",
            "Epoch: 97 | Iteration: 5056 | Loss: 0.5111072003435893\n",
            "Epoch: 97 | Iteration: 5120 | Loss: 0.7690777868123408\n",
            "Epoch: 97 | Iteration: 5184 | Loss: 0.28248182815161654\n",
            "Epoch: 97 | Iteration: 5248 | Loss: 0.3332757280611938\n",
            "Epoch: 97 | Iteration: 5312 | Loss: 0.3647769174437412\n",
            "Epoch: 97 | Iteration: 5376 | Loss: 0.10744604473567858\n",
            "Epoch: 97 | Iteration: 5440 | Loss: 0.11855553565735713\n",
            "Epoch: 97 | Iteration: 5504 | Loss: 0.3077513563627938\n",
            "Epoch: 97 | Iteration: 5568 | Loss: 0.16426056155431548\n",
            "Epoch: 97 | Iteration: 5632 | Loss: 0.1593953609184588\n",
            "Epoch: 97 | Iteration: 5696 | Loss: 0.8747297809605693\n",
            "Epoch: 97 | Iteration: 5760 | Loss: 1.1913977009516203\n",
            "Epoch: 97 | Iteration: 5824 | Loss: 0.1660936184255465\n",
            "Epoch: 97 | Iteration: 5888 | Loss: 0.9991059158517873\n",
            "Epoch: 97 | Iteration: 5952 | Loss: 0.04895723045665625\n",
            "Epoch: 97 | Iteration: 6016 | Loss: 0.0759176922550761\n",
            "Epoch: 97 | Iteration: 6080 | Loss: 0.46418530486729276\n",
            "Epoch: 97 | Iteration: 6144 | Loss: 0.39387819559237947\n",
            "Epoch: 97 | Iteration: 6208 | Loss: 0.4826867490252121\n",
            "Epoch: 97 | Iteration: 6272 | Loss: 0.03469718291632683\n",
            "Epoch: 97 | Iteration: 6336 | Loss: 0.1782114833020623\n",
            "Epoch: 97 | Iteration: 6400 | Loss: 1.0179104085166044\n",
            "Epoch: 97 | Iteration: 6464 | Loss: 0.05942490092547935\n",
            "Epoch: 97 | Iteration: 6528 | Loss: 0.24989851741971936\n",
            "Epoch: 97 | Iteration: 6592 | Loss: 0.02307223842787912\n",
            "Epoch: 97 | Iteration: 6656 | Loss: 0.26391960453840924\n",
            "Epoch: 97 | Iteration: 6720 | Loss: 0.05252132171924534\n",
            "Epoch: 97 | Iteration: 6784 | Loss: 0.9072620779366573\n",
            "Epoch: 97 | Iteration: 6848 | Loss: 1.7258763130306565\n",
            "Epoch: 97 | Iteration: 6912 | Loss: 0.4285452779095524\n",
            "Epoch: 97 | Iteration: 6976 | Loss: 0.5391787735983296\n",
            "Epoch: 97 | Iteration: 7040 | Loss: 1.0688799864351972\n",
            "Epoch: 97 | Iteration: 7104 | Loss: 0.22103249175205908\n",
            "Epoch: 97 | Iteration: 7168 | Loss: 0.1221292199543107\n",
            "Epoch: 97 | Iteration: 7232 | Loss: 0.8659946669757056\n",
            "Epoch: 97 | Iteration: 7296 | Loss: 0.7525019875980841\n",
            "Epoch: 97 | Iteration: 7360 | Loss: 0.045192174525513176\n",
            "Epoch: 97 | Iteration: 7424 | Loss: 0.08525664917137663\n",
            "Epoch: 97 | Iteration: 7488 | Loss: 0.522147951829599\n",
            "Epoch: 97 | Iteration: 7552 | Loss: 0.677731167352351\n",
            "Epoch: 97 | Iteration: 7616 | Loss: 0.1874270924937526\n",
            "Epoch: 97 | Iteration: 7680 | Loss: 0.5693805158806796\n",
            "Epoch: 97 | Iteration: 7744 | Loss: 0.328960373562347\n",
            "Epoch: 97 | Iteration: 7808 | Loss: 0.47757627625024207\n",
            "Epoch: 97 | Iteration: 7872 | Loss: 0.5737536353202966\n",
            "Epoch: 97 | Iteration: 7936 | Loss: 0.28935333001855096\n",
            "Epoch: 97 | Iteration: 8000 | Loss: 0.1050709831782006\n",
            "Epoch: 97 | Iteration: 8064 | Loss: 0.3085228959683573\n",
            "Epoch: 97 | Iteration: 8128 | Loss: 0.10068479103725322\n",
            "Epoch: 97 | Iteration: 8192 | Loss: 1.8177113146460433\n",
            "Epoch: 97 | Iteration: 8256 | Loss: 0.1415817409420308\n",
            "Epoch: 97 | Iteration: 8320 | Loss: 0.33261808068568216\n",
            "Epoch: 97 | Iteration: 8384 | Loss: 0.3466139481648618\n",
            "Epoch: 97 | Iteration: 8448 | Loss: 0.6057490132993172\n",
            "Epoch: 97 | Iteration: 8512 | Loss: 0.0625987982706837\n",
            "Epoch: 97 | Iteration: 8576 | Loss: 0.15214309984083285\n",
            "Epoch: 97 | Iteration: 8640 | Loss: 0.1927811421791985\n",
            "Epoch: 97 | Iteration: 8704 | Loss: 1.0802378054217274\n",
            "Epoch: 97 | Iteration: 8768 | Loss: 0.5507626697258934\n",
            "Epoch: 97 | Iteration: 8832 | Loss: 1.0697762802160729\n",
            "Epoch: 97 | Iteration: 8896 | Loss: 0.5524662783706784\n",
            "Epoch: 97 | Iteration: 8960 | Loss: 0.2113743142540404\n",
            "Epoch: 97 | Iteration: 9024 | Loss: 0.19022336173182502\n",
            "Epoch: 97 | Iteration: 9088 | Loss: 0.2313381098016795\n",
            "Epoch: 97 | Iteration: 9152 | Loss: 0.08533238790576009\n",
            "Epoch: 97 | Iteration: 9216 | Loss: 0.2514825689480318\n",
            "Epoch: 97 | Iteration: 9280 | Loss: 0.8021799832356451\n",
            "Epoch: 97 | Iteration: 9344 | Loss: 0.20901099014366856\n",
            "Epoch: 97 | Iteration: 9408 | Loss: 0.6049083756464837\n",
            "Epoch: 97 | Iteration: 9472 | Loss: 0.536723398566269\n",
            "Epoch: 97 | Iteration: 9536 | Loss: 0.08072960853430704\n",
            "Epoch: 97 | Iteration: 9600 | Loss: 0.14507889332547833\n",
            "Epoch: 97 | Iteration: 9664 | Loss: 0.0031065659829527053\n",
            "Epoch: 97 | Iteration: 9728 | Loss: 0.4240383855639035\n",
            "Epoch: 97 | Iteration: 9792 | Loss: 0.0881684445701941\n",
            "Epoch: 97 | Iteration: 9856 | Loss: 0.022117723404790485\n",
            "Epoch: 97 | Iteration: 9920 | Loss: 0.3141537253149853\n",
            "Epoch: 97 | Iteration: 9984 | Loss: 0.13765250837745222\n",
            "Epoch: 97 | Iteration: 10048 | Loss: 0.3343801525868136\n",
            "Epoch: 97 | Iteration: 10112 | Loss: 0.525370772764533\n",
            "Epoch: 97 | Iteration: 10176 | Loss: 0.6230917989298718\n",
            "Epoch: 97 | Iteration: 10240 | Loss: 0.5106169274014598\n",
            "Epoch: 97 | Iteration: 10304 | Loss: 0.026107124192161064\n",
            "Epoch: 97 | Iteration: 10368 | Loss: 0.12190477619407444\n",
            "Epoch: 97 | Iteration: 10432 | Loss: 0.0415632125828623\n",
            "Epoch: 97 | Iteration: 10496 | Loss: 0.026365780589408805\n",
            "Epoch: 97 | Iteration: 10560 | Loss: 0.03327736315704608\n",
            "Epoch: 97 | Iteration: 10624 | Loss: 0.14088833916152393\n",
            "Epoch: 97 | Iteration: 10688 | Loss: 0.09647755821312204\n",
            "Epoch: 97 | Iteration: 10752 | Loss: 0.3661010462190347\n",
            "Epoch: 97 | Iteration: 10816 | Loss: 0.18013110724353015\n",
            "Epoch: 97 | Iteration: 10880 | Loss: 0.16134656175399312\n",
            "Epoch: 97 | Iteration: 10944 | Loss: 1.1242648374613906\n",
            "Epoch: 97 | Iteration: 11008 | Loss: 0.22156945918296214\n",
            "Epoch: 97 | Iteration: 11072 | Loss: 0.314989910298773\n",
            "Epoch: 97 | Iteration: 11136 | Loss: 0.08247728117018943\n",
            "Epoch: 97 | Iteration: 11200 | Loss: 0.2514919182354969\n",
            "Epoch: 97 | Iteration: 11264 | Loss: 0.027560968139503407\n",
            "Epoch: 97 | Iteration: 11328 | Loss: 0.08913135895619076\n",
            "Epoch: 97 | Iteration: 11392 | Loss: 0.0259819835830892\n",
            "Epoch: 97 | Iteration: 11456 | Loss: 0.08768541987778883\n",
            "Epoch: 97 | Iteration: 11520 | Loss: 0.46381081389914525\n",
            "Epoch: 97 | Iteration: 11584 | Loss: 0.4769210550469666\n",
            "Epoch: 97 | Iteration: 11648 | Loss: 0.6523280111926687\n",
            "Epoch: 97 | Iteration: 11712 | Loss: 0.7492901934954371\n",
            "Epoch: 97 | Iteration: 11776 | Loss: 0.17777213109318707\n",
            "Epoch: 97 | Iteration: 11840 | Loss: 0.46873402473826364\n",
            "Epoch: 97 | Iteration: 11904 | Loss: 0.5018337836020045\n",
            "Epoch: 97 | Iteration: 11968 | Loss: 0.16620411985032452\n",
            "Epoch: 97 | Iteration: 12032 | Loss: 0.08419063299192431\n",
            "Epoch: 97 | Iteration: 12096 | Loss: 0.031887719086152545\n",
            "Epoch: 97 | Iteration: 12160 | Loss: 0.11751836984726977\n",
            "Epoch: 97 | Iteration: 12224 | Loss: 0.12879720781288942\n",
            "Epoch: 97 | Iteration: 12288 | Loss: 0.22313398232036977\n",
            "Epoch: 97 | Iteration: 12352 | Loss: 0.1637939141203158\n",
            "Epoch: 97 | Iteration: 12416 | Loss: 0.0902695405509594\n",
            "Epoch: 97 | Iteration: 12480 | Loss: 0.056898532181951934\n",
            "Epoch: 97 | Iteration: 12544 | Loss: 0.6631582360082695\n",
            "Epoch: 97 | Iteration: 12608 | Loss: 0.14462236993115815\n",
            "Epoch: 97 | Iteration: 12672 | Loss: 0.6055941406463654\n",
            "Epoch: 97 | Iteration: 12736 | Loss: 0.045404607228682295\n",
            "Epoch: 97 | Iteration: 12800 | Loss: 0.6151134086657397\n",
            "Epoch: 97 | Iteration: 12864 | Loss: 0.14506924127724735\n",
            "Epoch: 97 | Iteration: 12928 | Loss: 0.22222980096536993\n",
            "Epoch: 97 | Iteration: 12992 | Loss: 0.11786878388408534\n",
            "Epoch: 97 | Iteration: 13056 | Loss: 0.09748699743452696\n",
            "Epoch: 97 | Iteration: 13120 | Loss: 0.06406352411283603\n",
            "Epoch: 97 | Iteration: 13184 | Loss: 0.468378890204678\n",
            "Epoch: 97 | Iteration: 13248 | Loss: 0.05767374872153588\n",
            "Epoch: 97 | Iteration: 13312 | Loss: 0.014946627105731393\n",
            "Epoch: 97 | Iteration: 13376 | Loss: 0.4700414430286798\n",
            "Epoch: 97 | Iteration: 13440 | Loss: 0.061578369865500526\n",
            "Epoch: 97 | Iteration: 13504 | Loss: 0.2266050951620936\n",
            "Epoch: 97 | Iteration: 13568 | Loss: 0.09162446976607086\n",
            "Epoch: 97 | Iteration: 13632 | Loss: 0.3444953203081511\n",
            "Epoch: 97 | Iteration: 13696 | Loss: 0.47016870885200546\n",
            "Epoch: 97 | Iteration: 13760 | Loss: 0.023481999656508976\n",
            "Epoch: 97 | Iteration: 13824 | Loss: 0.2501496868512225\n",
            "Epoch: 97 | Iteration: 13888 | Loss: 0.12546052429318022\n",
            "Epoch: 97 | Iteration: 13952 | Loss: 0.8010148373614993\n",
            "Epoch: 97 | Iteration: 14016 | Loss: 0.23617734532721296\n",
            "Epoch: 97 | Iteration: 14080 | Loss: 0.24166634804265788\n",
            "Epoch: 97 | Iteration: 14144 | Loss: 0.21083734988949432\n",
            "Epoch: 97 | Iteration: 14208 | Loss: 0.5845225942701344\n",
            "Epoch: 97 | Iteration: 14272 | Loss: 0.431299318284173\n",
            "Epoch: 97 | Iteration: 14336 | Loss: 0.2515937231148469\n",
            "Epoch: 97 | Iteration: 14400 | Loss: 0.1548931275707816\n",
            "Epoch: 97 | Iteration: 14464 | Loss: 0.0834692901369187\n",
            "Epoch: 97 | Iteration: 14528 | Loss: 0.3026711069226081\n",
            "Epoch: 97 | Iteration: 14592 | Loss: 0.1771292894013003\n",
            "Epoch: 97 | Iteration: 14656 | Loss: 0.0877422777494712\n",
            "Epoch: 97 | Iteration: 14720 | Loss: 0.7296641925484519\n",
            "Epoch: 97 | Iteration: 14784 | Loss: 0.7597717741371204\n",
            "Epoch: 97 | Iteration: 14848 | Loss: 0.21936212492302581\n",
            "Epoch: 97 | Iteration: 14912 | Loss: 0.19907191502917684\n",
            "Epoch: 97 | Iteration: 14976 | Loss: 0.0374231243369893\n",
            "Epoch: 97 | Iteration: 15040 | Loss: 0.1363007183662072\n",
            "Epoch: 97 | Iteration: 15104 | Loss: 0.6166229287180964\n",
            "Epoch: 97 | Iteration: 15168 | Loss: 0.13176424120521946\n",
            "Epoch: 97 | Iteration: 15232 | Loss: 0.12734159991588265\n",
            "Epoch: 97 | Iteration: 15296 | Loss: 0.45784710647840404\n",
            "Epoch: 97 | Iteration: 15360 | Loss: 0.35525855076047586\n",
            "Epoch: 97 | Iteration: 15424 | Loss: 0.0662370183168022\n",
            "Epoch: 97 | Iteration: 15488 | Loss: 0.09498744059672465\n",
            "Epoch: 97 | Iteration: 15552 | Loss: 0.25926886710004793\n",
            "Epoch: 97 | Iteration: 15616 | Loss: 0.051143213185404775\n",
            "Epoch: 97 | Iteration: 15680 | Loss: 0.20879658787031952\n",
            "Epoch: 97 | Iteration: 15744 | Loss: 0.4097339155911988\n",
            "Epoch: 97 | Iteration: 15808 | Loss: 0.48929825326619436\n",
            "Epoch: 97 | Iteration: 15872 | Loss: 0.24742046081196958\n",
            "Epoch: 97 | Iteration: 15936 | Loss: 1.0224062482130232\n",
            "Epoch: 97 | Iteration: 16000 | Loss: 0.49100340236916273\n",
            "Epoch: 97 | Iteration: 16064 | Loss: 0.06001492160393876\n",
            "Epoch: 97 | Iteration: 16128 | Loss: 0.3320930653168932\n",
            "Epoch: 97 | Iteration: 16192 | Loss: 0.010829351647329559\n",
            "Epoch: 97 | Iteration: 16256 | Loss: 0.2117765696198516\n",
            "Epoch: 97 | Iteration: 16320 | Loss: 0.16043866868593532\n",
            "Epoch: 97 | Iteration: 16384 | Loss: 0.40186847696266675\n",
            "Epoch: 97 | Iteration: 16448 | Loss: 0.1591028930001497\n",
            "Epoch: 97 | Iteration: 16512 | Loss: 0.7398957040875227\n",
            "Epoch: 97 | Iteration: 16576 | Loss: 0.055332087940092686\n",
            "Epoch: 97 | Iteration: 16640 | Loss: 0.5103439645714527\n",
            "Epoch: 97 | Iteration: 16704 | Loss: 0.30487086116280154\n",
            "Epoch: 97 | Iteration: 16768 | Loss: 0.34914319187449006\n",
            "Epoch: 97 | Iteration: 16832 | Loss: 0.4235470756772598\n",
            "Epoch: 97 | Iteration: 16896 | Loss: 0.36145895877734546\n",
            "Epoch: 97 | Iteration: 16960 | Loss: 0.2889611555967135\n",
            "Epoch: 97 | Iteration: 17024 | Loss: 0.292830689776621\n",
            "Epoch: 97 | Iteration: 17088 | Loss: 0.5268841179983743\n",
            "Epoch: 97 | Iteration: 17152 | Loss: 0.405468740107927\n",
            "Epoch: 97 | Iteration: 17216 | Loss: 0.4843448804302497\n",
            "Epoch: 97 | Iteration: 17280 | Loss: 0.01247525951461167\n",
            "Epoch: 97 | Iteration: 17344 | Loss: 0.4146896315984021\n",
            "Epoch: 97 | Iteration: 17408 | Loss: 0.12705297607678312\n",
            "Epoch: 97 | Iteration: 17472 | Loss: 0.19121632882512501\n",
            "Epoch: 97 | Iteration: 17536 | Loss: 0.6200984103003085\n",
            "Epoch: 97 | Iteration: 17600 | Loss: 0.1952131383747302\n",
            "Epoch: 97 | Iteration: 17664 | Loss: 0.3748748629984088\n",
            "Epoch: 97 | Iteration: 17728 | Loss: 1.473218753277168\n",
            "Epoch: 97 | Iteration: 17792 | Loss: 0.21101387021331927\n",
            "Epoch: 97 | Iteration: 17856 | Loss: 0.3950164097230835\n",
            "Epoch: 97 | Iteration: 17920 | Loss: 0.04053973598431583\n",
            "Epoch: 97 | Iteration: 17984 | Loss: 0.07815992993838464\n",
            "Epoch: 97 | Iteration: 18048 | Loss: 0.45750210299402116\n",
            "Epoch: 97 | Iteration: 18112 | Loss: 0.21216443081576677\n",
            "Epoch: 97 | Iteration: 18176 | Loss: 0.10268306587697573\n",
            "Epoch: 97 | Iteration: 18240 | Loss: 0.599021740583916\n",
            "Epoch: 97 | Iteration: 18304 | Loss: 0.42586065968231807\n",
            "Epoch: 97 | Iteration: 18368 | Loss: 0.34020041406383994\n",
            "Epoch: 97 | Iteration: 18432 | Loss: 0.7060270527297376\n",
            "Epoch: 97 | Iteration: 18496 | Loss: 0.15624715082459376\n",
            "Epoch: 97 | Iteration: 18560 | Loss: 0.19007808958928052\n",
            "Epoch: 97 | Iteration: 18624 | Loss: 0.07103528360087547\n",
            "Epoch: 97 | Iteration: 18688 | Loss: 0.3487446044027478\n",
            "Epoch: 97 | Iteration: 18752 | Loss: 0.07578149375459191\n",
            "Epoch: 97 | Iteration: 18816 | Loss: 0.08407799874420681\n",
            "Epoch: 97 | Iteration: 18880 | Loss: 0.21167671247507233\n",
            "Epoch: 97 | Iteration: 18944 | Loss: 0.19220219488469442\n",
            "Epoch: 97 | Iteration: 19008 | Loss: 0.13629428128491103\n",
            "Epoch: 97 | Iteration: 19072 | Loss: 1.3301245630924443\n",
            "Epoch: 97 | Iteration: 19136 | Loss: 0.15356322659502109\n",
            "Epoch: 97 | Iteration: 19200 | Loss: 0.17614342861445006\n",
            "Epoch: 97 | Iteration: 19264 | Loss: 0.4947683684064006\n",
            "Epoch: 97 | Iteration: 19328 | Loss: 1.5642101800349235\n",
            "Epoch: 97 | Iteration: 19392 | Loss: 0.30780200419365644\n",
            "Epoch: 97 | Iteration: 19456 | Loss: 0.23851296947046535\n",
            "Epoch: 97 | Iteration: 19520 | Loss: 0.05402186460489018\n",
            "Epoch: 97 | Iteration: 19584 | Loss: 0.11421476625336113\n",
            "Epoch: 97 | Iteration: 19648 | Loss: 0.10454841647963606\n",
            "Epoch: 97 | Iteration: 19712 | Loss: 0.013788865153645823\n",
            "Epoch: 97 | Iteration: 19776 | Loss: 0.8526698012822975\n",
            "Epoch: 97 | Iteration: 19840 | Loss: 0.39238578986528955\n",
            "Epoch: 97 | Iteration: 19904 | Loss: 0.36591047835253904\n",
            "Epoch: 97 | Iteration: 19968 | Loss: 0.29568395481594556\n",
            "Epoch: 97 | Iteration: 20032 | Loss: 0.25790945363373646\n",
            "Epoch: 97 | Iteration: 20096 | Loss: 0.09041940124883277\n",
            "Epoch: 97 | Iteration: 20160 | Loss: 0.8504343768699822\n",
            "Epoch: 97 | Iteration: 20224 | Loss: 0.43758507470141367\n",
            "Epoch: 97 | Iteration: 20288 | Loss: 0.8112595650692124\n",
            "Epoch: 97 | Iteration: 20352 | Loss: 0.006335535698749739\n",
            "Epoch: 97 | Iteration: 20416 | Loss: 0.015030944770891463\n",
            "Epoch: 97 | Iteration: 20480 | Loss: 0.0999301327514702\n",
            "Epoch: 97 | Iteration: 20544 | Loss: 0.19623709597325079\n",
            "Epoch: 97 | Iteration: 20608 | Loss: 0.12488900086763832\n",
            "Epoch: 97 | Iteration: 20672 | Loss: 1.7367269799334286\n",
            "Epoch: 97 | Iteration: 20736 | Loss: 1.103666536898451\n",
            "Epoch: 97 | Iteration: 20800 | Loss: 0.3739190042195445\n",
            "Epoch: 97 | Iteration: 20864 | Loss: 0.331257583396635\n",
            "Epoch: 97 | Iteration: 20928 | Loss: 0.2755492713692401\n",
            "Epoch: 97 | Iteration: 20992 | Loss: 0.38651672334768966\n",
            "Epoch: 97 | Iteration: 21056 | Loss: 0.26986028181983546\n",
            "Epoch: 97 | Iteration: 21120 | Loss: 0.33844013088436986\n",
            "Epoch: 97 | Iteration: 21184 | Loss: 0.20992575032421293\n",
            "Epoch: 97 | Iteration: 21248 | Loss: 0.19791440368716773\n",
            "Epoch: 97 | Iteration: 21312 | Loss: 0.43190338863299577\n",
            "Epoch: 97 | Iteration: 21376 | Loss: 0.17143626410426147\n",
            "Epoch: 97 | Iteration: 21440 | Loss: 0.5901013410022564\n",
            "Epoch: 97 | Iteration: 21504 | Loss: 0.1933564347408928\n",
            "Epoch: 97 | Iteration: 21568 | Loss: 1.385890371561512\n",
            "Epoch: 97 | Iteration: 21632 | Loss: 0.23646174867064104\n",
            "Epoch: 97 | Iteration: 21696 | Loss: 0.10047345558299374\n",
            "Epoch: 97 | Iteration: 21760 | Loss: 0.00595350282906134\n",
            "Epoch: 97 | Iteration: 21824 | Loss: 0.003466180543576169\n",
            "Epoch: 97 | Iteration: 21888 | Loss: 0.5405272685194487\n",
            "Epoch: 97 | Iteration: 21952 | Loss: 0.3610651565142055\n",
            "Epoch: 97 | Iteration: 22016 | Loss: 0.0734113923473384\n",
            "Epoch: 97 | Iteration: 22080 | Loss: 0.4162757119543704\n",
            "Epoch: 97 | Iteration: 22144 | Loss: 0.3473441235622728\n",
            "Epoch: 97 | Iteration: 22208 | Loss: 0.23376115640972217\n",
            "Epoch: 97 | Iteration: 22272 | Loss: 0.7194930384344254\n",
            "Epoch: 97 | Iteration: 22336 | Loss: 0.03842912787073428\n",
            "Epoch: 97 | Iteration: 22400 | Loss: 0.19117873757067047\n",
            "Epoch: 97 | Iteration: 22464 | Loss: 0.5962194194400403\n",
            "Epoch: 97 | Iteration: 22528 | Loss: 0.9260464423121475\n",
            "Epoch: 97 | Iteration: 22592 | Loss: 0.4597112518470866\n",
            "Epoch: 97 | Iteration: 22656 | Loss: 0.12493970492318293\n",
            "Epoch: 97 | Iteration: 22720 | Loss: 0.5557234435101053\n",
            "Epoch: 97 | Iteration: 22784 | Loss: 0.1563690460645516\n",
            "Epoch: 97 | Iteration: 22848 | Loss: 0.26051985543354306\n",
            "Epoch: 97 | Iteration: 22912 | Loss: 0.0016963586859144687\n",
            "Epoch: 97 | Iteration: 22976 | Loss: 0.259566076309776\n",
            "Epoch: 97 | Iteration: 23040 | Loss: 0.22103855325742539\n",
            "Epoch: 97 | Iteration: 23104 | Loss: 0.11981786776222982\n",
            "Epoch: 97 | Iteration: 23168 | Loss: 0.4851551308990495\n",
            "Epoch: 97 | Iteration: 23232 | Loss: 0.02402154493703404\n",
            "Epoch: 97 | Iteration: 23296 | Loss: 0.11579230431141378\n",
            "Epoch: 97 | Iteration: 23360 | Loss: 0.3029262819876302\n",
            "Epoch: 97 | Iteration: 23424 | Loss: 0.08345462107261689\n",
            "Epoch: 97 | Iteration: 23488 | Loss: 0.08740536983888113\n",
            "Epoch: 97 | Iteration: 23552 | Loss: 0.6295856432325204\n",
            "Epoch: 97 | Iteration: 23616 | Loss: 0.6980860605795509\n",
            "Epoch: 97 | Iteration: 23680 | Loss: 0.8289182342072339\n",
            "Epoch: 97 | Iteration: 23744 | Loss: 0.012257748536208014\n",
            "Epoch: 97 | Iteration: 23808 | Loss: 0.3689510282539544\n",
            "Epoch: 97 | Iteration: 23872 | Loss: 0.5995241192550707\n",
            "Epoch: 97 | Iteration: 23936 | Loss: 1.0044203388186363\n",
            "Epoch: 97 | Iteration: 24000 | Loss: 0.15903006557563895\n",
            "Epoch: 97 | Iteration: 24064 | Loss: 0.07503229481202187\n",
            "Epoch: 97 | Iteration: 24128 | Loss: 0.1479728982328086\n",
            "Epoch: 97 | Iteration: 24192 | Loss: 0.2670530391739616\n",
            "Epoch: 97 | Iteration: 24256 | Loss: 0.6982246042860094\n",
            "Epoch: 97 | Iteration: 24320 | Loss: 0.3847783408520508\n",
            "Epoch: 97 | Iteration: 24384 | Loss: 0.009359775180377818\n",
            "Epoch: 97 | Iteration: 24448 | Loss: 0.21914343302720074\n",
            "Epoch: 97 | Iteration: 24512 | Loss: 0.18649079482293215\n",
            "Epoch: 97 | Iteration: 24576 | Loss: 0.36317127318288966\n",
            "Epoch: 97 | Iteration: 24640 | Loss: 0.18444394021925692\n",
            "Epoch: 97 | Iteration: 24704 | Loss: 0.11172973706251994\n",
            "Epoch: 97 | Iteration: 24768 | Loss: 0.5898676665496031\n",
            "Epoch: 97 | Iteration: 24832 | Loss: 0.05363042899259671\n",
            "Epoch: 97 | Iteration: 24896 | Loss: 0.26670716851784415\n",
            "Epoch: 97 | Iteration: 24960 | Loss: 0.010939727500118296\n",
            "Epoch: 97 | Iteration: 25024 | Loss: 0.041523598886317986\n",
            "Epoch: 97 | Iteration: 25088 | Loss: 0.3077378318655404\n",
            "Epoch: 97 | Iteration: 25152 | Loss: 0.7067540448226142\n",
            "Epoch: 97 | Iteration: 25216 | Loss: 0.18585153629031625\n",
            "Epoch: 97 | Iteration: 25280 | Loss: 0.14382874381630528\n",
            "Epoch: 97 | Iteration: 25344 | Loss: 0.023904415119269905\n",
            "Epoch: 97 | Iteration: 25408 | Loss: 0.030327971178000426\n",
            "Epoch: 97 | Iteration: 25472 | Loss: 0.5608903972593792\n",
            "Epoch: 97 | Iteration: 25536 | Loss: 1.188329198143669\n",
            "Epoch: 97 | Iteration: 25600 | Loss: 0.0689793326619409\n",
            "Epoch: 97 | Iteration: 25664 | Loss: 0.6009336827709434\n",
            "Epoch: 97 | Iteration: 25728 | Loss: 0.5724593645981045\n",
            "Epoch: 97 | Iteration: 25792 | Loss: 0.8356473163788753\n",
            "Epoch: 97 | Iteration: 25856 | Loss: 0.07845027955524489\n",
            "Epoch: 97 | Iteration: 25920 | Loss: 0.11033507313855384\n",
            "Epoch: 97 | Iteration: 25984 | Loss: 0.045566066324279314\n",
            "Epoch: 97 | Iteration: 26048 | Loss: 0.3019213092254153\n",
            "Epoch: 97 | Iteration: 26112 | Loss: 0.2589545541014217\n",
            "Epoch: 97 | Iteration: 26176 | Loss: 0.18692017047382448\n",
            "Epoch: 97 | Iteration: 26240 | Loss: 0.20207658763564335\n",
            "Epoch: 97 | Iteration: 26304 | Loss: 0.5303140874388144\n",
            "Epoch: 97 | Iteration: 26368 | Loss: 0.8460377185410708\n",
            "Epoch: 97 | Iteration: 26432 | Loss: 0.9294319521336201\n",
            "Epoch: 97 | Iteration: 26496 | Loss: 1.430292200441068\n",
            "Epoch: 97 | Iteration: 26560 | Loss: 0.9278346553703065\n",
            "Epoch: 97 | Iteration: 26624 | Loss: 0.9028150933229486\n",
            "Epoch: 97 | Iteration: 26688 | Loss: 0.7943961584105295\n",
            "Epoch: 97 | Iteration: 26752 | Loss: 0.9402972653269764\n",
            "Epoch: 97 | Iteration: 26816 | Loss: 0.26347905327355026\n",
            "Epoch: 97 | Iteration: 26880 | Loss: 1.0871428755605552\n",
            "Epoch: 97 | Iteration: 26944 | Loss: 0.015295689777678053\n",
            "Epoch: 97 | Iteration: 27008 | Loss: 0.12024182410514797\n",
            "Epoch: 97 | Iteration: 27072 | Loss: 0.07072879528719268\n",
            "Epoch: 97 | Iteration: 27136 | Loss: 1.599075944880632\n",
            "Epoch: 97 | Iteration: 27200 | Loss: 0.8940398889040884\n",
            "Epoch: 97 | Iteration: 27264 | Loss: 0.11462766191465956\n",
            "Epoch: 97 | Iteration: 27328 | Loss: 0.019586474731366314\n",
            "Epoch: 97 | Iteration: 27392 | Loss: 0.14004834392578397\n",
            "Epoch: 97 | Iteration: 27456 | Loss: 0.5466565198734296\n",
            "Epoch: 97 | Iteration: 27520 | Loss: 0.6225527407145199\n",
            "Epoch: 97 | Iteration: 27584 | Loss: 0.7135992740945281\n",
            "Epoch: 97 | Iteration: 27648 | Loss: 0.567081127149371\n",
            "Epoch: 97 | Iteration: 27712 | Loss: 0.212267380671939\n",
            "Epoch: 97 | Iteration: 27776 | Loss: 0.2992411479322243\n",
            "Epoch: 97 | Iteration: 27840 | Loss: 0.9018312710241365\n",
            "Epoch: 97 | Iteration: 27904 | Loss: 0.08445407352894839\n",
            "Epoch: 97 | Iteration: 27968 | Loss: 0.02482445996588945\n",
            "Epoch: 97 | Iteration: 28032 | Loss: 0.21177306792050105\n",
            "Epoch: 97 | Iteration: 28096 | Loss: 0.1364293554186272\n",
            "Epoch: 97 | Iteration: 28160 | Loss: 0.6194345913821826\n",
            "Epoch: 97 | Iteration: 28224 | Loss: 0.07234104702552985\n",
            "Epoch: 97 | Iteration: 28288 | Loss: 0.01163862698445605\n",
            "Epoch: 97 | Iteration: 28352 | Loss: 0.6842794495039453\n",
            "Epoch: 97 | Iteration: 28416 | Loss: 0.09747279289962985\n",
            "Epoch: 97 | Iteration: 28480 | Loss: 0.08364963802451918\n",
            "Epoch: 97 | Iteration: 28544 | Loss: 0.11830948617429218\n",
            "Epoch: 97 | Iteration: 28608 | Loss: 0.6571029379292332\n",
            "Epoch: 97 | Iteration: 28672 | Loss: 0.2806568481279383\n",
            "Epoch: 97 | Iteration: 28736 | Loss: 0.10665164602946312\n",
            "Epoch: 97 | Iteration: 28800 | Loss: 0.18835924750712565\n",
            "Epoch: 97 | Iteration: 28864 | Loss: 0.10687165389964015\n",
            "Epoch: 97 | Iteration: 28928 | Loss: 0.4659610981612774\n",
            "Epoch: 97 | Iteration: 28992 | Loss: 0.20961794028844727\n",
            "Epoch: 97 | Iteration: 29056 | Loss: 0.16066327471473568\n",
            "Epoch: 97 | Iteration: 29120 | Loss: 0.2956021812089143\n",
            "Epoch: 97 | Iteration: 29184 | Loss: 0.6073523450681059\n",
            "Epoch: 97 | Iteration: 29248 | Loss: 0.3701799485363383\n",
            "Epoch: 97 | Iteration: 29312 | Loss: 0.11437613573037676\n",
            "Epoch: 97 | Iteration: 29376 | Loss: 0.16549313320510303\n",
            "Epoch: 97 | Iteration: 29440 | Loss: 0.45816981894176223\n",
            "Epoch: 97 | Iteration: 29504 | Loss: 0.19590354296070223\n",
            "Epoch: 97 | Iteration: 29568 | Loss: 0.2128841641215783\n",
            "Epoch: 97 | Iteration: 29632 | Loss: 0.13983207820746155\n",
            "Epoch: 97 | Iteration: 29696 | Loss: 0.5231912507254342\n",
            "Epoch: 97 | Iteration: 29760 | Loss: 0.04648111233593057\n",
            "Epoch: 97 | Iteration: 29824 | Loss: 0.2586741372874837\n",
            "Epoch: 97 | Iteration: 29888 | Loss: 0.4831893215605722\n",
            "Epoch: 97 | Iteration: 29952 | Loss: 0.06247172316384262\n",
            "Epoch: 97 | Iteration: 30016 | Loss: 0.6517307101314644\n",
            "Epoch: 97 | Iteration: 30080 | Loss: 0.22640370702846882\n",
            "Epoch: 97 | Iteration: 30144 | Loss: 0.2805536154692597\n",
            "Epoch: 97 | Iteration: 30208 | Loss: 0.10357045887017198\n",
            "Epoch: 97 | Iteration: 30272 | Loss: 0.05392558796908721\n",
            "Epoch: 97 | Iteration: 30336 | Loss: 0.22548067139268768\n",
            "Epoch: 97 | Iteration: 30400 | Loss: 0.051404593422035266\n",
            "Epoch: 97 | Iteration: 30464 | Loss: 0.285325308506031\n",
            "Epoch: 97 | Iteration: 30528 | Loss: 0.1416901064171314\n",
            "Epoch: 97 | Iteration: 30592 | Loss: 0.093349807677005\n",
            "Epoch: 97 | Iteration: 30656 | Loss: 0.5758007864439112\n",
            "Epoch: 97 | Iteration: 30720 | Loss: 0.23883003601928315\n",
            "Epoch: 97 | Iteration: 30784 | Loss: 0.1832241891699532\n",
            "Epoch: 97 | Iteration: 30848 | Loss: 0.33872667064282075\n",
            "Epoch: 97 | Iteration: 30912 | Loss: 0.7382550699751738\n",
            "Epoch: 97 | Iteration: 30976 | Loss: 0.17867943193942898\n",
            "Epoch: 97 | Iteration: 31040 | Loss: 0.47549190980013556\n",
            "Epoch: 97 | Iteration: 31104 | Loss: 0.4378514060404807\n",
            "Epoch: 97 | Iteration: 31168 | Loss: 1.693500331471561\n",
            "Epoch: 97 | Iteration: 31232 | Loss: 0.17553948278487952\n",
            "Epoch: 97 | Iteration: 31296 | Loss: 0.8200668134423486\n",
            "Epoch: 97 | Iteration: 31360 | Loss: 0.27040644537248704\n",
            "Epoch: 97 | Iteration: 31424 | Loss: 0.30545715406771456\n",
            "Epoch: 97 | Iteration: 31488 | Loss: 0.08181875807518738\n",
            "Epoch: 97 | Iteration: 31552 | Loss: 1.097994710166304\n",
            "Epoch: 97 | Iteration: 31616 | Loss: 0.33993412340470813\n",
            "Epoch: 97 | Iteration: 31680 | Loss: 1.360784873199072\n",
            "Epoch: 97 | Iteration: 31744 | Loss: 0.7506870364797149\n",
            "Epoch: 97 | Iteration: 31808 | Loss: 0.20088282136095656\n",
            "Epoch: 97 | Iteration: 31872 | Loss: 0.17776884311395388\n",
            "Epoch: 97 | Iteration: 31936 | Loss: 0.24552832912192282\n",
            "Epoch: 97 | Iteration: 32000 | Loss: 0.2846822005971924\n",
            "Epoch: 97 | Iteration: 32064 | Loss: 0.3062544113624986\n",
            "Epoch: 97 | Iteration: 32128 | Loss: 0.15631438429913658\n",
            "Epoch: 97 | Iteration: 32192 | Loss: 0.4524460707693676\n",
            "Epoch: 97 | Iteration: 32256 | Loss: 0.20506227359651036\n",
            "Epoch: 97 | Iteration: 32320 | Loss: 0.7181043466319382\n",
            "Epoch: 97 | Iteration: 32384 | Loss: 0.3163361872852084\n",
            "Epoch: 97 | Iteration: 32448 | Loss: 1.0364107203396569\n",
            "Epoch: 97 | Iteration: 32512 | Loss: 0.29503306675925617\n",
            "Epoch: 97 | Iteration: 32576 | Loss: 0.0006863858307878605\n",
            "Epoch: 97 | Iteration: 32640 | Loss: 0.31764815171967276\n",
            "Epoch: 97 | Iteration: 32704 | Loss: 0.2983378217995747\n",
            "Epoch: 97 | Iteration: 32768 | Loss: 0.8685128948318698\n",
            "Epoch: 97 | Iteration: 32832 | Loss: 0.6582125505319363\n",
            "Epoch: 97 | Iteration: 32896 | Loss: 0.6804341855975822\n",
            "Epoch: 97 | Iteration: 32960 | Loss: 0.014217941889132762\n",
            "Epoch: 97 | Iteration: 33024 | Loss: 0.2700923038328972\n",
            "Epoch: 97 | Iteration: 33088 | Loss: 0.03335995363783373\n",
            "Epoch: 97 | Iteration: 33152 | Loss: 0.07782634475786492\n",
            "Epoch: 97 | Iteration: 33216 | Loss: 0.21456896893092645\n",
            "Epoch: 97 | Iteration: 33280 | Loss: 0.2633418317002024\n",
            "Epoch: 97 | Iteration: 33344 | Loss: 0.3422283007158611\n",
            "Epoch: 97 | Iteration: 33408 | Loss: 0.6099326107317627\n",
            "Epoch: 97 | Iteration: 33472 | Loss: 0.5249951694040108\n",
            "Epoch: 97 | Iteration: 33536 | Loss: 0.41944832871285503\n",
            "Epoch: 97 | Iteration: 33600 | Loss: 0.06944445306850283\n",
            "Epoch: 97 | Iteration: 33664 | Loss: 0.1347668876439688\n",
            "Epoch: 97 | Iteration: 33728 | Loss: 0.46230064356238126\n",
            "Epoch: 97 | Iteration: 33792 | Loss: 0.11668929363962197\n",
            "Epoch: 97 | Iteration: 33856 | Loss: 0.010956624703324563\n",
            "Epoch: 97 | Iteration: 33920 | Loss: 0.012809972209263384\n",
            "Epoch: 97 | Iteration: 33984 | Loss: 0.5275214305852034\n",
            "Epoch: 97 | Iteration: 34048 | Loss: 0.4845068188343549\n",
            "Epoch: 97 | Iteration: 34112 | Loss: 0.012507182894332523\n",
            "Epoch: 97 | Iteration: 34176 | Loss: 0.05857742691113989\n",
            "Epoch: 97 | Iteration: 34240 | Loss: 0.00286143361018677\n",
            "Epoch: 97 | Iteration: 34304 | Loss: 0.10609361545621887\n",
            "Epoch: 97 | Iteration: 34368 | Loss: 1.3528204060338662\n",
            "Epoch: 97 | Iteration: 34432 | Loss: 0.1550030973410716\n",
            "Epoch: 97 | Iteration: 34496 | Loss: 0.16945627096289478\n",
            "Epoch: 97 | Iteration: 34560 | Loss: 0.12456831796847982\n",
            "Epoch: 97 | Iteration: 34624 | Loss: 0.6019033103218218\n",
            "Epoch: 97 | Iteration: 34688 | Loss: 0.8038194252666011\n",
            "Epoch: 97 | Iteration: 34752 | Loss: 0.36934353964178906\n",
            "Epoch: 97 | Iteration: 34816 | Loss: 0.7653208744989299\n",
            "Epoch: 97 | Iteration: 34880 | Loss: 0.46335710479964076\n",
            "Epoch: 97 | Iteration: 34944 | Loss: 0.04259230644709358\n",
            "Epoch: 97 | Iteration: 35008 | Loss: 1.2986638990127295\n",
            "Epoch: 97 | Iteration: 35072 | Loss: 0.030225571808581\n",
            "Epoch: 97 | Iteration: 35136 | Loss: 1.0018933627351971\n",
            "Epoch: 97 | Iteration: 35200 | Loss: 1.183529312530553\n",
            "Epoch: 97 | Iteration: 35264 | Loss: 0.11541190833225298\n",
            "Epoch: 97 | Iteration: 35328 | Loss: 0.15784926479311434\n",
            "Epoch: 97 | Iteration: 35392 | Loss: 0.2238020084150543\n",
            "Epoch: 97 | Iteration: 35456 | Loss: 1.1448195298041317\n",
            "Epoch: 97 | Iteration: 35520 | Loss: 0.1399599339813068\n",
            "Epoch: 97 | Iteration: 35584 | Loss: 0.988723988062282\n",
            "Epoch: 97 | Iteration: 35648 | Loss: 0.24243167962406978\n",
            "Epoch: 97 | Iteration: 35712 | Loss: 0.10213556171185265\n",
            "Epoch: 97 | Iteration: 35776 | Loss: 0.04234119369742188\n",
            "Epoch: 97 | Iteration: 35840 | Loss: 0.23481029068830475\n",
            "Epoch: 97 | Iteration: 35904 | Loss: 0.279711185096403\n",
            "Epoch: 97 | Iteration: 35968 | Loss: 0.06541074289484627\n",
            "Epoch: 97 | Iteration: 36032 | Loss: 0.9239055706521473\n",
            "Epoch: 97 | Iteration: 36096 | Loss: 1.320064849504199\n",
            "Epoch: 97 | Iteration: 36160 | Loss: 0.40865151450080356\n",
            "Epoch: 97 | Iteration: 36224 | Loss: 0.10608235725334612\n",
            "Epoch: 97 | Iteration: 36288 | Loss: 0.020562742272224693\n",
            "Epoch: 97 | Iteration: 36352 | Loss: 0.18200530613855143\n",
            "Epoch: 97 | Iteration: 36416 | Loss: 0.7325593471027771\n",
            "Epoch: 97 | Iteration: 36480 | Loss: 0.019866994650618214\n",
            "Epoch: 97 | Iteration: 36544 | Loss: 0.23991012034492895\n",
            "Epoch: 97 | Iteration: 36608 | Loss: 0.06147437377730878\n",
            "Epoch: 97 | Iteration: 36672 | Loss: 0.21639210644172374\n",
            "Epoch: 97 | Iteration: 36736 | Loss: 0.9292910878195684\n",
            "Epoch: 97 | Iteration: 36800 | Loss: 0.3887028682018573\n",
            "Epoch: 97 | Iteration: 36864 | Loss: 0.1513210242703729\n",
            "Epoch: 97 | Iteration: 36928 | Loss: 0.32888238318417395\n",
            "Epoch: 97 | Iteration: 36992 | Loss: 1.067471885521422\n",
            "Epoch: 97 | Iteration: 37056 | Loss: 0.2942166202169162\n",
            "Epoch: 97 | Iteration: 37120 | Loss: 0.4548440931780559\n",
            "Epoch: 97 | Iteration: 37184 | Loss: 0.10616844526777763\n",
            "Epoch: 97 | Iteration: 37248 | Loss: 0.37428067388463765\n",
            "Epoch: 97 | Iteration: 37312 | Loss: 0.9623764660362456\n",
            "Epoch: 97 | Iteration: 37376 | Loss: 0.9479226429884325\n",
            "Epoch: 97 | Iteration: 37440 | Loss: 0.46556168276501575\n",
            "Epoch: 97 | Iteration: 37504 | Loss: 0.2134649550249887\n",
            "Epoch: 97 | Iteration: 37568 | Loss: 0.05327581048766196\n",
            "Epoch: 97 | Iteration: 37632 | Loss: 0.20236203456066248\n",
            "Epoch: 97 | Iteration: 37696 | Loss: 0.639008869448237\n",
            "Epoch: 97 | Iteration: 37760 | Loss: 0.7533743715939976\n",
            "Epoch: 97 | Iteration: 37824 | Loss: 0.9993699389314643\n",
            "Epoch: 97 | Iteration: 37888 | Loss: 0.06465840447252555\n",
            "Epoch: 97 | Iteration: 37952 | Loss: 0.04323226017066269\n",
            "Epoch: 97 | Iteration: 38016 | Loss: 0.1544716823491765\n",
            "Epoch: 97 | Iteration: 38080 | Loss: 0.06552456587323804\n",
            "Epoch: 97 | Iteration: 38144 | Loss: 0.15710982646138658\n",
            "Epoch: 97 | Iteration: 38208 | Loss: 0.06437367338701053\n",
            "Epoch: 97 | Iteration: 38272 | Loss: 0.4964856898824213\n",
            "Epoch: 97 | Iteration: 38336 | Loss: 0.699054046581951\n",
            "Epoch: 97 | Iteration: 38400 | Loss: 0.13934843084752113\n",
            "Epoch: 97 | Iteration: 38464 | Loss: 0.6078621635503911\n",
            "Epoch: 97 | Iteration: 38528 | Loss: 0.5267216094979704\n",
            "Epoch: 97 | Iteration: 38592 | Loss: 0.2946922763951673\n",
            "Epoch: 97 | Iteration: 38656 | Loss: 0.5818141983079395\n",
            "Epoch: 97 | Iteration: 38720 | Loss: 0.20910943054683206\n",
            "Epoch: 97 | Iteration: 38784 | Loss: 0.007161618087712346\n",
            "Epoch: 97 | Iteration: 38848 | Loss: 0.08110328552742058\n",
            "Epoch: 97 | Iteration: 38912 | Loss: 0.24820537480666688\n",
            "Epoch: 97 | Iteration: 38976 | Loss: 0.2005015929531038\n",
            "Epoch: 97 | Iteration: 39040 | Loss: 0.006459289258515857\n",
            "Epoch: 97 | Iteration: 39104 | Loss: 0.09596604876215573\n",
            "Epoch: 97 | Iteration: 39168 | Loss: 0.6996607589223965\n",
            "Epoch: 97 | Iteration: 39232 | Loss: 0.38015443207030286\n",
            "Epoch: 97 | Iteration: 39296 | Loss: 0.8937569855655616\n",
            "Epoch: 97 | Iteration: 39360 | Loss: 1.0287761608284012\n",
            "Epoch: 97 | Iteration: 39424 | Loss: 0.17609851578271862\n",
            "Epoch: 97 | Iteration: 39488 | Loss: 0.05041326047691615\n",
            "Epoch: 97 | Iteration: 39552 | Loss: 0.4186136516001064\n",
            "Epoch: 97 | Iteration: 39616 | Loss: 0.46113936890603513\n",
            "Epoch: 97 | Iteration: 39680 | Loss: 0.3700910992313474\n",
            "Epoch: 97 | Iteration: 39744 | Loss: 0.676853911404292\n",
            "Epoch: 97 | Iteration: 39808 | Loss: 0.6878878307713698\n",
            "Epoch: 97 | Iteration: 39872 | Loss: 0.15143788483697176\n",
            "Epoch: 97 | Iteration: 39936 | Loss: 0.05881489767970902\n",
            "Epoch: 97 | Iteration: 40000 | Loss: 0.23629944935786285\n",
            "Epoch: 97 | Iteration: 40064 | Loss: 0.22399100233267133\n",
            "Epoch: 97 | Iteration: 40128 | Loss: 0.14717660483676084\n",
            "Epoch: 97 | Iteration: 40192 | Loss: 0.30905747630092645\n",
            "Epoch: 97 | Iteration: 40256 | Loss: 0.42298954883746653\n",
            "Epoch: 97 | Iteration: 40320 | Loss: 0.19345503917583112\n",
            "Epoch: 97 | Iteration: 40384 | Loss: 0.22413293731413442\n",
            "Epoch: 97 | Iteration: 40448 | Loss: 0.06196656332611544\n",
            "Epoch: 97 | Iteration: 40512 | Loss: 0.09313632043895313\n",
            "Epoch: 97 | Iteration: 40576 | Loss: 0.6535076114473617\n",
            "Epoch: 97 | Iteration: 40640 | Loss: 0.620164742383001\n",
            "Epoch: 97 | Iteration: 40704 | Loss: 0.47792687062244155\n",
            "Epoch: 97 | Iteration: 40768 | Loss: 0.20107477800993712\n",
            "Epoch: 97 | Iteration: 40832 | Loss: 0.13010413331005793\n",
            "Epoch: 97 | Iteration: 40896 | Loss: 0.07728871203843263\n",
            "Epoch: 97 | Iteration: 40960 | Loss: 0.7796093300156665\n",
            "Epoch: 97 | Iteration: 41024 | Loss: 0.3984787193432974\n",
            "Epoch: 97 | Iteration: 41088 | Loss: 0.14047902020446698\n",
            "Epoch: 97 | Iteration: 41152 | Loss: 0.8275293425268018\n",
            "Epoch: 97 | Iteration: 41216 | Loss: 1.0558353411774757\n",
            "Epoch: 97 | Iteration: 41280 | Loss: 0.7970284956967877\n",
            "Epoch: 97 | Iteration: 41344 | Loss: 0.2894205242437474\n",
            "Epoch: 97 | Iteration: 41408 | Loss: 0.27738110089785606\n",
            "Epoch: 97 | Iteration: 41472 | Loss: 0.03935245639113377\n",
            "Epoch: 97 | Iteration: 41536 | Loss: 0.9772201298245009\n",
            "Epoch: 97 | Iteration: 41600 | Loss: 0.1262744145571446\n",
            "Epoch: 97 | Iteration: 41664 | Loss: 0.07152418841631891\n",
            "Epoch: 97 | Iteration: 41728 | Loss: 0.2557997129196028\n",
            "Epoch: 97 | Iteration: 41792 | Loss: 0.14392020378951276\n",
            "Epoch: 97 | Iteration: 41856 | Loss: 0.47848620440917067\n",
            "Epoch: 97 | Iteration: 41920 | Loss: 0.3426871494347931\n",
            "Epoch: 97 | Iteration: 41984 | Loss: 0.08710413339153207\n",
            "Epoch: 97 | Iteration: 42048 | Loss: 0.03221592248697875\n",
            "Epoch: 97 | Iteration: 42112 | Loss: 0.18269853496345095\n",
            "Epoch: 97 | Iteration: 42176 | Loss: 0.19328445602427\n",
            "Epoch: 97 | Iteration: 42240 | Loss: 0.06586999076497126\n",
            "Epoch: 97 | Iteration: 42304 | Loss: 0.6940038156501807\n",
            "Epoch: 97 | Iteration: 42368 | Loss: 0.5234610453586461\n",
            "Epoch: 97 | Iteration: 42432 | Loss: 0.7861256510890438\n",
            "Epoch: 97 | Iteration: 42496 | Loss: 0.5681996203408665\n",
            "Epoch: 97 | Iteration: 42560 | Loss: 0.9037694841592175\n",
            "Epoch: 97 | Iteration: 42624 | Loss: 0.6085707996076616\n",
            "Epoch: 97 | Iteration: 42688 | Loss: 0.35887870238813535\n",
            "Epoch: 97 | Iteration: 42752 | Loss: 1.0340837610070084\n",
            "Epoch: 97 | Iteration: 42816 | Loss: 1.031078451271121\n",
            "Epoch: 97 | Iteration: 42880 | Loss: 0.7387889186321643\n",
            "Epoch: 97 | Iteration: 42944 | Loss: 0.42897340862926675\n",
            "Epoch: 97 | Iteration: 43008 | Loss: 0.616622094757493\n",
            "Epoch: 97 | Iteration: 43072 | Loss: 1.3294723315818453\n",
            "Epoch: 97 | Iteration: 43136 | Loss: 0.038351597834821866\n",
            "Epoch: 97 | Iteration: 43200 | Loss: 0.08779809014237595\n",
            "Epoch: 97 | Iteration: 43264 | Loss: 0.0027088419010832154\n",
            "Epoch: 97 | Iteration: 43328 | Loss: 0.015618882163175323\n",
            "Epoch: 97 | Iteration: 43392 | Loss: 0.3955699086169816\n",
            "Epoch: 97 | Iteration: 43456 | Loss: 0.08532117854567604\n",
            "Epoch: 97 | Iteration: 43520 | Loss: 0.38561424986958404\n",
            "Epoch: 97 | Iteration: 43584 | Loss: 0.19635118257625866\n",
            "Epoch: 97 | Iteration: 43648 | Loss: 1.244371238840952\n",
            "Epoch: 97 | Iteration: 43712 | Loss: 0.00609746682978581\n",
            "Epoch: 97 | Iteration: 43776 | Loss: 0.04784814643473077\n",
            "Epoch: 97 | Iteration: 43840 | Loss: 0.7957990543446783\n",
            "Epoch: 97 | Iteration: 43904 | Loss: 0.25240651533073605\n",
            "Epoch: 97 | Iteration: 43968 | Loss: 0.19910859549416696\n",
            "Epoch: 97 | Iteration: 44032 | Loss: 0.24694428976724622\n",
            "Epoch: 97 | Iteration: 44096 | Loss: 0.2744150375565858\n",
            "Epoch: 97 | Iteration: 44160 | Loss: 0.10110631553878172\n",
            "Epoch: 97 | Iteration: 44224 | Loss: 0.18064963850064783\n",
            "Epoch: 97 | Iteration: 44288 | Loss: 0.33623476072975134\n",
            "Epoch: 97 | Iteration: 44352 | Loss: 0.3177482939559737\n",
            "Epoch: 97 | Iteration: 44416 | Loss: 1.07372337278152\n",
            "Epoch: 97 | Iteration: 44480 | Loss: 0.3070562271587577\n",
            "Epoch: 97 | Iteration: 44544 | Loss: 0.030834488667300798\n",
            "Epoch: 97 | Iteration: 44608 | Loss: 0.40341881476119934\n",
            "Epoch: 97 | Iteration: 44672 | Loss: 0.0647206088410746\n",
            "Epoch: 97 | Iteration: 44736 | Loss: 0.037151227651963215\n",
            "Epoch: 97 | Iteration: 44800 | Loss: 0.3985614304042856\n",
            "Epoch: 97 | Iteration: 44864 | Loss: 0.18674119242046852\n",
            "Epoch: 97 | Iteration: 44928 | Loss: 0.6362657845614592\n",
            "Epoch: 97 | Iteration: 44992 | Loss: 0.16034106706692933\n",
            "Epoch: 97 | Iteration: 45056 | Loss: 0.16088404372999604\n",
            "Epoch: 97 | Iteration: 45120 | Loss: 0.8393497193822005\n",
            "Epoch: 97 | Iteration: 45184 | Loss: 0.9742716247159512\n",
            "Epoch: 97 | Iteration: 45248 | Loss: 0.34311733984326986\n",
            "Epoch: 97 | Iteration: 45312 | Loss: 0.5656672872565205\n",
            "Epoch: 97 | Iteration: 45376 | Loss: 0.30600998396866524\n",
            "Epoch: 97 | Iteration: 45440 | Loss: 0.7840183504998013\n",
            "Epoch: 97 | Iteration: 45504 | Loss: 0.3595811037896893\n",
            "Epoch: 97 | Iteration: 45568 | Loss: 0.5906593073240475\n",
            "Epoch: 97 | Iteration: 45632 | Loss: 0.146058638271677\n",
            "Epoch: 97 | Iteration: 45696 | Loss: 0.007236805642344366\n",
            "Epoch: 97 | Iteration: 45760 | Loss: 0.5312776141248459\n",
            "Epoch: 97 | Iteration: 45824 | Loss: 0.32483350815663886\n",
            "Epoch: 97 | Iteration: 45888 | Loss: 0.6633977248627538\n",
            "Epoch: 97 | Iteration: 45952 | Loss: 0.5103762617653167\n",
            "Epoch: 97 | Iteration: 46016 | Loss: 1.013220822573484\n",
            "Epoch: 97 | Iteration: 46080 | Loss: 1.176842550909703\n",
            "Epoch: 97 | Iteration: 46144 | Loss: 0.07124842215471464\n",
            "Epoch: 97 | Iteration: 46208 | Loss: 1.3523699679513783\n",
            "Epoch: 97 | Iteration: 46272 | Loss: 1.2820062651575952\n",
            "Epoch: 97 | Iteration: 46336 | Loss: 0.45232922251077823\n",
            "Epoch: 97 | Iteration: 46400 | Loss: 0.3050263792222421\n",
            "Epoch: 97 | Iteration: 46464 | Loss: 0.01626314142269398\n",
            "Epoch: 97 | Iteration: 46528 | Loss: 0.037693462505703934\n",
            "Epoch: 97 | Iteration: 46592 | Loss: 0.0430589128280366\n",
            "Epoch: 97 | Iteration: 46656 | Loss: 0.2228106764330653\n",
            "Epoch: 97 | Iteration: 46720 | Loss: 0.85008385650449\n",
            "Epoch: 97 | Iteration: 46784 | Loss: 0.04115690560816783\n",
            "Epoch: 97 | Iteration: 46848 | Loss: 0.7178054175195554\n",
            "Epoch: 97 | Iteration: 46912 | Loss: 0.31447529711765954\n",
            "Epoch: 97 | Iteration: 46976 | Loss: 1.2630101785314447\n",
            "Epoch: 97 | Iteration: 47040 | Loss: 0.039164087097258535\n",
            "Epoch: 97 | Iteration: 47104 | Loss: 0.29177703713792247\n",
            "Epoch: 97 | Iteration: 47168 | Loss: 0.31594385035969313\n",
            "Epoch: 97 | Iteration: 47232 | Loss: 1.3453904169505861\n",
            "Epoch: 97 | Iteration: 47296 | Loss: 0.3446858028486305\n",
            "Epoch: 97 | Iteration: 47360 | Loss: 0.8625507148359144\n",
            "Epoch: 97 | Iteration: 47424 | Loss: 0.2663478684842013\n",
            "Epoch: 97 | Iteration: 47488 | Loss: 0.08899642010762196\n",
            "Epoch: 97 | Iteration: 47552 | Loss: 1.2040516751615462\n",
            "Epoch: 97 | Iteration: 47616 | Loss: 0.013739608319691923\n",
            "Epoch: 97 | Iteration: 47680 | Loss: 1.3600730053778491\n",
            "Epoch: 97 | Iteration: 47744 | Loss: 0.8288145482912671\n",
            "Epoch: 97 | Iteration: 47808 | Loss: 0.0786345508175531\n",
            "Epoch: 97 | Iteration: 47872 | Loss: 0.7435187356689037\n",
            "Epoch: 97 | Iteration: 47936 | Loss: 0.46165072187132544\n",
            "Epoch: 98 | Iteration: 0 | Loss: 0.15044420765103164\n",
            "Epoch: 98 | Iteration: 64 | Loss: 0.566591341550194\n",
            "Epoch: 98 | Iteration: 128 | Loss: 1.0882860679828088\n",
            "Epoch: 98 | Iteration: 192 | Loss: 0.2528305832757663\n",
            "Epoch: 98 | Iteration: 256 | Loss: 0.16514526295969273\n",
            "Epoch: 98 | Iteration: 320 | Loss: 0.015674111726137877\n",
            "Epoch: 98 | Iteration: 384 | Loss: 0.1213737355551175\n",
            "Epoch: 98 | Iteration: 448 | Loss: 1.76441731503987\n",
            "Epoch: 98 | Iteration: 512 | Loss: 0.4238276607813895\n",
            "Epoch: 98 | Iteration: 576 | Loss: 0.37473082387144796\n",
            "Epoch: 98 | Iteration: 640 | Loss: 0.35724774408277865\n",
            "Epoch: 98 | Iteration: 704 | Loss: 0.16846516683881957\n",
            "Epoch: 98 | Iteration: 768 | Loss: 0.4380756735998135\n",
            "Epoch: 98 | Iteration: 832 | Loss: 0.4146687443659398\n",
            "Epoch: 98 | Iteration: 896 | Loss: 0.8656455171323667\n",
            "Epoch: 98 | Iteration: 960 | Loss: 0.4785812220553278\n",
            "Epoch: 98 | Iteration: 1024 | Loss: 0.9024686010725087\n",
            "Epoch: 98 | Iteration: 1088 | Loss: 0.7033183025401217\n",
            "Epoch: 98 | Iteration: 1152 | Loss: 0.28911172870072194\n",
            "Epoch: 98 | Iteration: 1216 | Loss: 0.8489997958723763\n",
            "Epoch: 98 | Iteration: 1280 | Loss: 0.16758822579236024\n",
            "Epoch: 98 | Iteration: 1344 | Loss: 1.534342954838662\n",
            "Epoch: 98 | Iteration: 1408 | Loss: 0.09603926991563044\n",
            "Epoch: 98 | Iteration: 1472 | Loss: 0.519876600362636\n",
            "Epoch: 98 | Iteration: 1536 | Loss: 0.04620091898908682\n",
            "Epoch: 98 | Iteration: 1600 | Loss: 0.9884240581043104\n",
            "Epoch: 98 | Iteration: 1664 | Loss: 0.09652914599781687\n",
            "Epoch: 98 | Iteration: 1728 | Loss: 0.1508514316554928\n",
            "Epoch: 98 | Iteration: 1792 | Loss: 0.18086724145477412\n",
            "Epoch: 98 | Iteration: 1856 | Loss: 0.1722333093894147\n",
            "Epoch: 98 | Iteration: 1920 | Loss: 0.8398517089208009\n",
            "Epoch: 98 | Iteration: 1984 | Loss: 0.19674335495253686\n",
            "Epoch: 98 | Iteration: 2048 | Loss: 0.31770933114745165\n",
            "Epoch: 98 | Iteration: 2112 | Loss: 0.03464861061532027\n",
            "Epoch: 98 | Iteration: 2176 | Loss: 0.36067818184141054\n",
            "Epoch: 98 | Iteration: 2240 | Loss: 0.09509243230596223\n",
            "Epoch: 98 | Iteration: 2304 | Loss: 0.1277214543460998\n",
            "Epoch: 98 | Iteration: 2368 | Loss: 0.19469937979877322\n",
            "Epoch: 98 | Iteration: 2432 | Loss: 0.012659728486521186\n",
            "Epoch: 98 | Iteration: 2496 | Loss: 0.1310691113849622\n",
            "Epoch: 98 | Iteration: 2560 | Loss: 0.3983441269517806\n",
            "Epoch: 98 | Iteration: 2624 | Loss: 0.8471350465897353\n",
            "Epoch: 98 | Iteration: 2688 | Loss: 0.8919894469696632\n",
            "Epoch: 98 | Iteration: 2752 | Loss: 0.06335886386671437\n",
            "Epoch: 98 | Iteration: 2816 | Loss: 0.12716312789136358\n",
            "Epoch: 98 | Iteration: 2880 | Loss: 0.314170949790479\n",
            "Epoch: 98 | Iteration: 2944 | Loss: 0.09989219802529903\n",
            "Epoch: 98 | Iteration: 3008 | Loss: 0.29111737922129033\n",
            "Epoch: 98 | Iteration: 3072 | Loss: 0.08221372172878957\n",
            "Epoch: 98 | Iteration: 3136 | Loss: 0.0014010790149018024\n",
            "Epoch: 98 | Iteration: 3200 | Loss: 0.09595598557645041\n",
            "Epoch: 98 | Iteration: 3264 | Loss: 0.07066988107396993\n",
            "Epoch: 98 | Iteration: 3328 | Loss: 0.06700893895832917\n",
            "Epoch: 98 | Iteration: 3392 | Loss: 0.051580152245015526\n",
            "Epoch: 98 | Iteration: 3456 | Loss: 0.21729850229041042\n",
            "Epoch: 98 | Iteration: 3520 | Loss: 0.1527157151088828\n",
            "Epoch: 98 | Iteration: 3584 | Loss: 0.06925324038764329\n",
            "Epoch: 98 | Iteration: 3648 | Loss: 0.5026442887867435\n",
            "Epoch: 98 | Iteration: 3712 | Loss: 0.10483267627665396\n",
            "Epoch: 98 | Iteration: 3776 | Loss: 0.33228471731675235\n",
            "Epoch: 98 | Iteration: 3840 | Loss: 0.006508121191121559\n",
            "Epoch: 98 | Iteration: 3904 | Loss: 0.19781876030818818\n",
            "Epoch: 98 | Iteration: 3968 | Loss: 0.07183261470591984\n",
            "Epoch: 98 | Iteration: 4032 | Loss: 0.14421373977933455\n",
            "Epoch: 98 | Iteration: 4096 | Loss: 0.2650782109706587\n",
            "Epoch: 98 | Iteration: 4160 | Loss: 0.1952952105899992\n",
            "Epoch: 98 | Iteration: 4224 | Loss: 0.4951160869342143\n",
            "Epoch: 98 | Iteration: 4288 | Loss: 0.09179135187629771\n",
            "Epoch: 98 | Iteration: 4352 | Loss: 0.28646888045986957\n",
            "Epoch: 98 | Iteration: 4416 | Loss: 0.9476929772426945\n",
            "Epoch: 98 | Iteration: 4480 | Loss: 0.06266307660949717\n",
            "Epoch: 98 | Iteration: 4544 | Loss: 0.04799863560144895\n",
            "Epoch: 98 | Iteration: 4608 | Loss: 0.6855101650080656\n",
            "Epoch: 98 | Iteration: 4672 | Loss: 0.05759132447247782\n",
            "Epoch: 98 | Iteration: 4736 | Loss: 0.09064701931271715\n",
            "Epoch: 98 | Iteration: 4800 | Loss: 0.15819431420268343\n",
            "Epoch: 98 | Iteration: 4864 | Loss: 0.027427885868048736\n",
            "Epoch: 98 | Iteration: 4928 | Loss: 0.36652662281370874\n",
            "Epoch: 98 | Iteration: 4992 | Loss: 0.7269326574310189\n",
            "Epoch: 98 | Iteration: 5056 | Loss: 0.4993563419324248\n",
            "Epoch: 98 | Iteration: 5120 | Loss: 0.7594541066202419\n",
            "Epoch: 98 | Iteration: 5184 | Loss: 0.2746995934254422\n",
            "Epoch: 98 | Iteration: 5248 | Loss: 0.326361742675677\n",
            "Epoch: 98 | Iteration: 5312 | Loss: 0.35665033677760427\n",
            "Epoch: 98 | Iteration: 5376 | Loss: 0.1045068413810796\n",
            "Epoch: 98 | Iteration: 5440 | Loss: 0.1176514258821275\n",
            "Epoch: 98 | Iteration: 5504 | Loss: 0.30273668547555543\n",
            "Epoch: 98 | Iteration: 5568 | Loss: 0.1613614011186284\n",
            "Epoch: 98 | Iteration: 5632 | Loss: 0.15610127330849397\n",
            "Epoch: 98 | Iteration: 5696 | Loss: 0.8628452624356863\n",
            "Epoch: 98 | Iteration: 5760 | Loss: 1.180354037234676\n",
            "Epoch: 98 | Iteration: 5824 | Loss: 0.15998804301349345\n",
            "Epoch: 98 | Iteration: 5888 | Loss: 0.982868169424198\n",
            "Epoch: 98 | Iteration: 5952 | Loss: 0.0480721841775944\n",
            "Epoch: 98 | Iteration: 6016 | Loss: 0.07484167494336548\n",
            "Epoch: 98 | Iteration: 6080 | Loss: 0.4516349123000366\n",
            "Epoch: 98 | Iteration: 6144 | Loss: 0.3905766662655987\n",
            "Epoch: 98 | Iteration: 6208 | Loss: 0.47736741080629363\n",
            "Epoch: 98 | Iteration: 6272 | Loss: 0.033721451267177405\n",
            "Epoch: 98 | Iteration: 6336 | Loss: 0.1735110964249275\n",
            "Epoch: 98 | Iteration: 6400 | Loss: 1.0048011554434826\n",
            "Epoch: 98 | Iteration: 6464 | Loss: 0.05720841281734228\n",
            "Epoch: 98 | Iteration: 6528 | Loss: 0.24373805964093087\n",
            "Epoch: 98 | Iteration: 6592 | Loss: 0.022395728014589007\n",
            "Epoch: 98 | Iteration: 6656 | Loss: 0.25914450484662843\n",
            "Epoch: 98 | Iteration: 6720 | Loss: 0.05113876631217229\n",
            "Epoch: 98 | Iteration: 6784 | Loss: 0.8929470849733886\n",
            "Epoch: 98 | Iteration: 6848 | Loss: 1.7075929026684502\n",
            "Epoch: 98 | Iteration: 6912 | Loss: 0.41691737256487316\n",
            "Epoch: 98 | Iteration: 6976 | Loss: 0.5224358919209925\n",
            "Epoch: 98 | Iteration: 7040 | Loss: 1.0615281521417814\n",
            "Epoch: 98 | Iteration: 7104 | Loss: 0.21554323874868114\n",
            "Epoch: 98 | Iteration: 7168 | Loss: 0.1190103104946457\n",
            "Epoch: 98 | Iteration: 7232 | Loss: 0.8535371465593011\n",
            "Epoch: 98 | Iteration: 7296 | Loss: 0.7448783792831685\n",
            "Epoch: 98 | Iteration: 7360 | Loss: 0.04443373948044339\n",
            "Epoch: 98 | Iteration: 7424 | Loss: 0.08257200191991906\n",
            "Epoch: 98 | Iteration: 7488 | Loss: 0.5102382424138273\n",
            "Epoch: 98 | Iteration: 7552 | Loss: 0.667786846121409\n",
            "Epoch: 98 | Iteration: 7616 | Loss: 0.18367225529621567\n",
            "Epoch: 98 | Iteration: 7680 | Loss: 0.5613618211446022\n",
            "Epoch: 98 | Iteration: 7744 | Loss: 0.32293592372587565\n",
            "Epoch: 98 | Iteration: 7808 | Loss: 0.45833300546116074\n",
            "Epoch: 98 | Iteration: 7872 | Loss: 0.5642231089653902\n",
            "Epoch: 98 | Iteration: 7936 | Loss: 0.2858049627605498\n",
            "Epoch: 98 | Iteration: 8000 | Loss: 0.10291197522365605\n",
            "Epoch: 98 | Iteration: 8064 | Loss: 0.2996208833744134\n",
            "Epoch: 98 | Iteration: 8128 | Loss: 0.098864844924025\n",
            "Epoch: 98 | Iteration: 8192 | Loss: 1.7898590495519637\n",
            "Epoch: 98 | Iteration: 8256 | Loss: 0.13877794305098914\n",
            "Epoch: 98 | Iteration: 8320 | Loss: 0.3259462881270867\n",
            "Epoch: 98 | Iteration: 8384 | Loss: 0.3404497018909213\n",
            "Epoch: 98 | Iteration: 8448 | Loss: 0.5945010216207891\n",
            "Epoch: 98 | Iteration: 8512 | Loss: 0.06091965728299556\n",
            "Epoch: 98 | Iteration: 8576 | Loss: 0.14628745606271157\n",
            "Epoch: 98 | Iteration: 8640 | Loss: 0.1856332396309307\n",
            "Epoch: 98 | Iteration: 8704 | Loss: 1.0661501171765997\n",
            "Epoch: 98 | Iteration: 8768 | Loss: 0.5362195296186907\n",
            "Epoch: 98 | Iteration: 8832 | Loss: 1.0477849039730798\n",
            "Epoch: 98 | Iteration: 8896 | Loss: 0.5428877812461552\n",
            "Epoch: 98 | Iteration: 8960 | Loss: 0.206806901672585\n",
            "Epoch: 98 | Iteration: 9024 | Loss: 0.18761966463111893\n",
            "Epoch: 98 | Iteration: 9088 | Loss: 0.22390736418522983\n",
            "Epoch: 98 | Iteration: 9152 | Loss: 0.08313085827761982\n",
            "Epoch: 98 | Iteration: 9216 | Loss: 0.2436216437276706\n",
            "Epoch: 98 | Iteration: 9280 | Loss: 0.7938678634471487\n",
            "Epoch: 98 | Iteration: 9344 | Loss: 0.20444270675724013\n",
            "Epoch: 98 | Iteration: 9408 | Loss: 0.5936932014709837\n",
            "Epoch: 98 | Iteration: 9472 | Loss: 0.5294928141733151\n",
            "Epoch: 98 | Iteration: 9536 | Loss: 0.07820315236426313\n",
            "Epoch: 98 | Iteration: 9600 | Loss: 0.13991760793920138\n",
            "Epoch: 98 | Iteration: 9664 | Loss: 0.0030383632222787515\n",
            "Epoch: 98 | Iteration: 9728 | Loss: 0.4156545762343141\n",
            "Epoch: 98 | Iteration: 9792 | Loss: 0.08684017684425122\n",
            "Epoch: 98 | Iteration: 9856 | Loss: 0.021216024302182886\n",
            "Epoch: 98 | Iteration: 9920 | Loss: 0.3050491764602573\n",
            "Epoch: 98 | Iteration: 9984 | Loss: 0.1323347043916532\n",
            "Epoch: 98 | Iteration: 10048 | Loss: 0.3277188804874692\n",
            "Epoch: 98 | Iteration: 10112 | Loss: 0.5158416139709048\n",
            "Epoch: 98 | Iteration: 10176 | Loss: 0.6102068101339131\n",
            "Epoch: 98 | Iteration: 10240 | Loss: 0.4995993184107002\n",
            "Epoch: 98 | Iteration: 10304 | Loss: 0.02555722192237706\n",
            "Epoch: 98 | Iteration: 10368 | Loss: 0.11893036110730494\n",
            "Epoch: 98 | Iteration: 10432 | Loss: 0.04080968065103987\n",
            "Epoch: 98 | Iteration: 10496 | Loss: 0.0258601342909165\n",
            "Epoch: 98 | Iteration: 10560 | Loss: 0.03228360467563926\n",
            "Epoch: 98 | Iteration: 10624 | Loss: 0.13730599957045247\n",
            "Epoch: 98 | Iteration: 10688 | Loss: 0.09299851301958409\n",
            "Epoch: 98 | Iteration: 10752 | Loss: 0.35853661381799695\n",
            "Epoch: 98 | Iteration: 10816 | Loss: 0.1754523109777607\n",
            "Epoch: 98 | Iteration: 10880 | Loss: 0.15863210020961305\n",
            "Epoch: 98 | Iteration: 10944 | Loss: 1.119846754436678\n",
            "Epoch: 98 | Iteration: 11008 | Loss: 0.2180315196276591\n",
            "Epoch: 98 | Iteration: 11072 | Loss: 0.3092081524995868\n",
            "Epoch: 98 | Iteration: 11136 | Loss: 0.08106024859531978\n",
            "Epoch: 98 | Iteration: 11200 | Loss: 0.24468101226900874\n",
            "Epoch: 98 | Iteration: 11264 | Loss: 0.026593192893236958\n",
            "Epoch: 98 | Iteration: 11328 | Loss: 0.08787692981122924\n",
            "Epoch: 98 | Iteration: 11392 | Loss: 0.02506169964887767\n",
            "Epoch: 98 | Iteration: 11456 | Loss: 0.08512462295164913\n",
            "Epoch: 98 | Iteration: 11520 | Loss: 0.45001210012997533\n",
            "Epoch: 98 | Iteration: 11584 | Loss: 0.46641711508819494\n",
            "Epoch: 98 | Iteration: 11648 | Loss: 0.6346754895169768\n",
            "Epoch: 98 | Iteration: 11712 | Loss: 0.7457006879964372\n",
            "Epoch: 98 | Iteration: 11776 | Loss: 0.1723250786133165\n",
            "Epoch: 98 | Iteration: 11840 | Loss: 0.4629426961764358\n",
            "Epoch: 98 | Iteration: 11904 | Loss: 0.49683369007558725\n",
            "Epoch: 98 | Iteration: 11968 | Loss: 0.16346113150711533\n",
            "Epoch: 98 | Iteration: 12032 | Loss: 0.08188671210095294\n",
            "Epoch: 98 | Iteration: 12096 | Loss: 0.031022010352417036\n",
            "Epoch: 98 | Iteration: 12160 | Loss: 0.11522107300928468\n",
            "Epoch: 98 | Iteration: 12224 | Loss: 0.12641936734804038\n",
            "Epoch: 98 | Iteration: 12288 | Loss: 0.21756504697621437\n",
            "Epoch: 98 | Iteration: 12352 | Loss: 0.15932418606860163\n",
            "Epoch: 98 | Iteration: 12416 | Loss: 0.08756532621116073\n",
            "Epoch: 98 | Iteration: 12480 | Loss: 0.05577835735919246\n",
            "Epoch: 98 | Iteration: 12544 | Loss: 0.6453316561072843\n",
            "Epoch: 98 | Iteration: 12608 | Loss: 0.1395351869652306\n",
            "Epoch: 98 | Iteration: 12672 | Loss: 0.585379145204861\n",
            "Epoch: 98 | Iteration: 12736 | Loss: 0.04428974367346628\n",
            "Epoch: 98 | Iteration: 12800 | Loss: 0.6111560589419226\n",
            "Epoch: 98 | Iteration: 12864 | Loss: 0.14253308008470206\n",
            "Epoch: 98 | Iteration: 12928 | Loss: 0.21660518003063475\n",
            "Epoch: 98 | Iteration: 12992 | Loss: 0.115537018941447\n",
            "Epoch: 98 | Iteration: 13056 | Loss: 0.0953122882973581\n",
            "Epoch: 98 | Iteration: 13120 | Loss: 0.06271757789669431\n",
            "Epoch: 98 | Iteration: 13184 | Loss: 0.4608357708406573\n",
            "Epoch: 98 | Iteration: 13248 | Loss: 0.05592359128665007\n",
            "Epoch: 98 | Iteration: 13312 | Loss: 0.014559765334143155\n",
            "Epoch: 98 | Iteration: 13376 | Loss: 0.4598672013487265\n",
            "Epoch: 98 | Iteration: 13440 | Loss: 0.0590821989387544\n",
            "Epoch: 98 | Iteration: 13504 | Loss: 0.2226152166877896\n",
            "Epoch: 98 | Iteration: 13568 | Loss: 0.08988400567458091\n",
            "Epoch: 98 | Iteration: 13632 | Loss: 0.3353628842790297\n",
            "Epoch: 98 | Iteration: 13696 | Loss: 0.4583727273027359\n",
            "Epoch: 98 | Iteration: 13760 | Loss: 0.022934325300927472\n",
            "Epoch: 98 | Iteration: 13824 | Loss: 0.2427487747038388\n",
            "Epoch: 98 | Iteration: 13888 | Loss: 0.12162023728855736\n",
            "Epoch: 98 | Iteration: 13952 | Loss: 0.7845714098650125\n",
            "Epoch: 98 | Iteration: 14016 | Loss: 0.23078493742547032\n",
            "Epoch: 98 | Iteration: 14080 | Loss: 0.23629667519989286\n",
            "Epoch: 98 | Iteration: 14144 | Loss: 0.20257372916600616\n",
            "Epoch: 98 | Iteration: 14208 | Loss: 0.577542204493357\n",
            "Epoch: 98 | Iteration: 14272 | Loss: 0.4234474393965386\n",
            "Epoch: 98 | Iteration: 14336 | Loss: 0.2459452776244362\n",
            "Epoch: 98 | Iteration: 14400 | Loss: 0.15253420694794093\n",
            "Epoch: 98 | Iteration: 14464 | Loss: 0.08225641106627951\n",
            "Epoch: 98 | Iteration: 14528 | Loss: 0.29444322397350897\n",
            "Epoch: 98 | Iteration: 14592 | Loss: 0.17309672718025715\n",
            "Epoch: 98 | Iteration: 14656 | Loss: 0.08566255920201289\n",
            "Epoch: 98 | Iteration: 14720 | Loss: 0.7072859130059713\n",
            "Epoch: 98 | Iteration: 14784 | Loss: 0.7434344091732754\n",
            "Epoch: 98 | Iteration: 14848 | Loss: 0.21387045280405212\n",
            "Epoch: 98 | Iteration: 14912 | Loss: 0.19252440681124533\n",
            "Epoch: 98 | Iteration: 14976 | Loss: 0.0355867206600802\n",
            "Epoch: 98 | Iteration: 15040 | Loss: 0.13292068351345587\n",
            "Epoch: 98 | Iteration: 15104 | Loss: 0.5954475811677776\n",
            "Epoch: 98 | Iteration: 15168 | Loss: 0.1290095659567758\n",
            "Epoch: 98 | Iteration: 15232 | Loss: 0.12215440719541104\n",
            "Epoch: 98 | Iteration: 15296 | Loss: 0.4468458472607685\n",
            "Epoch: 98 | Iteration: 15360 | Loss: 0.34412091812135626\n",
            "Epoch: 98 | Iteration: 15424 | Loss: 0.06360510631448658\n",
            "Epoch: 98 | Iteration: 15488 | Loss: 0.09233611565439069\n",
            "Epoch: 98 | Iteration: 15552 | Loss: 0.2515504486279502\n",
            "Epoch: 98 | Iteration: 15616 | Loss: 0.04968817848405094\n",
            "Epoch: 98 | Iteration: 15680 | Loss: 0.20218341880961843\n",
            "Epoch: 98 | Iteration: 15744 | Loss: 0.4004922524129511\n",
            "Epoch: 98 | Iteration: 15808 | Loss: 0.47505016339412187\n",
            "Epoch: 98 | Iteration: 15872 | Loss: 0.2431355917114769\n",
            "Epoch: 98 | Iteration: 15936 | Loss: 1.0158549148311684\n",
            "Epoch: 98 | Iteration: 16000 | Loss: 0.47935558748379137\n",
            "Epoch: 98 | Iteration: 16064 | Loss: 0.05901314921354244\n",
            "Epoch: 98 | Iteration: 16128 | Loss: 0.32792099928275753\n",
            "Epoch: 98 | Iteration: 16192 | Loss: 0.010387748785649301\n",
            "Epoch: 98 | Iteration: 16256 | Loss: 0.20384969821903193\n",
            "Epoch: 98 | Iteration: 16320 | Loss: 0.15534167311233746\n",
            "Epoch: 98 | Iteration: 16384 | Loss: 0.39233599813422915\n",
            "Epoch: 98 | Iteration: 16448 | Loss: 0.15331417670429526\n",
            "Epoch: 98 | Iteration: 16512 | Loss: 0.7251037283988303\n",
            "Epoch: 98 | Iteration: 16576 | Loss: 0.05349794578003902\n",
            "Epoch: 98 | Iteration: 16640 | Loss: 0.5010656118535683\n",
            "Epoch: 98 | Iteration: 16704 | Loss: 0.2993801667964443\n",
            "Epoch: 98 | Iteration: 16768 | Loss: 0.33838356729502383\n",
            "Epoch: 98 | Iteration: 16832 | Loss: 0.4131565373050072\n",
            "Epoch: 98 | Iteration: 16896 | Loss: 0.35512741604753084\n",
            "Epoch: 98 | Iteration: 16960 | Loss: 0.2799812517751952\n",
            "Epoch: 98 | Iteration: 17024 | Loss: 0.2870671165852837\n",
            "Epoch: 98 | Iteration: 17088 | Loss: 0.5089288292824962\n",
            "Epoch: 98 | Iteration: 17152 | Loss: 0.3963296006022058\n",
            "Epoch: 98 | Iteration: 17216 | Loss: 0.47129827362489524\n",
            "Epoch: 98 | Iteration: 17280 | Loss: 0.012069461209115848\n",
            "Epoch: 98 | Iteration: 17344 | Loss: 0.40625768090693215\n",
            "Epoch: 98 | Iteration: 17408 | Loss: 0.12403087619389289\n",
            "Epoch: 98 | Iteration: 17472 | Loss: 0.18584073600501738\n",
            "Epoch: 98 | Iteration: 17536 | Loss: 0.6047525357186792\n",
            "Epoch: 98 | Iteration: 17600 | Loss: 0.19118295223804005\n",
            "Epoch: 98 | Iteration: 17664 | Loss: 0.3673309808888849\n",
            "Epoch: 98 | Iteration: 17728 | Loss: 1.447590861050257\n",
            "Epoch: 98 | Iteration: 17792 | Loss: 0.20745745744490518\n",
            "Epoch: 98 | Iteration: 17856 | Loss: 0.3869156525192923\n",
            "Epoch: 98 | Iteration: 17920 | Loss: 0.03902885745930826\n",
            "Epoch: 98 | Iteration: 17984 | Loss: 0.07561532033957345\n",
            "Epoch: 98 | Iteration: 18048 | Loss: 0.4458134211165249\n",
            "Epoch: 98 | Iteration: 18112 | Loss: 0.20584969260280506\n",
            "Epoch: 98 | Iteration: 18176 | Loss: 0.10094086980816847\n",
            "Epoch: 98 | Iteration: 18240 | Loss: 0.591874545853408\n",
            "Epoch: 98 | Iteration: 18304 | Loss: 0.41132187737983406\n",
            "Epoch: 98 | Iteration: 18368 | Loss: 0.33045210445168216\n",
            "Epoch: 98 | Iteration: 18432 | Loss: 0.699514406654395\n",
            "Epoch: 98 | Iteration: 18496 | Loss: 0.15253939864502059\n",
            "Epoch: 98 | Iteration: 18560 | Loss: 0.18197159248635858\n",
            "Epoch: 98 | Iteration: 18624 | Loss: 0.0698080115241163\n",
            "Epoch: 98 | Iteration: 18688 | Loss: 0.3423308413256569\n",
            "Epoch: 98 | Iteration: 18752 | Loss: 0.07368800083225453\n",
            "Epoch: 98 | Iteration: 18816 | Loss: 0.08327121804573578\n",
            "Epoch: 98 | Iteration: 18880 | Loss: 0.20774899870726163\n",
            "Epoch: 98 | Iteration: 18944 | Loss: 0.18651673027629193\n",
            "Epoch: 98 | Iteration: 19008 | Loss: 0.13196008390752312\n",
            "Epoch: 98 | Iteration: 19072 | Loss: 1.3227971778507275\n",
            "Epoch: 98 | Iteration: 19136 | Loss: 0.15080209169082778\n",
            "Epoch: 98 | Iteration: 19200 | Loss: 0.17169196215210258\n",
            "Epoch: 98 | Iteration: 19264 | Loss: 0.48827207361672453\n",
            "Epoch: 98 | Iteration: 19328 | Loss: 1.5427283996994519\n",
            "Epoch: 98 | Iteration: 19392 | Loss: 0.2984498513744179\n",
            "Epoch: 98 | Iteration: 19456 | Loss: 0.2328771511164996\n",
            "Epoch: 98 | Iteration: 19520 | Loss: 0.053235601351411484\n",
            "Epoch: 98 | Iteration: 19584 | Loss: 0.11054989793057386\n",
            "Epoch: 98 | Iteration: 19648 | Loss: 0.10103554259605144\n",
            "Epoch: 98 | Iteration: 19712 | Loss: 0.01343569576061613\n",
            "Epoch: 98 | Iteration: 19776 | Loss: 0.8430849242442904\n",
            "Epoch: 98 | Iteration: 19840 | Loss: 0.3851670749495875\n",
            "Epoch: 98 | Iteration: 19904 | Loss: 0.35925943349733874\n",
            "Epoch: 98 | Iteration: 19968 | Loss: 0.29096634288574597\n",
            "Epoch: 98 | Iteration: 20032 | Loss: 0.2520684219399673\n",
            "Epoch: 98 | Iteration: 20096 | Loss: 0.08842318856511713\n",
            "Epoch: 98 | Iteration: 20160 | Loss: 0.8280973283577893\n",
            "Epoch: 98 | Iteration: 20224 | Loss: 0.4315538563661456\n",
            "Epoch: 98 | Iteration: 20288 | Loss: 0.8058498455386819\n",
            "Epoch: 98 | Iteration: 20352 | Loss: 0.006072350587392982\n",
            "Epoch: 98 | Iteration: 20416 | Loss: 0.014720803915006245\n",
            "Epoch: 98 | Iteration: 20480 | Loss: 0.0978641873558701\n",
            "Epoch: 98 | Iteration: 20544 | Loss: 0.18724474330855237\n",
            "Epoch: 98 | Iteration: 20608 | Loss: 0.12256288221448053\n",
            "Epoch: 98 | Iteration: 20672 | Loss: 1.7019727078653704\n",
            "Epoch: 98 | Iteration: 20736 | Loss: 1.0795149375141657\n",
            "Epoch: 98 | Iteration: 20800 | Loss: 0.3632472218214846\n",
            "Epoch: 98 | Iteration: 20864 | Loss: 0.3185600301352067\n",
            "Epoch: 98 | Iteration: 20928 | Loss: 0.26458164201623424\n",
            "Epoch: 98 | Iteration: 20992 | Loss: 0.37839974723677816\n",
            "Epoch: 98 | Iteration: 21056 | Loss: 0.26370766249946714\n",
            "Epoch: 98 | Iteration: 21120 | Loss: 0.3305631911129526\n",
            "Epoch: 98 | Iteration: 21184 | Loss: 0.20559960115461653\n",
            "Epoch: 98 | Iteration: 21248 | Loss: 0.19346587716703617\n",
            "Epoch: 98 | Iteration: 21312 | Loss: 0.42864327934898794\n",
            "Epoch: 98 | Iteration: 21376 | Loss: 0.16689178440002111\n",
            "Epoch: 98 | Iteration: 21440 | Loss: 0.584981520292585\n",
            "Epoch: 98 | Iteration: 21504 | Loss: 0.1912166611244225\n",
            "Epoch: 98 | Iteration: 21568 | Loss: 1.3708613283467097\n",
            "Epoch: 98 | Iteration: 21632 | Loss: 0.23313560378250175\n",
            "Epoch: 98 | Iteration: 21696 | Loss: 0.09644785631588743\n",
            "Epoch: 98 | Iteration: 21760 | Loss: 0.005859138831851958\n",
            "Epoch: 98 | Iteration: 21824 | Loss: 0.003349988679087994\n",
            "Epoch: 98 | Iteration: 21888 | Loss: 0.533515623230507\n",
            "Epoch: 98 | Iteration: 21952 | Loss: 0.35214985025298506\n",
            "Epoch: 98 | Iteration: 22016 | Loss: 0.07198469425038292\n",
            "Epoch: 98 | Iteration: 22080 | Loss: 0.4092942992481279\n",
            "Epoch: 98 | Iteration: 22144 | Loss: 0.3383515118407651\n",
            "Epoch: 98 | Iteration: 22208 | Loss: 0.22893111274700928\n",
            "Epoch: 98 | Iteration: 22272 | Loss: 0.7108901216632196\n",
            "Epoch: 98 | Iteration: 22336 | Loss: 0.03753501213771125\n",
            "Epoch: 98 | Iteration: 22400 | Loss: 0.1864494909534987\n",
            "Epoch: 98 | Iteration: 22464 | Loss: 0.583529614511264\n",
            "Epoch: 98 | Iteration: 22528 | Loss: 0.9091861637715906\n",
            "Epoch: 98 | Iteration: 22592 | Loss: 0.4455308515191846\n",
            "Epoch: 98 | Iteration: 22656 | Loss: 0.12338200737241185\n",
            "Epoch: 98 | Iteration: 22720 | Loss: 0.54427896957861\n",
            "Epoch: 98 | Iteration: 22784 | Loss: 0.15260691904807025\n",
            "Epoch: 98 | Iteration: 22848 | Loss: 0.2537293678345993\n",
            "Epoch: 98 | Iteration: 22912 | Loss: 0.001659651244365637\n",
            "Epoch: 98 | Iteration: 22976 | Loss: 0.25065204729082136\n",
            "Epoch: 98 | Iteration: 23040 | Loss: 0.21346606310405195\n",
            "Epoch: 98 | Iteration: 23104 | Loss: 0.1174223529256563\n",
            "Epoch: 98 | Iteration: 23168 | Loss: 0.4701170290297909\n",
            "Epoch: 98 | Iteration: 23232 | Loss: 0.023385033315280645\n",
            "Epoch: 98 | Iteration: 23296 | Loss: 0.11429740646418929\n",
            "Epoch: 98 | Iteration: 23360 | Loss: 0.2958211528702179\n",
            "Epoch: 98 | Iteration: 23424 | Loss: 0.07921488668386364\n",
            "Epoch: 98 | Iteration: 23488 | Loss: 0.08635918700401612\n",
            "Epoch: 98 | Iteration: 23552 | Loss: 0.6240707085531518\n",
            "Epoch: 98 | Iteration: 23616 | Loss: 0.6887707033638124\n",
            "Epoch: 98 | Iteration: 23680 | Loss: 0.8155217574455531\n",
            "Epoch: 98 | Iteration: 23744 | Loss: 0.011968449902805624\n",
            "Epoch: 98 | Iteration: 23808 | Loss: 0.3601186578988219\n",
            "Epoch: 98 | Iteration: 23872 | Loss: 0.5910388693737532\n",
            "Epoch: 98 | Iteration: 23936 | Loss: 0.9894064381725951\n",
            "Epoch: 98 | Iteration: 24000 | Loss: 0.15575605068622947\n",
            "Epoch: 98 | Iteration: 24064 | Loss: 0.07341820939879701\n",
            "Epoch: 98 | Iteration: 24128 | Loss: 0.14544260317479224\n",
            "Epoch: 98 | Iteration: 24192 | Loss: 0.2574072532913626\n",
            "Epoch: 98 | Iteration: 24256 | Loss: 0.6881337723096483\n",
            "Epoch: 98 | Iteration: 24320 | Loss: 0.3798309385959196\n",
            "Epoch: 98 | Iteration: 24384 | Loss: 0.009089977261655948\n",
            "Epoch: 98 | Iteration: 24448 | Loss: 0.21316747266561079\n",
            "Epoch: 98 | Iteration: 24512 | Loss: 0.17962177178875116\n",
            "Epoch: 98 | Iteration: 24576 | Loss: 0.3519855852827785\n",
            "Epoch: 98 | Iteration: 24640 | Loss: 0.17999722542147284\n",
            "Epoch: 98 | Iteration: 24704 | Loss: 0.10811675065488954\n",
            "Epoch: 98 | Iteration: 24768 | Loss: 0.5775774450054127\n",
            "Epoch: 98 | Iteration: 24832 | Loss: 0.052304172468734894\n",
            "Epoch: 98 | Iteration: 24896 | Loss: 0.2595609079861493\n",
            "Epoch: 98 | Iteration: 24960 | Loss: 0.010684061668872273\n",
            "Epoch: 98 | Iteration: 25024 | Loss: 0.04064139942191469\n",
            "Epoch: 98 | Iteration: 25088 | Loss: 0.29998423903932725\n",
            "Epoch: 98 | Iteration: 25152 | Loss: 0.6928801027703775\n",
            "Epoch: 98 | Iteration: 25216 | Loss: 0.17985090719095115\n",
            "Epoch: 98 | Iteration: 25280 | Loss: 0.1401941098616806\n",
            "Epoch: 98 | Iteration: 25344 | Loss: 0.02346666276810777\n",
            "Epoch: 98 | Iteration: 25408 | Loss: 0.029551772577578524\n",
            "Epoch: 98 | Iteration: 25472 | Loss: 0.5510586596806775\n",
            "Epoch: 98 | Iteration: 25536 | Loss: 1.1832572288822663\n",
            "Epoch: 98 | Iteration: 25600 | Loss: 0.0662018445584393\n",
            "Epoch: 98 | Iteration: 25664 | Loss: 0.587295940471433\n",
            "Epoch: 98 | Iteration: 25728 | Loss: 0.5642603131323287\n",
            "Epoch: 98 | Iteration: 25792 | Loss: 0.8124771278281395\n",
            "Epoch: 98 | Iteration: 25856 | Loss: 0.07694840140501835\n",
            "Epoch: 98 | Iteration: 25920 | Loss: 0.10751852296863765\n",
            "Epoch: 98 | Iteration: 25984 | Loss: 0.044465002773539763\n",
            "Epoch: 98 | Iteration: 26048 | Loss: 0.29622463176917047\n",
            "Epoch: 98 | Iteration: 26112 | Loss: 0.2545766513337175\n",
            "Epoch: 98 | Iteration: 26176 | Loss: 0.18257896923897862\n",
            "Epoch: 98 | Iteration: 26240 | Loss: 0.19772841993646836\n",
            "Epoch: 98 | Iteration: 26304 | Loss: 0.5219476538131629\n",
            "Epoch: 98 | Iteration: 26368 | Loss: 0.8197288354645836\n",
            "Epoch: 98 | Iteration: 26432 | Loss: 0.9164618510411142\n",
            "Epoch: 98 | Iteration: 26496 | Loss: 1.411980307361004\n",
            "Epoch: 98 | Iteration: 26560 | Loss: 0.9276251801587749\n",
            "Epoch: 98 | Iteration: 26624 | Loss: 0.8958454462801275\n",
            "Epoch: 98 | Iteration: 26688 | Loss: 0.7804528292129409\n",
            "Epoch: 98 | Iteration: 26752 | Loss: 0.9318587161571016\n",
            "Epoch: 98 | Iteration: 26816 | Loss: 0.2594573166534492\n",
            "Epoch: 98 | Iteration: 26880 | Loss: 1.0830716823804847\n",
            "Epoch: 98 | Iteration: 26944 | Loss: 0.014984816385909597\n",
            "Epoch: 98 | Iteration: 27008 | Loss: 0.11807537790400599\n",
            "Epoch: 98 | Iteration: 27072 | Loss: 0.06863541201227161\n",
            "Epoch: 98 | Iteration: 27136 | Loss: 1.5809935180704846\n",
            "Epoch: 98 | Iteration: 27200 | Loss: 0.8783474704867535\n",
            "Epoch: 98 | Iteration: 27264 | Loss: 0.11320858481357482\n",
            "Epoch: 98 | Iteration: 27328 | Loss: 0.019065262408003915\n",
            "Epoch: 98 | Iteration: 27392 | Loss: 0.13600530329585955\n",
            "Epoch: 98 | Iteration: 27456 | Loss: 0.537019260522995\n",
            "Epoch: 98 | Iteration: 27520 | Loss: 0.6155608270815178\n",
            "Epoch: 98 | Iteration: 27584 | Loss: 0.6998679846656983\n",
            "Epoch: 98 | Iteration: 27648 | Loss: 0.5582134066541908\n",
            "Epoch: 98 | Iteration: 27712 | Loss: 0.2037848203414302\n",
            "Epoch: 98 | Iteration: 27776 | Loss: 0.29309630849752577\n",
            "Epoch: 98 | Iteration: 27840 | Loss: 0.8920072373338059\n",
            "Epoch: 98 | Iteration: 27904 | Loss: 0.0822653745731073\n",
            "Epoch: 98 | Iteration: 27968 | Loss: 0.02418873689311134\n",
            "Epoch: 98 | Iteration: 28032 | Loss: 0.20742303333834958\n",
            "Epoch: 98 | Iteration: 28096 | Loss: 0.13400348371313736\n",
            "Epoch: 98 | Iteration: 28160 | Loss: 0.613909642106661\n",
            "Epoch: 98 | Iteration: 28224 | Loss: 0.07148608152791869\n",
            "Epoch: 98 | Iteration: 28288 | Loss: 0.011273782544755733\n",
            "Epoch: 98 | Iteration: 28352 | Loss: 0.6612603295233603\n",
            "Epoch: 98 | Iteration: 28416 | Loss: 0.09466771776241767\n",
            "Epoch: 98 | Iteration: 28480 | Loss: 0.08134242458528053\n",
            "Epoch: 98 | Iteration: 28544 | Loss: 0.11551263520249813\n",
            "Epoch: 98 | Iteration: 28608 | Loss: 0.6413823255024791\n",
            "Epoch: 98 | Iteration: 28672 | Loss: 0.2728700881427031\n",
            "Epoch: 98 | Iteration: 28736 | Loss: 0.1024669763988593\n",
            "Epoch: 98 | Iteration: 28800 | Loss: 0.183527080132893\n",
            "Epoch: 98 | Iteration: 28864 | Loss: 0.10629144748852021\n",
            "Epoch: 98 | Iteration: 28928 | Loss: 0.45292520979934414\n",
            "Epoch: 98 | Iteration: 28992 | Loss: 0.20571465552699142\n",
            "Epoch: 98 | Iteration: 29056 | Loss: 0.1553482930639426\n",
            "Epoch: 98 | Iteration: 29120 | Loss: 0.28698085134378293\n",
            "Epoch: 98 | Iteration: 29184 | Loss: 0.5942025054277429\n",
            "Epoch: 98 | Iteration: 29248 | Loss: 0.36477120327683293\n",
            "Epoch: 98 | Iteration: 29312 | Loss: 0.11123700447890222\n",
            "Epoch: 98 | Iteration: 29376 | Loss: 0.15929053366988116\n",
            "Epoch: 98 | Iteration: 29440 | Loss: 0.4502554245814947\n",
            "Epoch: 98 | Iteration: 29504 | Loss: 0.1907883266028326\n",
            "Epoch: 98 | Iteration: 29568 | Loss: 0.20647642599296595\n",
            "Epoch: 98 | Iteration: 29632 | Loss: 0.13686845838274\n",
            "Epoch: 98 | Iteration: 29696 | Loss: 0.5098331056829644\n",
            "Epoch: 98 | Iteration: 29760 | Loss: 0.04538091260522148\n",
            "Epoch: 98 | Iteration: 29824 | Loss: 0.25114127065231573\n",
            "Epoch: 98 | Iteration: 29888 | Loss: 0.47440044376729734\n",
            "Epoch: 98 | Iteration: 29952 | Loss: 0.06067106767406\n",
            "Epoch: 98 | Iteration: 30016 | Loss: 0.6369321409117571\n",
            "Epoch: 98 | Iteration: 30080 | Loss: 0.2213906336164374\n",
            "Epoch: 98 | Iteration: 30144 | Loss: 0.2733603323955865\n",
            "Epoch: 98 | Iteration: 30208 | Loss: 0.10008587210632874\n",
            "Epoch: 98 | Iteration: 30272 | Loss: 0.05241397845500095\n",
            "Epoch: 98 | Iteration: 30336 | Loss: 0.22025435391072437\n",
            "Epoch: 98 | Iteration: 30400 | Loss: 0.05025192973759902\n",
            "Epoch: 98 | Iteration: 30464 | Loss: 0.27606742467121725\n",
            "Epoch: 98 | Iteration: 30528 | Loss: 0.13881363318388718\n",
            "Epoch: 98 | Iteration: 30592 | Loss: 0.08973425862348289\n",
            "Epoch: 98 | Iteration: 30656 | Loss: 0.5649911269679908\n",
            "Epoch: 98 | Iteration: 30720 | Loss: 0.2312723344187784\n",
            "Epoch: 98 | Iteration: 30784 | Loss: 0.17867671390823978\n",
            "Epoch: 98 | Iteration: 30848 | Loss: 0.3283114451343117\n",
            "Epoch: 98 | Iteration: 30912 | Loss: 0.7174067865887417\n",
            "Epoch: 98 | Iteration: 30976 | Loss: 0.17749873496400834\n",
            "Epoch: 98 | Iteration: 31040 | Loss: 0.4672740905059382\n",
            "Epoch: 98 | Iteration: 31104 | Loss: 0.42541477804463257\n",
            "Epoch: 98 | Iteration: 31168 | Loss: 1.6792556819330096\n",
            "Epoch: 98 | Iteration: 31232 | Loss: 0.1709456888295756\n",
            "Epoch: 98 | Iteration: 31296 | Loss: 0.8024321765287017\n",
            "Epoch: 98 | Iteration: 31360 | Loss: 0.26259816600594826\n",
            "Epoch: 98 | Iteration: 31424 | Loss: 0.29843230914920954\n",
            "Epoch: 98 | Iteration: 31488 | Loss: 0.0810572844918606\n",
            "Epoch: 98 | Iteration: 31552 | Loss: 1.0889522067803867\n",
            "Epoch: 98 | Iteration: 31616 | Loss: 0.3317034528048303\n",
            "Epoch: 98 | Iteration: 31680 | Loss: 1.3461118286232143\n",
            "Epoch: 98 | Iteration: 31744 | Loss: 0.7444872131900206\n",
            "Epoch: 98 | Iteration: 31808 | Loss: 0.19482992656911988\n",
            "Epoch: 98 | Iteration: 31872 | Loss: 0.17301596477658826\n",
            "Epoch: 98 | Iteration: 31936 | Loss: 0.24007334993865764\n",
            "Epoch: 98 | Iteration: 32000 | Loss: 0.2780710308951179\n",
            "Epoch: 98 | Iteration: 32064 | Loss: 0.30153812512098577\n",
            "Epoch: 98 | Iteration: 32128 | Loss: 0.15216606382126746\n",
            "Epoch: 98 | Iteration: 32192 | Loss: 0.44250055790751447\n",
            "Epoch: 98 | Iteration: 32256 | Loss: 0.1979299198822415\n",
            "Epoch: 98 | Iteration: 32320 | Loss: 0.6971024444769982\n",
            "Epoch: 98 | Iteration: 32384 | Loss: 0.3090459463097093\n",
            "Epoch: 98 | Iteration: 32448 | Loss: 1.0294684434879489\n",
            "Epoch: 98 | Iteration: 32512 | Loss: 0.2881930000630645\n",
            "Epoch: 98 | Iteration: 32576 | Loss: 0.0006631254297502795\n",
            "Epoch: 98 | Iteration: 32640 | Loss: 0.3067109120121828\n",
            "Epoch: 98 | Iteration: 32704 | Loss: 0.2941872635713241\n",
            "Epoch: 98 | Iteration: 32768 | Loss: 0.8414270912576782\n",
            "Epoch: 98 | Iteration: 32832 | Loss: 0.6421951201678872\n",
            "Epoch: 98 | Iteration: 32896 | Loss: 0.6694896941410484\n",
            "Epoch: 98 | Iteration: 32960 | Loss: 0.013716463189453246\n",
            "Epoch: 98 | Iteration: 33024 | Loss: 0.25866299216019195\n",
            "Epoch: 98 | Iteration: 33088 | Loss: 0.03263117733255375\n",
            "Epoch: 98 | Iteration: 33152 | Loss: 0.0750810681972171\n",
            "Epoch: 98 | Iteration: 33216 | Loss: 0.20705176281784116\n",
            "Epoch: 98 | Iteration: 33280 | Loss: 0.2551046978960656\n",
            "Epoch: 98 | Iteration: 33344 | Loss: 0.3323745300681601\n",
            "Epoch: 98 | Iteration: 33408 | Loss: 0.5957274914057445\n",
            "Epoch: 98 | Iteration: 33472 | Loss: 0.5112693522378386\n",
            "Epoch: 98 | Iteration: 33536 | Loss: 0.4084260951644309\n",
            "Epoch: 98 | Iteration: 33600 | Loss: 0.06782990375123943\n",
            "Epoch: 98 | Iteration: 33664 | Loss: 0.12942071832412913\n",
            "Epoch: 98 | Iteration: 33728 | Loss: 0.44278527433720855\n",
            "Epoch: 98 | Iteration: 33792 | Loss: 0.11417605230103958\n",
            "Epoch: 98 | Iteration: 33856 | Loss: 0.010658668245760386\n",
            "Epoch: 98 | Iteration: 33920 | Loss: 0.012549069912470949\n",
            "Epoch: 98 | Iteration: 33984 | Loss: 0.5234698656099012\n",
            "Epoch: 98 | Iteration: 34048 | Loss: 0.47322859461069433\n",
            "Epoch: 98 | Iteration: 34112 | Loss: 0.012331974298354188\n",
            "Epoch: 98 | Iteration: 34176 | Loss: 0.056890093466035074\n",
            "Epoch: 98 | Iteration: 34240 | Loss: 0.00276947182210414\n",
            "Epoch: 98 | Iteration: 34304 | Loss: 0.10507182647061619\n",
            "Epoch: 98 | Iteration: 34368 | Loss: 1.3472010781479868\n",
            "Epoch: 98 | Iteration: 34432 | Loss: 0.15129716322607195\n",
            "Epoch: 98 | Iteration: 34496 | Loss: 0.16628983337488862\n",
            "Epoch: 98 | Iteration: 34560 | Loss: 0.12105830221522164\n",
            "Epoch: 98 | Iteration: 34624 | Loss: 0.5894539645815678\n",
            "Epoch: 98 | Iteration: 34688 | Loss: 0.793800841229388\n",
            "Epoch: 98 | Iteration: 34752 | Loss: 0.3608654277890027\n",
            "Epoch: 98 | Iteration: 34816 | Loss: 0.7480388584758437\n",
            "Epoch: 98 | Iteration: 34880 | Loss: 0.4526845102391979\n",
            "Epoch: 98 | Iteration: 34944 | Loss: 0.04158254825521818\n",
            "Epoch: 98 | Iteration: 35008 | Loss: 1.276397984140216\n",
            "Epoch: 98 | Iteration: 35072 | Loss: 0.028826971136592383\n",
            "Epoch: 98 | Iteration: 35136 | Loss: 0.9951609261539287\n",
            "Epoch: 98 | Iteration: 35200 | Loss: 1.1753249227586848\n",
            "Epoch: 98 | Iteration: 35264 | Loss: 0.11294433429469941\n",
            "Epoch: 98 | Iteration: 35328 | Loss: 0.15501013077579212\n",
            "Epoch: 98 | Iteration: 35392 | Loss: 0.22012273087479184\n",
            "Epoch: 98 | Iteration: 35456 | Loss: 1.136650282576199\n",
            "Epoch: 98 | Iteration: 35520 | Loss: 0.13806661455080843\n",
            "Epoch: 98 | Iteration: 35584 | Loss: 0.9857221442753972\n",
            "Epoch: 98 | Iteration: 35648 | Loss: 0.23480591637438117\n",
            "Epoch: 98 | Iteration: 35712 | Loss: 0.10043139452589753\n",
            "Epoch: 98 | Iteration: 35776 | Loss: 0.04103877366366861\n",
            "Epoch: 98 | Iteration: 35840 | Loss: 0.22909823203537122\n",
            "Epoch: 98 | Iteration: 35904 | Loss: 0.2699335242379094\n",
            "Epoch: 98 | Iteration: 35968 | Loss: 0.06377877248822778\n",
            "Epoch: 98 | Iteration: 36032 | Loss: 0.9036641741069147\n",
            "Epoch: 98 | Iteration: 36096 | Loss: 1.3136506405947679\n",
            "Epoch: 98 | Iteration: 36160 | Loss: 0.39751505947949556\n",
            "Epoch: 98 | Iteration: 36224 | Loss: 0.10317106516454239\n",
            "Epoch: 98 | Iteration: 36288 | Loss: 0.02004107828750939\n",
            "Epoch: 98 | Iteration: 36352 | Loss: 0.17621491735900044\n",
            "Epoch: 98 | Iteration: 36416 | Loss: 0.720115307442487\n",
            "Epoch: 98 | Iteration: 36480 | Loss: 0.01926592549611556\n",
            "Epoch: 98 | Iteration: 36544 | Loss: 0.2351251030395316\n",
            "Epoch: 98 | Iteration: 36608 | Loss: 0.05977375614864726\n",
            "Epoch: 98 | Iteration: 36672 | Loss: 0.21174553956605216\n",
            "Epoch: 98 | Iteration: 36736 | Loss: 0.9174951948881311\n",
            "Epoch: 98 | Iteration: 36800 | Loss: 0.38253138532596365\n",
            "Epoch: 98 | Iteration: 36864 | Loss: 0.14876700699581447\n",
            "Epoch: 98 | Iteration: 36928 | Loss: 0.3229222408439878\n",
            "Epoch: 98 | Iteration: 36992 | Loss: 1.0597547378262588\n",
            "Epoch: 98 | Iteration: 37056 | Loss: 0.28510857812685475\n",
            "Epoch: 98 | Iteration: 37120 | Loss: 0.44098594309009487\n",
            "Epoch: 98 | Iteration: 37184 | Loss: 0.10402036230241388\n",
            "Epoch: 98 | Iteration: 37248 | Loss: 0.36654752552713976\n",
            "Epoch: 98 | Iteration: 37312 | Loss: 0.936397308832167\n",
            "Epoch: 98 | Iteration: 37376 | Loss: 0.9392737723376928\n",
            "Epoch: 98 | Iteration: 37440 | Loss: 0.4510477576897405\n",
            "Epoch: 98 | Iteration: 37504 | Loss: 0.20852628573291007\n",
            "Epoch: 98 | Iteration: 37568 | Loss: 0.05171123804059357\n",
            "Epoch: 98 | Iteration: 37632 | Loss: 0.19851793544029794\n",
            "Epoch: 98 | Iteration: 37696 | Loss: 0.6242191778040004\n",
            "Epoch: 98 | Iteration: 37760 | Loss: 0.7483899059649769\n",
            "Epoch: 98 | Iteration: 37824 | Loss: 0.9949083098355158\n",
            "Epoch: 98 | Iteration: 37888 | Loss: 0.06368035939972949\n",
            "Epoch: 98 | Iteration: 37952 | Loss: 0.04154525530315067\n",
            "Epoch: 98 | Iteration: 38016 | Loss: 0.15142162865657946\n",
            "Epoch: 98 | Iteration: 38080 | Loss: 0.06339464571504581\n",
            "Epoch: 98 | Iteration: 38144 | Loss: 0.1539421685961735\n",
            "Epoch: 98 | Iteration: 38208 | Loss: 0.06316821112008747\n",
            "Epoch: 98 | Iteration: 38272 | Loss: 0.4835556018988776\n",
            "Epoch: 98 | Iteration: 38336 | Loss: 0.6902746857842454\n",
            "Epoch: 98 | Iteration: 38400 | Loss: 0.1349892762767995\n",
            "Epoch: 98 | Iteration: 38464 | Loss: 0.5996915336364512\n",
            "Epoch: 98 | Iteration: 38528 | Loss: 0.5155537676057843\n",
            "Epoch: 98 | Iteration: 38592 | Loss: 0.2891136796850967\n",
            "Epoch: 98 | Iteration: 38656 | Loss: 0.5799254647394924\n",
            "Epoch: 98 | Iteration: 38720 | Loss: 0.2055314650689843\n",
            "Epoch: 98 | Iteration: 38784 | Loss: 0.0069277675078289165\n",
            "Epoch: 98 | Iteration: 38848 | Loss: 0.07991254097233187\n",
            "Epoch: 98 | Iteration: 38912 | Loss: 0.2412162626445041\n",
            "Epoch: 98 | Iteration: 38976 | Loss: 0.19537188313348164\n",
            "Epoch: 98 | Iteration: 39040 | Loss: 0.006252272577091555\n",
            "Epoch: 98 | Iteration: 39104 | Loss: 0.09368381855736353\n",
            "Epoch: 98 | Iteration: 39168 | Loss: 0.6818623353633598\n",
            "Epoch: 98 | Iteration: 39232 | Loss: 0.3732648485085752\n",
            "Epoch: 98 | Iteration: 39296 | Loss: 0.8706753673526116\n",
            "Epoch: 98 | Iteration: 39360 | Loss: 1.0126856026379203\n",
            "Epoch: 98 | Iteration: 39424 | Loss: 0.17079028836256466\n",
            "Epoch: 98 | Iteration: 39488 | Loss: 0.04915147246056884\n",
            "Epoch: 98 | Iteration: 39552 | Loss: 0.4111488604157735\n",
            "Epoch: 98 | Iteration: 39616 | Loss: 0.4522165113428571\n",
            "Epoch: 98 | Iteration: 39680 | Loss: 0.3629028041082292\n",
            "Epoch: 98 | Iteration: 39744 | Loss: 0.6738093442378061\n",
            "Epoch: 98 | Iteration: 39808 | Loss: 0.6727739535719741\n",
            "Epoch: 98 | Iteration: 39872 | Loss: 0.14952355730557004\n",
            "Epoch: 98 | Iteration: 39936 | Loss: 0.05678720150279637\n",
            "Epoch: 98 | Iteration: 40000 | Loss: 0.23251404724298252\n",
            "Epoch: 98 | Iteration: 40064 | Loss: 0.21998598447576245\n",
            "Epoch: 98 | Iteration: 40128 | Loss: 0.1432840093535412\n",
            "Epoch: 98 | Iteration: 40192 | Loss: 0.30423344327012736\n",
            "Epoch: 98 | Iteration: 40256 | Loss: 0.41385707999528615\n",
            "Epoch: 98 | Iteration: 40320 | Loss: 0.18716568334168793\n",
            "Epoch: 98 | Iteration: 40384 | Loss: 0.21917829497553823\n",
            "Epoch: 98 | Iteration: 40448 | Loss: 0.0594664918459646\n",
            "Epoch: 98 | Iteration: 40512 | Loss: 0.09066594073341883\n",
            "Epoch: 98 | Iteration: 40576 | Loss: 0.6439999747794976\n",
            "Epoch: 98 | Iteration: 40640 | Loss: 0.6061043548063076\n",
            "Epoch: 98 | Iteration: 40704 | Loss: 0.46613081549075913\n",
            "Epoch: 98 | Iteration: 40768 | Loss: 0.19828119255107693\n",
            "Epoch: 98 | Iteration: 40832 | Loss: 0.1269391794093765\n",
            "Epoch: 98 | Iteration: 40896 | Loss: 0.07508952085954596\n",
            "Epoch: 98 | Iteration: 40960 | Loss: 0.7705234067202595\n",
            "Epoch: 98 | Iteration: 41024 | Loss: 0.3914952849490365\n",
            "Epoch: 98 | Iteration: 41088 | Loss: 0.13605100562541944\n",
            "Epoch: 98 | Iteration: 41152 | Loss: 0.8187410016513517\n",
            "Epoch: 98 | Iteration: 41216 | Loss: 1.0344794677916522\n",
            "Epoch: 98 | Iteration: 41280 | Loss: 0.7775504105935525\n",
            "Epoch: 98 | Iteration: 41344 | Loss: 0.28139844055834895\n",
            "Epoch: 98 | Iteration: 41408 | Loss: 0.26875929724086356\n",
            "Epoch: 98 | Iteration: 41472 | Loss: 0.0385064739668258\n",
            "Epoch: 98 | Iteration: 41536 | Loss: 0.9700071463612453\n",
            "Epoch: 98 | Iteration: 41600 | Loss: 0.12404645052267896\n",
            "Epoch: 98 | Iteration: 41664 | Loss: 0.0693890007454386\n",
            "Epoch: 98 | Iteration: 41728 | Loss: 0.24712799005729374\n",
            "Epoch: 98 | Iteration: 41792 | Loss: 0.14094500994543835\n",
            "Epoch: 98 | Iteration: 41856 | Loss: 0.4703151926713498\n",
            "Epoch: 98 | Iteration: 41920 | Loss: 0.33469812283545886\n",
            "Epoch: 98 | Iteration: 41984 | Loss: 0.08410714378183573\n",
            "Epoch: 98 | Iteration: 42048 | Loss: 0.031039671633760788\n",
            "Epoch: 98 | Iteration: 42112 | Loss: 0.17972795819789067\n",
            "Epoch: 98 | Iteration: 42176 | Loss: 0.18901971055319577\n",
            "Epoch: 98 | Iteration: 42240 | Loss: 0.06454979573932762\n",
            "Epoch: 98 | Iteration: 42304 | Loss: 0.6682177423203212\n",
            "Epoch: 98 | Iteration: 42368 | Loss: 0.5142821202304894\n",
            "Epoch: 98 | Iteration: 42432 | Loss: 0.7706853407503154\n",
            "Epoch: 98 | Iteration: 42496 | Loss: 0.5542094564172528\n",
            "Epoch: 98 | Iteration: 42560 | Loss: 0.8981746915150093\n",
            "Epoch: 98 | Iteration: 42624 | Loss: 0.6022256418045322\n",
            "Epoch: 98 | Iteration: 42688 | Loss: 0.355832075042004\n",
            "Epoch: 98 | Iteration: 42752 | Loss: 1.0274221522691187\n",
            "Epoch: 98 | Iteration: 42816 | Loss: 1.0190228811815518\n",
            "Epoch: 98 | Iteration: 42880 | Loss: 0.7268083544641479\n",
            "Epoch: 98 | Iteration: 42944 | Loss: 0.4188987875597702\n",
            "Epoch: 98 | Iteration: 43008 | Loss: 0.6034682857457454\n",
            "Epoch: 98 | Iteration: 43072 | Loss: 1.3220469619178337\n",
            "Epoch: 98 | Iteration: 43136 | Loss: 0.03712579273358654\n",
            "Epoch: 98 | Iteration: 43200 | Loss: 0.08561321309787237\n",
            "Epoch: 98 | Iteration: 43264 | Loss: 0.0026337966497724337\n",
            "Epoch: 98 | Iteration: 43328 | Loss: 0.015141206618477301\n",
            "Epoch: 98 | Iteration: 43392 | Loss: 0.38755970147698937\n",
            "Epoch: 98 | Iteration: 43456 | Loss: 0.08492415452327448\n",
            "Epoch: 98 | Iteration: 43520 | Loss: 0.37401972868733624\n",
            "Epoch: 98 | Iteration: 43584 | Loss: 0.1906857850803157\n",
            "Epoch: 98 | Iteration: 43648 | Loss: 1.2194811711447415\n",
            "Epoch: 98 | Iteration: 43712 | Loss: 0.006100868198187075\n",
            "Epoch: 98 | Iteration: 43776 | Loss: 0.04729847731795244\n",
            "Epoch: 98 | Iteration: 43840 | Loss: 0.7880488189535741\n",
            "Epoch: 98 | Iteration: 43904 | Loss: 0.2464380665720808\n",
            "Epoch: 98 | Iteration: 43968 | Loss: 0.1949137654530414\n",
            "Epoch: 98 | Iteration: 44032 | Loss: 0.24306062944728657\n",
            "Epoch: 98 | Iteration: 44096 | Loss: 0.2683018664677866\n",
            "Epoch: 98 | Iteration: 44160 | Loss: 0.09901914747192093\n",
            "Epoch: 98 | Iteration: 44224 | Loss: 0.1770163310752222\n",
            "Epoch: 98 | Iteration: 44288 | Loss: 0.3270009036233895\n",
            "Epoch: 98 | Iteration: 44352 | Loss: 0.30969114864183706\n",
            "Epoch: 98 | Iteration: 44416 | Loss: 1.0594662090638791\n",
            "Epoch: 98 | Iteration: 44480 | Loss: 0.30184528716340475\n",
            "Epoch: 98 | Iteration: 44544 | Loss: 0.030190565104723725\n",
            "Epoch: 98 | Iteration: 44608 | Loss: 0.3910337297468408\n",
            "Epoch: 98 | Iteration: 44672 | Loss: 0.06264781233574032\n",
            "Epoch: 98 | Iteration: 44736 | Loss: 0.036504600719641116\n",
            "Epoch: 98 | Iteration: 44800 | Loss: 0.38489668219056195\n",
            "Epoch: 98 | Iteration: 44864 | Loss: 0.18165046171450924\n",
            "Epoch: 98 | Iteration: 44928 | Loss: 0.6222264986370638\n",
            "Epoch: 98 | Iteration: 44992 | Loss: 0.15501874373127647\n",
            "Epoch: 98 | Iteration: 45056 | Loss: 0.1586931964931788\n",
            "Epoch: 98 | Iteration: 45120 | Loss: 0.8377565811146097\n",
            "Epoch: 98 | Iteration: 45184 | Loss: 0.9617041783743687\n",
            "Epoch: 98 | Iteration: 45248 | Loss: 0.3368429105043491\n",
            "Epoch: 98 | Iteration: 45312 | Loss: 0.5624635044976205\n",
            "Epoch: 98 | Iteration: 45376 | Loss: 0.2984614250834058\n",
            "Epoch: 98 | Iteration: 45440 | Loss: 0.7731263046654873\n",
            "Epoch: 98 | Iteration: 45504 | Loss: 0.3514608158097562\n",
            "Epoch: 98 | Iteration: 45568 | Loss: 0.5739289314706988\n",
            "Epoch: 98 | Iteration: 45632 | Loss: 0.14414408359429745\n",
            "Epoch: 98 | Iteration: 45696 | Loss: 0.007126753931595883\n",
            "Epoch: 98 | Iteration: 45760 | Loss: 0.5172191629602486\n",
            "Epoch: 98 | Iteration: 45824 | Loss: 0.3164696242816003\n",
            "Epoch: 98 | Iteration: 45888 | Loss: 0.6550973029459841\n",
            "Epoch: 98 | Iteration: 45952 | Loss: 0.5009468882437813\n",
            "Epoch: 98 | Iteration: 46016 | Loss: 1.0058801510998312\n",
            "Epoch: 98 | Iteration: 46080 | Loss: 1.1602463914264098\n",
            "Epoch: 98 | Iteration: 46144 | Loss: 0.06926471836173102\n",
            "Epoch: 98 | Iteration: 46208 | Loss: 1.3260991992047362\n",
            "Epoch: 98 | Iteration: 46272 | Loss: 1.2543763552859817\n",
            "Epoch: 98 | Iteration: 46336 | Loss: 0.44237098158581567\n",
            "Epoch: 98 | Iteration: 46400 | Loss: 0.29551293521197025\n",
            "Epoch: 98 | Iteration: 46464 | Loss: 0.01577083075157368\n",
            "Epoch: 98 | Iteration: 46528 | Loss: 0.03667167038709003\n",
            "Epoch: 98 | Iteration: 46592 | Loss: 0.041740746286612215\n",
            "Epoch: 98 | Iteration: 46656 | Loss: 0.2203934551960933\n",
            "Epoch: 98 | Iteration: 46720 | Loss: 0.8349644769146292\n",
            "Epoch: 98 | Iteration: 46784 | Loss: 0.0398880352655578\n",
            "Epoch: 98 | Iteration: 46848 | Loss: 0.7084791352449821\n",
            "Epoch: 98 | Iteration: 46912 | Loss: 0.3108758589224025\n",
            "Epoch: 98 | Iteration: 46976 | Loss: 1.2499745944945027\n",
            "Epoch: 98 | Iteration: 47040 | Loss: 0.0377657389403344\n",
            "Epoch: 98 | Iteration: 47104 | Loss: 0.28473264392512754\n",
            "Epoch: 98 | Iteration: 47168 | Loss: 0.3062591570970683\n",
            "Epoch: 98 | Iteration: 47232 | Loss: 1.3249672345228842\n",
            "Epoch: 98 | Iteration: 47296 | Loss: 0.338087012905545\n",
            "Epoch: 98 | Iteration: 47360 | Loss: 0.8522088230491776\n",
            "Epoch: 98 | Iteration: 47424 | Loss: 0.2604408366465566\n",
            "Epoch: 98 | Iteration: 47488 | Loss: 0.08676716869490718\n",
            "Epoch: 98 | Iteration: 47552 | Loss: 1.1916377413550168\n",
            "Epoch: 98 | Iteration: 47616 | Loss: 0.013489356578974081\n",
            "Epoch: 98 | Iteration: 47680 | Loss: 1.3461014205027153\n",
            "Epoch: 98 | Iteration: 47744 | Loss: 0.819551122354847\n",
            "Epoch: 98 | Iteration: 47808 | Loss: 0.0766136345091114\n",
            "Epoch: 98 | Iteration: 47872 | Loss: 0.7308816847909797\n",
            "Epoch: 98 | Iteration: 47936 | Loss: 0.4504700427399565\n",
            "Epoch: 99 | Iteration: 0 | Loss: 0.14957668030375978\n",
            "Epoch: 99 | Iteration: 64 | Loss: 0.5604482742988516\n",
            "Epoch: 99 | Iteration: 128 | Loss: 1.075350266179934\n",
            "Epoch: 99 | Iteration: 192 | Loss: 0.24721301228091103\n",
            "Epoch: 99 | Iteration: 256 | Loss: 0.16230877160162335\n",
            "Epoch: 99 | Iteration: 320 | Loss: 0.015294727115032915\n",
            "Epoch: 99 | Iteration: 384 | Loss: 0.11800438331101547\n",
            "Epoch: 99 | Iteration: 448 | Loss: 1.7547674696563287\n",
            "Epoch: 99 | Iteration: 512 | Loss: 0.4139823170735438\n",
            "Epoch: 99 | Iteration: 576 | Loss: 0.36477368643421343\n",
            "Epoch: 99 | Iteration: 640 | Loss: 0.34809473656204093\n",
            "Epoch: 99 | Iteration: 704 | Loss: 0.16338085899565663\n",
            "Epoch: 99 | Iteration: 768 | Loss: 0.4291112448483507\n",
            "Epoch: 99 | Iteration: 832 | Loss: 0.40485575636037385\n",
            "Epoch: 99 | Iteration: 896 | Loss: 0.8605205632904713\n",
            "Epoch: 99 | Iteration: 960 | Loss: 0.4700171761210765\n",
            "Epoch: 99 | Iteration: 1024 | Loss: 0.8772171058574461\n",
            "Epoch: 99 | Iteration: 1088 | Loss: 0.6923745912488695\n",
            "Epoch: 99 | Iteration: 1152 | Loss: 0.2792848514876629\n",
            "Epoch: 99 | Iteration: 1216 | Loss: 0.8360267051384314\n",
            "Epoch: 99 | Iteration: 1280 | Loss: 0.1635963243256704\n",
            "Epoch: 99 | Iteration: 1344 | Loss: 1.5097144542168115\n",
            "Epoch: 99 | Iteration: 1408 | Loss: 0.09262323885682001\n",
            "Epoch: 99 | Iteration: 1472 | Loss: 0.5065546608239735\n",
            "Epoch: 99 | Iteration: 1536 | Loss: 0.045297620887491286\n",
            "Epoch: 99 | Iteration: 1600 | Loss: 0.9693681782297718\n",
            "Epoch: 99 | Iteration: 1664 | Loss: 0.09520889319381472\n",
            "Epoch: 99 | Iteration: 1728 | Loss: 0.14658676767844336\n",
            "Epoch: 99 | Iteration: 1792 | Loss: 0.1773132264347086\n",
            "Epoch: 99 | Iteration: 1856 | Loss: 0.16906266511469936\n",
            "Epoch: 99 | Iteration: 1920 | Loss: 0.8234213066880558\n",
            "Epoch: 99 | Iteration: 1984 | Loss: 0.19478419981897\n",
            "Epoch: 99 | Iteration: 2048 | Loss: 0.30789441919249705\n",
            "Epoch: 99 | Iteration: 2112 | Loss: 0.03393126144241078\n",
            "Epoch: 99 | Iteration: 2176 | Loss: 0.3570975617331571\n",
            "Epoch: 99 | Iteration: 2240 | Loss: 0.09310326282169101\n",
            "Epoch: 99 | Iteration: 2304 | Loss: 0.1267859770958849\n",
            "Epoch: 99 | Iteration: 2368 | Loss: 0.19135832765344693\n",
            "Epoch: 99 | Iteration: 2432 | Loss: 0.012488041117175157\n",
            "Epoch: 99 | Iteration: 2496 | Loss: 0.1301236450462315\n",
            "Epoch: 99 | Iteration: 2560 | Loss: 0.3912737282744413\n",
            "Epoch: 99 | Iteration: 2624 | Loss: 0.8401229806019217\n",
            "Epoch: 99 | Iteration: 2688 | Loss: 0.8868095006759291\n",
            "Epoch: 99 | Iteration: 2752 | Loss: 0.06171026679932466\n",
            "Epoch: 99 | Iteration: 2816 | Loss: 0.12582886426664655\n",
            "Epoch: 99 | Iteration: 2880 | Loss: 0.3079359501012834\n",
            "Epoch: 99 | Iteration: 2944 | Loss: 0.09940124205387837\n",
            "Epoch: 99 | Iteration: 3008 | Loss: 0.2839265975062571\n",
            "Epoch: 99 | Iteration: 3072 | Loss: 0.08083432856724472\n",
            "Epoch: 99 | Iteration: 3136 | Loss: 0.0013651284839880425\n",
            "Epoch: 99 | Iteration: 3200 | Loss: 0.09491707853514623\n",
            "Epoch: 99 | Iteration: 3264 | Loss: 0.06901453170686384\n",
            "Epoch: 99 | Iteration: 3328 | Loss: 0.06609118304254845\n",
            "Epoch: 99 | Iteration: 3392 | Loss: 0.050077329982861185\n",
            "Epoch: 99 | Iteration: 3456 | Loss: 0.21083680395005927\n",
            "Epoch: 99 | Iteration: 3520 | Loss: 0.15065705370426447\n",
            "Epoch: 99 | Iteration: 3584 | Loss: 0.06825115624380781\n",
            "Epoch: 99 | Iteration: 3648 | Loss: 0.49073739806473204\n",
            "Epoch: 99 | Iteration: 3712 | Loss: 0.10298177795410828\n",
            "Epoch: 99 | Iteration: 3776 | Loss: 0.32631714982855275\n",
            "Epoch: 99 | Iteration: 3840 | Loss: 0.006304856824794452\n",
            "Epoch: 99 | Iteration: 3904 | Loss: 0.19486675694971928\n",
            "Epoch: 99 | Iteration: 3968 | Loss: 0.07002362112297611\n",
            "Epoch: 99 | Iteration: 4032 | Loss: 0.14026519904571155\n",
            "Epoch: 99 | Iteration: 4096 | Loss: 0.25738602474172595\n",
            "Epoch: 99 | Iteration: 4160 | Loss: 0.19015135252106075\n",
            "Epoch: 99 | Iteration: 4224 | Loss: 0.48645288413692894\n",
            "Epoch: 99 | Iteration: 4288 | Loss: 0.08886586644536924\n",
            "Epoch: 99 | Iteration: 4352 | Loss: 0.280182472166021\n",
            "Epoch: 99 | Iteration: 4416 | Loss: 0.9455967901692343\n",
            "Epoch: 99 | Iteration: 4480 | Loss: 0.06017435967305755\n",
            "Epoch: 99 | Iteration: 4544 | Loss: 0.047181710362166285\n",
            "Epoch: 99 | Iteration: 4608 | Loss: 0.6710837946666074\n",
            "Epoch: 99 | Iteration: 4672 | Loss: 0.05619416253788551\n",
            "Epoch: 99 | Iteration: 4736 | Loss: 0.08884151802680954\n",
            "Epoch: 99 | Iteration: 4800 | Loss: 0.15515994021184903\n",
            "Epoch: 99 | Iteration: 4864 | Loss: 0.0264858791828319\n",
            "Epoch: 99 | Iteration: 4928 | Loss: 0.3547559772372504\n",
            "Epoch: 99 | Iteration: 4992 | Loss: 0.7157428759210822\n",
            "Epoch: 99 | Iteration: 5056 | Loss: 0.4877372826019506\n",
            "Epoch: 99 | Iteration: 5120 | Loss: 0.7497719786975905\n",
            "Epoch: 99 | Iteration: 5184 | Loss: 0.267068291119705\n",
            "Epoch: 99 | Iteration: 5248 | Loss: 0.3195503424886087\n",
            "Epoch: 99 | Iteration: 5312 | Loss: 0.34855902872650435\n",
            "Epoch: 99 | Iteration: 5376 | Loss: 0.10167700890925285\n",
            "Epoch: 99 | Iteration: 5440 | Loss: 0.11676325179466227\n",
            "Epoch: 99 | Iteration: 5504 | Loss: 0.297793919746551\n",
            "Epoch: 99 | Iteration: 5568 | Loss: 0.15851042180640407\n",
            "Epoch: 99 | Iteration: 5632 | Loss: 0.15293513869137648\n",
            "Epoch: 99 | Iteration: 5696 | Loss: 0.85101122092949\n",
            "Epoch: 99 | Iteration: 5760 | Loss: 1.1693596495604983\n",
            "Epoch: 99 | Iteration: 5824 | Loss: 0.15417639993326226\n",
            "Epoch: 99 | Iteration: 5888 | Loss: 0.9667966788767646\n",
            "Epoch: 99 | Iteration: 5952 | Loss: 0.0472107769044914\n",
            "Epoch: 99 | Iteration: 6016 | Loss: 0.07377452030227973\n",
            "Epoch: 99 | Iteration: 6080 | Loss: 0.4394182382470586\n",
            "Epoch: 99 | Iteration: 6144 | Loss: 0.3872761509542247\n",
            "Epoch: 99 | Iteration: 6208 | Loss: 0.47215099805413707\n",
            "Epoch: 99 | Iteration: 6272 | Loss: 0.03277870815957132\n",
            "Epoch: 99 | Iteration: 6336 | Loss: 0.16902022808895212\n",
            "Epoch: 99 | Iteration: 6400 | Loss: 0.99168107675453\n",
            "Epoch: 99 | Iteration: 6464 | Loss: 0.05509705652999039\n",
            "Epoch: 99 | Iteration: 6528 | Loss: 0.23771041544478605\n",
            "Epoch: 99 | Iteration: 6592 | Loss: 0.021742145494315548\n",
            "Epoch: 99 | Iteration: 6656 | Loss: 0.25444837792798014\n",
            "Epoch: 99 | Iteration: 6720 | Loss: 0.04980650794856263\n",
            "Epoch: 99 | Iteration: 6784 | Loss: 0.8787401014736189\n",
            "Epoch: 99 | Iteration: 6848 | Loss: 1.6892762017649836\n",
            "Epoch: 99 | Iteration: 6912 | Loss: 0.4055699411409268\n",
            "Epoch: 99 | Iteration: 6976 | Loss: 0.5063264472180725\n",
            "Epoch: 99 | Iteration: 7040 | Loss: 1.0542752640258184\n",
            "Epoch: 99 | Iteration: 7104 | Loss: 0.21021593623454748\n",
            "Epoch: 99 | Iteration: 7168 | Loss: 0.11601779108464037\n",
            "Epoch: 99 | Iteration: 7232 | Loss: 0.8412674929777911\n",
            "Epoch: 99 | Iteration: 7296 | Loss: 0.7372213427910693\n",
            "Epoch: 99 | Iteration: 7360 | Loss: 0.0437012130356256\n",
            "Epoch: 99 | Iteration: 7424 | Loss: 0.07997949824844583\n",
            "Epoch: 99 | Iteration: 7488 | Loss: 0.49851277013832396\n",
            "Epoch: 99 | Iteration: 7552 | Loss: 0.6578748415422031\n",
            "Epoch: 99 | Iteration: 7616 | Loss: 0.18001503772425048\n",
            "Epoch: 99 | Iteration: 7680 | Loss: 0.5534382009831447\n",
            "Epoch: 99 | Iteration: 7744 | Loss: 0.31704184359921617\n",
            "Epoch: 99 | Iteration: 7808 | Loss: 0.43951002645912657\n",
            "Epoch: 99 | Iteration: 7872 | Loss: 0.5547389051881876\n",
            "Epoch: 99 | Iteration: 7936 | Loss: 0.2823035110196299\n",
            "Epoch: 99 | Iteration: 8000 | Loss: 0.10081968756264066\n",
            "Epoch: 99 | Iteration: 8064 | Loss: 0.2910631477262131\n",
            "Epoch: 99 | Iteration: 8128 | Loss: 0.09707496563706035\n",
            "Epoch: 99 | Iteration: 8192 | Loss: 1.7622680088166067\n",
            "Epoch: 99 | Iteration: 8256 | Loss: 0.136083595516085\n",
            "Epoch: 99 | Iteration: 8320 | Loss: 0.3193796927466047\n",
            "Epoch: 99 | Iteration: 8384 | Loss: 0.33441525082450063\n",
            "Epoch: 99 | Iteration: 8448 | Loss: 0.5834824384496982\n",
            "Epoch: 99 | Iteration: 8512 | Loss: 0.059287417975363896\n",
            "Epoch: 99 | Iteration: 8576 | Loss: 0.1407001414463765\n",
            "Epoch: 99 | Iteration: 8640 | Loss: 0.17889043446638972\n",
            "Epoch: 99 | Iteration: 8704 | Loss: 1.0521632020139908\n",
            "Epoch: 99 | Iteration: 8768 | Loss: 0.5219830840386613\n",
            "Epoch: 99 | Iteration: 8832 | Loss: 1.025903214679942\n",
            "Epoch: 99 | Iteration: 8896 | Loss: 0.5335193845768718\n",
            "Epoch: 99 | Iteration: 8960 | Loss: 0.20234053413795455\n",
            "Epoch: 99 | Iteration: 9024 | Loss: 0.1850223840966873\n",
            "Epoch: 99 | Iteration: 9088 | Loss: 0.21677162476869222\n",
            "Epoch: 99 | Iteration: 9152 | Loss: 0.08100475379899633\n",
            "Epoch: 99 | Iteration: 9216 | Loss: 0.23606818342675184\n",
            "Epoch: 99 | Iteration: 9280 | Loss: 0.7852807802147824\n",
            "Epoch: 99 | Iteration: 9344 | Loss: 0.19999419459478543\n",
            "Epoch: 99 | Iteration: 9408 | Loss: 0.5826128672994508\n",
            "Epoch: 99 | Iteration: 9472 | Loss: 0.5223928825723531\n",
            "Epoch: 99 | Iteration: 9536 | Loss: 0.0757791518607573\n",
            "Epoch: 99 | Iteration: 9600 | Loss: 0.13497791099812145\n",
            "Epoch: 99 | Iteration: 9664 | Loss: 0.0029737540078217436\n",
            "Epoch: 99 | Iteration: 9728 | Loss: 0.40736824859403853\n",
            "Epoch: 99 | Iteration: 9792 | Loss: 0.085531985869653\n",
            "Epoch: 99 | Iteration: 9856 | Loss: 0.020354779757063884\n",
            "Epoch: 99 | Iteration: 9920 | Loss: 0.2962462693990724\n",
            "Epoch: 99 | Iteration: 9984 | Loss: 0.12729044359441882\n",
            "Epoch: 99 | Iteration: 10048 | Loss: 0.32119031300288337\n",
            "Epoch: 99 | Iteration: 10112 | Loss: 0.5063817696634516\n",
            "Epoch: 99 | Iteration: 10176 | Loss: 0.5975273223870288\n",
            "Epoch: 99 | Iteration: 10240 | Loss: 0.4888785698418358\n",
            "Epoch: 99 | Iteration: 10304 | Loss: 0.02503266761571893\n",
            "Epoch: 99 | Iteration: 10368 | Loss: 0.11603989247593594\n",
            "Epoch: 99 | Iteration: 10432 | Loss: 0.04006661261230254\n",
            "Epoch: 99 | Iteration: 10496 | Loss: 0.02536216784471418\n",
            "Epoch: 99 | Iteration: 10560 | Loss: 0.03131984554708098\n",
            "Epoch: 99 | Iteration: 10624 | Loss: 0.1338269266522294\n",
            "Epoch: 99 | Iteration: 10688 | Loss: 0.0896871071233341\n",
            "Epoch: 99 | Iteration: 10752 | Loss: 0.3510953639115466\n",
            "Epoch: 99 | Iteration: 10816 | Loss: 0.17092056615543538\n",
            "Epoch: 99 | Iteration: 10880 | Loss: 0.1559649431302763\n",
            "Epoch: 99 | Iteration: 10944 | Loss: 1.115567123379088\n",
            "Epoch: 99 | Iteration: 11008 | Loss: 0.21453984602570922\n",
            "Epoch: 99 | Iteration: 11072 | Loss: 0.3034833630753413\n",
            "Epoch: 99 | Iteration: 11136 | Loss: 0.07966801673045841\n",
            "Epoch: 99 | Iteration: 11200 | Loss: 0.23806022395303036\n",
            "Epoch: 99 | Iteration: 11264 | Loss: 0.025661192609040144\n",
            "Epoch: 99 | Iteration: 11328 | Loss: 0.08662698661785195\n",
            "Epoch: 99 | Iteration: 11392 | Loss: 0.024193424739382637\n",
            "Epoch: 99 | Iteration: 11456 | Loss: 0.08267530071162202\n",
            "Epoch: 99 | Iteration: 11520 | Loss: 0.4367158327550892\n",
            "Epoch: 99 | Iteration: 11584 | Loss: 0.4560915360556581\n",
            "Epoch: 99 | Iteration: 11648 | Loss: 0.6174987478257111\n",
            "Epoch: 99 | Iteration: 11712 | Loss: 0.7421474470961831\n",
            "Epoch: 99 | Iteration: 11776 | Loss: 0.16706303183324328\n",
            "Epoch: 99 | Iteration: 11840 | Loss: 0.45720670296017846\n",
            "Epoch: 99 | Iteration: 11904 | Loss: 0.49182002264245317\n",
            "Epoch: 99 | Iteration: 11968 | Loss: 0.16077199337638143\n",
            "Epoch: 99 | Iteration: 12032 | Loss: 0.07963642430239809\n",
            "Epoch: 99 | Iteration: 12096 | Loss: 0.03020282154193602\n",
            "Epoch: 99 | Iteration: 12160 | Loss: 0.11297230919687502\n",
            "Epoch: 99 | Iteration: 12224 | Loss: 0.12415832877496627\n",
            "Epoch: 99 | Iteration: 12288 | Loss: 0.21214755253496845\n",
            "Epoch: 99 | Iteration: 12352 | Loss: 0.15502172283187843\n",
            "Epoch: 99 | Iteration: 12416 | Loss: 0.08495258446935383\n",
            "Epoch: 99 | Iteration: 12480 | Loss: 0.05470550789331331\n",
            "Epoch: 99 | Iteration: 12544 | Loss: 0.6278273047198297\n",
            "Epoch: 99 | Iteration: 12608 | Loss: 0.13470884738843425\n",
            "Epoch: 99 | Iteration: 12672 | Loss: 0.565743455752752\n",
            "Epoch: 99 | Iteration: 12736 | Loss: 0.043216662828316196\n",
            "Epoch: 99 | Iteration: 12800 | Loss: 0.6071301855634277\n",
            "Epoch: 99 | Iteration: 12864 | Loss: 0.14005302182786897\n",
            "Epoch: 99 | Iteration: 12928 | Loss: 0.21115523135286085\n",
            "Epoch: 99 | Iteration: 12992 | Loss: 0.11329263520974994\n",
            "Epoch: 99 | Iteration: 13056 | Loss: 0.09322114658699343\n",
            "Epoch: 99 | Iteration: 13120 | Loss: 0.06140080625230853\n",
            "Epoch: 99 | Iteration: 13184 | Loss: 0.4533298660446033\n",
            "Epoch: 99 | Iteration: 13248 | Loss: 0.05425745715111928\n",
            "Epoch: 99 | Iteration: 13312 | Loss: 0.014186228472817471\n",
            "Epoch: 99 | Iteration: 13376 | Loss: 0.4498960115383146\n",
            "Epoch: 99 | Iteration: 13440 | Loss: 0.056714641087159665\n",
            "Epoch: 99 | Iteration: 13504 | Loss: 0.21868814233246162\n",
            "Epoch: 99 | Iteration: 13568 | Loss: 0.08817127679436483\n",
            "Epoch: 99 | Iteration: 13632 | Loss: 0.3265007531422002\n",
            "Epoch: 99 | Iteration: 13696 | Loss: 0.44692305511678276\n",
            "Epoch: 99 | Iteration: 13760 | Loss: 0.022408940807374374\n",
            "Epoch: 99 | Iteration: 13824 | Loss: 0.23558638895140718\n",
            "Epoch: 99 | Iteration: 13888 | Loss: 0.11793115048599792\n",
            "Epoch: 99 | Iteration: 13952 | Loss: 0.7680182571350714\n",
            "Epoch: 99 | Iteration: 14016 | Loss: 0.22558508665235616\n",
            "Epoch: 99 | Iteration: 14080 | Loss: 0.2310149226159729\n",
            "Epoch: 99 | Iteration: 14144 | Loss: 0.19471976219613288\n",
            "Epoch: 99 | Iteration: 14208 | Loss: 0.570520955459589\n",
            "Epoch: 99 | Iteration: 14272 | Loss: 0.41580129259945064\n",
            "Epoch: 99 | Iteration: 14336 | Loss: 0.24044349463598827\n",
            "Epoch: 99 | Iteration: 14400 | Loss: 0.1502172507805014\n",
            "Epoch: 99 | Iteration: 14464 | Loss: 0.08103813234784998\n",
            "Epoch: 99 | Iteration: 14528 | Loss: 0.2865598731244889\n",
            "Epoch: 99 | Iteration: 14592 | Loss: 0.1691831372512051\n",
            "Epoch: 99 | Iteration: 14656 | Loss: 0.083631172544497\n",
            "Epoch: 99 | Iteration: 14720 | Loss: 0.6855484671812209\n",
            "Epoch: 99 | Iteration: 14784 | Loss: 0.7271442698959145\n",
            "Epoch: 99 | Iteration: 14848 | Loss: 0.20850245622234545\n",
            "Epoch: 99 | Iteration: 14912 | Loss: 0.1862429715104519\n",
            "Epoch: 99 | Iteration: 14976 | Loss: 0.0338541595383441\n",
            "Epoch: 99 | Iteration: 15040 | Loss: 0.1296501391779436\n",
            "Epoch: 99 | Iteration: 15104 | Loss: 0.5749542338722732\n",
            "Epoch: 99 | Iteration: 15168 | Loss: 0.12634589536274365\n",
            "Epoch: 99 | Iteration: 15232 | Loss: 0.11718434502560315\n",
            "Epoch: 99 | Iteration: 15296 | Loss: 0.43616811105658815\n",
            "Epoch: 99 | Iteration: 15360 | Loss: 0.33329362773828597\n",
            "Epoch: 99 | Iteration: 15424 | Loss: 0.06112711323886975\n",
            "Epoch: 99 | Iteration: 15488 | Loss: 0.08978160143400285\n",
            "Epoch: 99 | Iteration: 15552 | Loss: 0.24406389353974955\n",
            "Epoch: 99 | Iteration: 15616 | Loss: 0.04828416909436649\n",
            "Epoch: 99 | Iteration: 15680 | Loss: 0.19582616408515294\n",
            "Epoch: 99 | Iteration: 15744 | Loss: 0.3915126922040113\n",
            "Epoch: 99 | Iteration: 15808 | Loss: 0.4612644139133363\n",
            "Epoch: 99 | Iteration: 15872 | Loss: 0.2388696223065809\n",
            "Epoch: 99 | Iteration: 15936 | Loss: 1.0092470749410387\n",
            "Epoch: 99 | Iteration: 16000 | Loss: 0.46798740416931495\n",
            "Epoch: 99 | Iteration: 16064 | Loss: 0.058032091069630545\n",
            "Epoch: 99 | Iteration: 16128 | Loss: 0.32380837523737127\n",
            "Epoch: 99 | Iteration: 16192 | Loss: 0.009968000204538877\n",
            "Epoch: 99 | Iteration: 16256 | Loss: 0.1962474732791361\n",
            "Epoch: 99 | Iteration: 16320 | Loss: 0.1504313029946923\n",
            "Epoch: 99 | Iteration: 16384 | Loss: 0.38287594886872633\n",
            "Epoch: 99 | Iteration: 16448 | Loss: 0.14776857858136616\n",
            "Epoch: 99 | Iteration: 16512 | Loss: 0.7104859248097116\n",
            "Epoch: 99 | Iteration: 16576 | Loss: 0.05173659230859508\n",
            "Epoch: 99 | Iteration: 16640 | Loss: 0.49192208609921884\n",
            "Epoch: 99 | Iteration: 16704 | Loss: 0.2940936923025227\n",
            "Epoch: 99 | Iteration: 16768 | Loss: 0.3279584096526827\n",
            "Epoch: 99 | Iteration: 16832 | Loss: 0.4030219742350162\n",
            "Epoch: 99 | Iteration: 16896 | Loss: 0.3489386791086815\n",
            "Epoch: 99 | Iteration: 16960 | Loss: 0.27139189450333\n",
            "Epoch: 99 | Iteration: 17024 | Loss: 0.2814790412410064\n",
            "Epoch: 99 | Iteration: 17088 | Loss: 0.4915880826965052\n",
            "Epoch: 99 | Iteration: 17152 | Loss: 0.3874394039665269\n",
            "Epoch: 99 | Iteration: 17216 | Loss: 0.4585494941297892\n",
            "Epoch: 99 | Iteration: 17280 | Loss: 0.011686418073092956\n",
            "Epoch: 99 | Iteration: 17344 | Loss: 0.3979701301774401\n",
            "Epoch: 99 | Iteration: 17408 | Loss: 0.12108810707442871\n",
            "Epoch: 99 | Iteration: 17472 | Loss: 0.18066710478761722\n",
            "Epoch: 99 | Iteration: 17536 | Loss: 0.5898192855762705\n",
            "Epoch: 99 | Iteration: 17600 | Loss: 0.18723604876559516\n",
            "Epoch: 99 | Iteration: 17664 | Loss: 0.3599861096538906\n",
            "Epoch: 99 | Iteration: 17728 | Loss: 1.4221078910970613\n",
            "Epoch: 99 | Iteration: 17792 | Loss: 0.20400412367727325\n",
            "Epoch: 99 | Iteration: 17856 | Loss: 0.3789799502039738\n",
            "Epoch: 99 | Iteration: 17920 | Loss: 0.03759460563448047\n",
            "Epoch: 99 | Iteration: 17984 | Loss: 0.07320313585245869\n",
            "Epoch: 99 | Iteration: 18048 | Loss: 0.43416770432439145\n",
            "Epoch: 99 | Iteration: 18112 | Loss: 0.1997228559996411\n",
            "Epoch: 99 | Iteration: 18176 | Loss: 0.09923374989335246\n",
            "Epoch: 99 | Iteration: 18240 | Loss: 0.5846001697126729\n",
            "Epoch: 99 | Iteration: 18304 | Loss: 0.3971587076012761\n",
            "Epoch: 99 | Iteration: 18368 | Loss: 0.3209888000288439\n",
            "Epoch: 99 | Iteration: 18432 | Loss: 0.6929044984431192\n",
            "Epoch: 99 | Iteration: 18496 | Loss: 0.14893509659216558\n",
            "Epoch: 99 | Iteration: 18560 | Loss: 0.17422879044249923\n",
            "Epoch: 99 | Iteration: 18624 | Loss: 0.06860354870250632\n",
            "Epoch: 99 | Iteration: 18688 | Loss: 0.3360627338244263\n",
            "Epoch: 99 | Iteration: 18752 | Loss: 0.07165190628663551\n",
            "Epoch: 99 | Iteration: 18816 | Loss: 0.08248978811982313\n",
            "Epoch: 99 | Iteration: 18880 | Loss: 0.2038645839391255\n",
            "Epoch: 99 | Iteration: 18944 | Loss: 0.18098031477570437\n",
            "Epoch: 99 | Iteration: 19008 | Loss: 0.1277904830446216\n",
            "Epoch: 99 | Iteration: 19072 | Loss: 1.3156293967139077\n",
            "Epoch: 99 | Iteration: 19136 | Loss: 0.14814413709455357\n",
            "Epoch: 99 | Iteration: 19200 | Loss: 0.16742024612233047\n",
            "Epoch: 99 | Iteration: 19264 | Loss: 0.48174197070222746\n",
            "Epoch: 99 | Iteration: 19328 | Loss: 1.5211431611191124\n",
            "Epoch: 99 | Iteration: 19392 | Loss: 0.2894018939109456\n",
            "Epoch: 99 | Iteration: 19456 | Loss: 0.22747145620087855\n",
            "Epoch: 99 | Iteration: 19520 | Loss: 0.05245926630265671\n",
            "Epoch: 99 | Iteration: 19584 | Loss: 0.10704542258113652\n",
            "Epoch: 99 | Iteration: 19648 | Loss: 0.09762696832062692\n",
            "Epoch: 99 | Iteration: 19712 | Loss: 0.013093263751415299\n",
            "Epoch: 99 | Iteration: 19776 | Loss: 0.8334385450383787\n",
            "Epoch: 99 | Iteration: 19840 | Loss: 0.3780489935034007\n",
            "Epoch: 99 | Iteration: 19904 | Loss: 0.35267165230871594\n",
            "Epoch: 99 | Iteration: 19968 | Loss: 0.28638081016664646\n",
            "Epoch: 99 | Iteration: 20032 | Loss: 0.24642984918069807\n",
            "Epoch: 99 | Iteration: 20096 | Loss: 0.08648923983236184\n",
            "Epoch: 99 | Iteration: 20160 | Loss: 0.8063879025545959\n",
            "Epoch: 99 | Iteration: 20224 | Loss: 0.42549440403894756\n",
            "Epoch: 99 | Iteration: 20288 | Loss: 0.8002097019487302\n",
            "Epoch: 99 | Iteration: 20352 | Loss: 0.0058261587174664765\n",
            "Epoch: 99 | Iteration: 20416 | Loss: 0.014423737626590181\n",
            "Epoch: 99 | Iteration: 20480 | Loss: 0.09584311298552661\n",
            "Epoch: 99 | Iteration: 20544 | Loss: 0.17870634389009532\n",
            "Epoch: 99 | Iteration: 20608 | Loss: 0.12033116186340814\n",
            "Epoch: 99 | Iteration: 20672 | Loss: 1.6678314318010343\n",
            "Epoch: 99 | Iteration: 20736 | Loss: 1.0557167546238286\n",
            "Epoch: 99 | Iteration: 20800 | Loss: 0.3529942207302831\n",
            "Epoch: 99 | Iteration: 20864 | Loss: 0.30642990194966424\n",
            "Epoch: 99 | Iteration: 20928 | Loss: 0.254141918925808\n",
            "Epoch: 99 | Iteration: 20992 | Loss: 0.3705404075090818\n",
            "Epoch: 99 | Iteration: 21056 | Loss: 0.2577411753704545\n",
            "Epoch: 99 | Iteration: 21120 | Loss: 0.32291491675404893\n",
            "Epoch: 99 | Iteration: 21184 | Loss: 0.20133709255056234\n",
            "Epoch: 99 | Iteration: 21248 | Loss: 0.18910739261481158\n",
            "Epoch: 99 | Iteration: 21312 | Loss: 0.4253356620340193\n",
            "Epoch: 99 | Iteration: 21376 | Loss: 0.1624848795549511\n",
            "Epoch: 99 | Iteration: 21440 | Loss: 0.5798478902602973\n",
            "Epoch: 99 | Iteration: 21504 | Loss: 0.18904089352895387\n",
            "Epoch: 99 | Iteration: 21568 | Loss: 1.3560427453881427\n",
            "Epoch: 99 | Iteration: 21632 | Loss: 0.22984721153435594\n",
            "Epoch: 99 | Iteration: 21696 | Loss: 0.09260802311143734\n",
            "Epoch: 99 | Iteration: 21760 | Loss: 0.005767790308431019\n",
            "Epoch: 99 | Iteration: 21824 | Loss: 0.003239347266199746\n",
            "Epoch: 99 | Iteration: 21888 | Loss: 0.5266025601155951\n",
            "Epoch: 99 | Iteration: 21952 | Loss: 0.34346579484766243\n",
            "Epoch: 99 | Iteration: 22016 | Loss: 0.0706075720476286\n",
            "Epoch: 99 | Iteration: 22080 | Loss: 0.40233255657949657\n",
            "Epoch: 99 | Iteration: 22144 | Loss: 0.3295735021684201\n",
            "Epoch: 99 | Iteration: 22208 | Loss: 0.22421979761252359\n",
            "Epoch: 99 | Iteration: 22272 | Loss: 0.7023348175912861\n",
            "Epoch: 99 | Iteration: 22336 | Loss: 0.036672727027814156\n",
            "Epoch: 99 | Iteration: 22400 | Loss: 0.181807476499361\n",
            "Epoch: 99 | Iteration: 22464 | Loss: 0.5711468137015918\n",
            "Epoch: 99 | Iteration: 22528 | Loss: 0.8926861828614194\n",
            "Epoch: 99 | Iteration: 22592 | Loss: 0.4319227245618228\n",
            "Epoch: 99 | Iteration: 22656 | Loss: 0.12183730195679132\n",
            "Epoch: 99 | Iteration: 22720 | Loss: 0.5330043431634144\n",
            "Epoch: 99 | Iteration: 22784 | Loss: 0.14893298497537566\n",
            "Epoch: 99 | Iteration: 22848 | Loss: 0.2471285551894184\n",
            "Epoch: 99 | Iteration: 22912 | Loss: 0.0016240992751002264\n",
            "Epoch: 99 | Iteration: 22976 | Loss: 0.24212707259992522\n",
            "Epoch: 99 | Iteration: 23040 | Loss: 0.20619878978900633\n",
            "Epoch: 99 | Iteration: 23104 | Loss: 0.11507445876749077\n",
            "Epoch: 99 | Iteration: 23168 | Loss: 0.45557130250387423\n",
            "Epoch: 99 | Iteration: 23232 | Loss: 0.022772342276538182\n",
            "Epoch: 99 | Iteration: 23296 | Loss: 0.11283979768417313\n",
            "Epoch: 99 | Iteration: 23360 | Loss: 0.2889525520295631\n",
            "Epoch: 99 | Iteration: 23424 | Loss: 0.07522988063991043\n",
            "Epoch: 99 | Iteration: 23488 | Loss: 0.08531610518215446\n",
            "Epoch: 99 | Iteration: 23552 | Loss: 0.6184925223266229\n",
            "Epoch: 99 | Iteration: 23616 | Loss: 0.6794679500158277\n",
            "Epoch: 99 | Iteration: 23680 | Loss: 0.8023371607057317\n",
            "Epoch: 99 | Iteration: 23744 | Loss: 0.011687563910307106\n",
            "Epoch: 99 | Iteration: 23808 | Loss: 0.351502266189748\n",
            "Epoch: 99 | Iteration: 23872 | Loss: 0.5826435323512499\n",
            "Epoch: 99 | Iteration: 23936 | Loss: 0.9740925525956123\n",
            "Epoch: 99 | Iteration: 24000 | Loss: 0.15257101418153665\n",
            "Epoch: 99 | Iteration: 24064 | Loss: 0.07185449811906525\n",
            "Epoch: 99 | Iteration: 24128 | Loss: 0.14296282186560147\n",
            "Epoch: 99 | Iteration: 24192 | Loss: 0.24818652105846584\n",
            "Epoch: 99 | Iteration: 24256 | Loss: 0.6781274829215109\n",
            "Epoch: 99 | Iteration: 24320 | Loss: 0.37473237985158175\n",
            "Epoch: 99 | Iteration: 24384 | Loss: 0.00882932970669447\n",
            "Epoch: 99 | Iteration: 24448 | Loss: 0.20735608969214134\n",
            "Epoch: 99 | Iteration: 24512 | Loss: 0.17307036806510495\n",
            "Epoch: 99 | Iteration: 24576 | Loss: 0.34120283571194554\n",
            "Epoch: 99 | Iteration: 24640 | Loss: 0.17567219631396921\n",
            "Epoch: 99 | Iteration: 24704 | Loss: 0.10467221852250883\n",
            "Epoch: 99 | Iteration: 24768 | Loss: 0.5654859702630227\n",
            "Epoch: 99 | Iteration: 24832 | Loss: 0.051011142211925944\n",
            "Epoch: 99 | Iteration: 24896 | Loss: 0.25264116572374185\n",
            "Epoch: 99 | Iteration: 24960 | Loss: 0.010437169696301173\n",
            "Epoch: 99 | Iteration: 25024 | Loss: 0.03977840232880173\n",
            "Epoch: 99 | Iteration: 25088 | Loss: 0.29248693543105053\n",
            "Epoch: 99 | Iteration: 25152 | Loss: 0.6791842186641928\n",
            "Epoch: 99 | Iteration: 25216 | Loss: 0.17407668826287603\n",
            "Epoch: 99 | Iteration: 25280 | Loss: 0.13664893845924425\n",
            "Epoch: 99 | Iteration: 25344 | Loss: 0.023042458432314124\n",
            "Epoch: 99 | Iteration: 25408 | Loss: 0.028798920425525128\n",
            "Epoch: 99 | Iteration: 25472 | Loss: 0.5412885334207189\n",
            "Epoch: 99 | Iteration: 25536 | Loss: 1.1782575592377045\n",
            "Epoch: 99 | Iteration: 25600 | Loss: 0.06355135466273171\n",
            "Epoch: 99 | Iteration: 25664 | Loss: 0.5739322691035772\n",
            "Epoch: 99 | Iteration: 25728 | Loss: 0.5560279182342129\n",
            "Epoch: 99 | Iteration: 25792 | Loss: 0.7900788816049567\n",
            "Epoch: 99 | Iteration: 25856 | Loss: 0.07547432504217488\n",
            "Epoch: 99 | Iteration: 25920 | Loss: 0.10481954152802345\n",
            "Epoch: 99 | Iteration: 25984 | Loss: 0.04340881877488956\n",
            "Epoch: 99 | Iteration: 26048 | Loss: 0.2906594526676528\n",
            "Epoch: 99 | Iteration: 26112 | Loss: 0.2502663380750897\n",
            "Epoch: 99 | Iteration: 26176 | Loss: 0.17831308760740922\n",
            "Epoch: 99 | Iteration: 26240 | Loss: 0.19347593258758128\n",
            "Epoch: 99 | Iteration: 26304 | Loss: 0.5136923904656148\n",
            "Epoch: 99 | Iteration: 26368 | Loss: 0.7941634894240416\n",
            "Epoch: 99 | Iteration: 26432 | Loss: 0.9036015776654122\n",
            "Epoch: 99 | Iteration: 26496 | Loss: 1.3935069707150074\n",
            "Epoch: 99 | Iteration: 26560 | Loss: 0.9273804036821884\n",
            "Epoch: 99 | Iteration: 26624 | Loss: 0.8888646478230595\n",
            "Epoch: 99 | Iteration: 26688 | Loss: 0.766662488650494\n",
            "Epoch: 99 | Iteration: 26752 | Loss: 0.9231189243155421\n",
            "Epoch: 99 | Iteration: 26816 | Loss: 0.25547077491013803\n",
            "Epoch: 99 | Iteration: 26880 | Loss: 1.0790581707988003\n",
            "Epoch: 99 | Iteration: 26944 | Loss: 0.014689678946589776\n",
            "Epoch: 99 | Iteration: 27008 | Loss: 0.11592616210735555\n",
            "Epoch: 99 | Iteration: 27072 | Loss: 0.0665938658833\n",
            "Epoch: 99 | Iteration: 27136 | Loss: 1.5631691440128068\n",
            "Epoch: 99 | Iteration: 27200 | Loss: 0.8627370712712801\n",
            "Epoch: 99 | Iteration: 27264 | Loss: 0.11179061200235299\n",
            "Epoch: 99 | Iteration: 27328 | Loss: 0.018568509707702748\n",
            "Epoch: 99 | Iteration: 27392 | Loss: 0.1321118655653971\n",
            "Epoch: 99 | Iteration: 27456 | Loss: 0.5275471176915685\n",
            "Epoch: 99 | Iteration: 27520 | Loss: 0.6085087526987238\n",
            "Epoch: 99 | Iteration: 27584 | Loss: 0.6862729664619867\n",
            "Epoch: 99 | Iteration: 27648 | Loss: 0.5495695860903195\n",
            "Epoch: 99 | Iteration: 27712 | Loss: 0.19569219160924256\n",
            "Epoch: 99 | Iteration: 27776 | Loss: 0.287142288125862\n",
            "Epoch: 99 | Iteration: 27840 | Loss: 0.8815149199513649\n",
            "Epoch: 99 | Iteration: 27904 | Loss: 0.0801319058401084\n",
            "Epoch: 99 | Iteration: 27968 | Loss: 0.023571263825311904\n",
            "Epoch: 99 | Iteration: 28032 | Loss: 0.20310305302187576\n",
            "Epoch: 99 | Iteration: 28096 | Loss: 0.13166504165987056\n",
            "Epoch: 99 | Iteration: 28160 | Loss: 0.6083264469918249\n",
            "Epoch: 99 | Iteration: 28224 | Loss: 0.07065116979648382\n",
            "Epoch: 99 | Iteration: 28288 | Loss: 0.010924868411691605\n",
            "Epoch: 99 | Iteration: 28352 | Loss: 0.6391784097758255\n",
            "Epoch: 99 | Iteration: 28416 | Loss: 0.09196683072350373\n",
            "Epoch: 99 | Iteration: 28480 | Loss: 0.07912369285799066\n",
            "Epoch: 99 | Iteration: 28544 | Loss: 0.11281070040279625\n",
            "Epoch: 99 | Iteration: 28608 | Loss: 0.6260243145550086\n",
            "Epoch: 99 | Iteration: 28672 | Loss: 0.2652715285540751\n",
            "Epoch: 99 | Iteration: 28736 | Loss: 0.09845876877450334\n",
            "Epoch: 99 | Iteration: 28800 | Loss: 0.17884328369018487\n",
            "Epoch: 99 | Iteration: 28864 | Loss: 0.10572810927468698\n",
            "Epoch: 99 | Iteration: 28928 | Loss: 0.44026816579633765\n",
            "Epoch: 99 | Iteration: 28992 | Loss: 0.20186669851669992\n",
            "Epoch: 99 | Iteration: 29056 | Loss: 0.15022056008665813\n",
            "Epoch: 99 | Iteration: 29120 | Loss: 0.2786298357119208\n",
            "Epoch: 99 | Iteration: 29184 | Loss: 0.5813680012942233\n",
            "Epoch: 99 | Iteration: 29248 | Loss: 0.3594568173765803\n",
            "Epoch: 99 | Iteration: 29312 | Loss: 0.1082191065978596\n",
            "Epoch: 99 | Iteration: 29376 | Loss: 0.153402757577349\n",
            "Epoch: 99 | Iteration: 29440 | Loss: 0.4424194622797927\n",
            "Epoch: 99 | Iteration: 29504 | Loss: 0.18583009100625764\n",
            "Epoch: 99 | Iteration: 29568 | Loss: 0.20028036294891477\n",
            "Epoch: 99 | Iteration: 29632 | Loss: 0.13398406665307375\n",
            "Epoch: 99 | Iteration: 29696 | Loss: 0.496767688041342\n",
            "Epoch: 99 | Iteration: 29760 | Loss: 0.04431455534139854\n",
            "Epoch: 99 | Iteration: 29824 | Loss: 0.24387051099312984\n",
            "Epoch: 99 | Iteration: 29888 | Loss: 0.4657528405282104\n",
            "Epoch: 99 | Iteration: 29952 | Loss: 0.05894765669915674\n",
            "Epoch: 99 | Iteration: 30016 | Loss: 0.622517921459856\n",
            "Epoch: 99 | Iteration: 30080 | Loss: 0.21647584875385245\n",
            "Epoch: 99 | Iteration: 30144 | Loss: 0.2663414303562216\n",
            "Epoch: 99 | Iteration: 30208 | Loss: 0.09675235180749872\n",
            "Epoch: 99 | Iteration: 30272 | Loss: 0.05098559072280841\n",
            "Epoch: 99 | Iteration: 30336 | Loss: 0.21509586724861585\n",
            "Epoch: 99 | Iteration: 30400 | Loss: 0.04912926056452736\n",
            "Epoch: 99 | Iteration: 30464 | Loss: 0.2671921964027211\n",
            "Epoch: 99 | Iteration: 30528 | Loss: 0.13598539303050722\n",
            "Epoch: 99 | Iteration: 30592 | Loss: 0.08629223305268735\n",
            "Epoch: 99 | Iteration: 30656 | Loss: 0.5541256922886021\n",
            "Epoch: 99 | Iteration: 30720 | Loss: 0.22401176429341\n",
            "Epoch: 99 | Iteration: 30784 | Loss: 0.17425444515781147\n",
            "Epoch: 99 | Iteration: 30848 | Loss: 0.31827222686643264\n",
            "Epoch: 99 | Iteration: 30912 | Loss: 0.6969595841074974\n",
            "Epoch: 99 | Iteration: 30976 | Loss: 0.1762657493409406\n",
            "Epoch: 99 | Iteration: 31040 | Loss: 0.4591703622768799\n",
            "Epoch: 99 | Iteration: 31104 | Loss: 0.41337315076939984\n",
            "Epoch: 99 | Iteration: 31168 | Loss: 1.6645412972666982\n",
            "Epoch: 99 | Iteration: 31232 | Loss: 0.16650684515922107\n",
            "Epoch: 99 | Iteration: 31296 | Loss: 0.7851545471798975\n",
            "Epoch: 99 | Iteration: 31360 | Loss: 0.2550519386122646\n",
            "Epoch: 99 | Iteration: 31424 | Loss: 0.2916211936680909\n",
            "Epoch: 99 | Iteration: 31488 | Loss: 0.0803455065279821\n",
            "Epoch: 99 | Iteration: 31552 | Loss: 1.080031173364125\n",
            "Epoch: 99 | Iteration: 31616 | Loss: 0.3237096467764767\n",
            "Epoch: 99 | Iteration: 31680 | Loss: 1.3316053302642978\n",
            "Epoch: 99 | Iteration: 31744 | Loss: 0.7382370182764091\n",
            "Epoch: 99 | Iteration: 31808 | Loss: 0.18902717218198833\n",
            "Epoch: 99 | Iteration: 31872 | Loss: 0.1684103499056211\n",
            "Epoch: 99 | Iteration: 31936 | Loss: 0.23475635312156118\n",
            "Epoch: 99 | Iteration: 32000 | Loss: 0.27164314843128756\n",
            "Epoch: 99 | Iteration: 32064 | Loss: 0.2969292542857227\n",
            "Epoch: 99 | Iteration: 32128 | Loss: 0.14814928296001023\n",
            "Epoch: 99 | Iteration: 32192 | Loss: 0.43282321287908293\n",
            "Epoch: 99 | Iteration: 32256 | Loss: 0.1910633265492736\n",
            "Epoch: 99 | Iteration: 32320 | Loss: 0.676310998132927\n",
            "Epoch: 99 | Iteration: 32384 | Loss: 0.3019900729015055\n",
            "Epoch: 99 | Iteration: 32448 | Loss: 1.0224540443078531\n",
            "Epoch: 99 | Iteration: 32512 | Loss: 0.2815155029068119\n",
            "Epoch: 99 | Iteration: 32576 | Loss: 0.0006410568262918911\n",
            "Epoch: 99 | Iteration: 32640 | Loss: 0.29629753063113373\n",
            "Epoch: 99 | Iteration: 32704 | Loss: 0.29008423823255935\n",
            "Epoch: 99 | Iteration: 32768 | Loss: 0.8147692935941144\n",
            "Epoch: 99 | Iteration: 32832 | Loss: 0.626518105358095\n",
            "Epoch: 99 | Iteration: 32896 | Loss: 0.6582777449081448\n",
            "Epoch: 99 | Iteration: 32960 | Loss: 0.013237681285084232\n",
            "Epoch: 99 | Iteration: 33024 | Loss: 0.2476909394765987\n",
            "Epoch: 99 | Iteration: 33088 | Loss: 0.031928364816767335\n",
            "Epoch: 99 | Iteration: 33152 | Loss: 0.07243893251109836\n",
            "Epoch: 99 | Iteration: 33216 | Loss: 0.19982574024962232\n",
            "Epoch: 99 | Iteration: 33280 | Loss: 0.24722482052736489\n",
            "Epoch: 99 | Iteration: 33344 | Loss: 0.32281718227806\n",
            "Epoch: 99 | Iteration: 33408 | Loss: 0.5817424792469614\n",
            "Epoch: 99 | Iteration: 33472 | Loss: 0.4978679366336086\n",
            "Epoch: 99 | Iteration: 33536 | Loss: 0.3976332125097547\n",
            "Epoch: 99 | Iteration: 33600 | Loss: 0.06626629399789652\n",
            "Epoch: 99 | Iteration: 33664 | Loss: 0.12442315464645044\n",
            "Epoch: 99 | Iteration: 33728 | Loss: 0.4242091975161791\n",
            "Epoch: 99 | Iteration: 33792 | Loss: 0.11172823883661895\n",
            "Epoch: 99 | Iteration: 33856 | Loss: 0.010370490242171711\n",
            "Epoch: 99 | Iteration: 33920 | Loss: 0.012302703183861324\n",
            "Epoch: 99 | Iteration: 33984 | Loss: 0.5193725751526479\n",
            "Epoch: 99 | Iteration: 34048 | Loss: 0.4621512011259039\n",
            "Epoch: 99 | Iteration: 34112 | Loss: 0.012162665644143538\n",
            "Epoch: 99 | Iteration: 34176 | Loss: 0.05524317404906029\n",
            "Epoch: 99 | Iteration: 34240 | Loss: 0.0026815127040677746\n",
            "Epoch: 99 | Iteration: 34304 | Loss: 0.10407509991981262\n",
            "Epoch: 99 | Iteration: 34368 | Loss: 1.341706858524877\n",
            "Epoch: 99 | Iteration: 34432 | Loss: 0.1476970766829553\n",
            "Epoch: 99 | Iteration: 34496 | Loss: 0.16328383577835368\n",
            "Epoch: 99 | Iteration: 34560 | Loss: 0.11765854215509564\n",
            "Epoch: 99 | Iteration: 34624 | Loss: 0.5772505902886982\n",
            "Epoch: 99 | Iteration: 34688 | Loss: 0.7837734555377878\n",
            "Epoch: 99 | Iteration: 34752 | Loss: 0.352579511672936\n",
            "Epoch: 99 | Iteration: 34816 | Loss: 0.7310521935917659\n",
            "Epoch: 99 | Iteration: 34880 | Loss: 0.4422115108032973\n",
            "Epoch: 99 | Iteration: 34944 | Loss: 0.04060063136095401\n",
            "Epoch: 99 | Iteration: 35008 | Loss: 1.2543122871597898\n",
            "Epoch: 99 | Iteration: 35072 | Loss: 0.027517856237426367\n",
            "Epoch: 99 | Iteration: 35136 | Loss: 0.98803820237252\n",
            "Epoch: 99 | Iteration: 35200 | Loss: 1.1670776001856693\n",
            "Epoch: 99 | Iteration: 35264 | Loss: 0.11054569582180107\n",
            "Epoch: 99 | Iteration: 35328 | Loss: 0.15223068705250145\n",
            "Epoch: 99 | Iteration: 35392 | Loss: 0.21650211924780807\n",
            "Epoch: 99 | Iteration: 35456 | Loss: 1.1286981668411062\n",
            "Epoch: 99 | Iteration: 35520 | Loss: 0.1361931166459575\n",
            "Epoch: 99 | Iteration: 35584 | Loss: 0.9826595467354823\n",
            "Epoch: 99 | Iteration: 35648 | Loss: 0.22742652126966947\n",
            "Epoch: 99 | Iteration: 35712 | Loss: 0.09876478053590465\n",
            "Epoch: 99 | Iteration: 35776 | Loss: 0.03978380488946613\n",
            "Epoch: 99 | Iteration: 35840 | Loss: 0.22353344197165276\n",
            "Epoch: 99 | Iteration: 35904 | Loss: 0.26044996615574467\n",
            "Epoch: 99 | Iteration: 35968 | Loss: 0.06221359970059046\n",
            "Epoch: 99 | Iteration: 36032 | Loss: 0.8837327419133834\n",
            "Epoch: 99 | Iteration: 36096 | Loss: 1.3073130685688903\n",
            "Epoch: 99 | Iteration: 36160 | Loss: 0.3867313325964621\n",
            "Epoch: 99 | Iteration: 36224 | Loss: 0.10035462475381215\n",
            "Epoch: 99 | Iteration: 36288 | Loss: 0.019536788581373365\n",
            "Epoch: 99 | Iteration: 36352 | Loss: 0.17063141413506444\n",
            "Epoch: 99 | Iteration: 36416 | Loss: 0.7078250234533947\n",
            "Epoch: 99 | Iteration: 36480 | Loss: 0.018688403848195926\n",
            "Epoch: 99 | Iteration: 36544 | Loss: 0.23043246464921743\n",
            "Epoch: 99 | Iteration: 36608 | Loss: 0.05811945649037957\n",
            "Epoch: 99 | Iteration: 36672 | Loss: 0.20727244123333494\n",
            "Epoch: 99 | Iteration: 36736 | Loss: 0.9057327315172836\n",
            "Epoch: 99 | Iteration: 36800 | Loss: 0.3764345959060944\n",
            "Epoch: 99 | Iteration: 36864 | Loss: 0.14627960298156742\n",
            "Epoch: 99 | Iteration: 36928 | Loss: 0.3170459092157396\n",
            "Epoch: 99 | Iteration: 36992 | Loss: 1.0520460530334201\n",
            "Epoch: 99 | Iteration: 37056 | Loss: 0.2762814775856428\n",
            "Epoch: 99 | Iteration: 37120 | Loss: 0.4274726343452282\n",
            "Epoch: 99 | Iteration: 37184 | Loss: 0.10190131682756462\n",
            "Epoch: 99 | Iteration: 37248 | Loss: 0.3589996365678878\n",
            "Epoch: 99 | Iteration: 37312 | Loss: 0.9108815382629546\n",
            "Epoch: 99 | Iteration: 37376 | Loss: 0.9305585912625122\n",
            "Epoch: 99 | Iteration: 37440 | Loss: 0.43720060354514717\n",
            "Epoch: 99 | Iteration: 37504 | Loss: 0.20374825334194374\n",
            "Epoch: 99 | Iteration: 37568 | Loss: 0.05019544771363672\n",
            "Epoch: 99 | Iteration: 37632 | Loss: 0.19480828771349296\n",
            "Epoch: 99 | Iteration: 37696 | Loss: 0.6097962494170497\n",
            "Epoch: 99 | Iteration: 37760 | Loss: 0.7433016964686091\n",
            "Epoch: 99 | Iteration: 37824 | Loss: 0.9906293106531687\n",
            "Epoch: 99 | Iteration: 37888 | Loss: 0.06272887554234888\n",
            "Epoch: 99 | Iteration: 37952 | Loss: 0.03994430291598173\n",
            "Epoch: 99 | Iteration: 38016 | Loss: 0.1484493725605145\n",
            "Epoch: 99 | Iteration: 38080 | Loss: 0.061335322222934315\n",
            "Epoch: 99 | Iteration: 38144 | Loss: 0.15082884330530144\n",
            "Epoch: 99 | Iteration: 38208 | Loss: 0.061978029966939546\n",
            "Epoch: 99 | Iteration: 38272 | Loss: 0.471064068873042\n",
            "Epoch: 99 | Iteration: 38336 | Loss: 0.6816819423229726\n",
            "Epoch: 99 | Iteration: 38400 | Loss: 0.13076750085144098\n",
            "Epoch: 99 | Iteration: 38464 | Loss: 0.5917463055297092\n",
            "Epoch: 99 | Iteration: 38528 | Loss: 0.504481395738394\n",
            "Epoch: 99 | Iteration: 38592 | Loss: 0.28366399965297706\n",
            "Epoch: 99 | Iteration: 38656 | Loss: 0.5780314109743661\n",
            "Epoch: 99 | Iteration: 38720 | Loss: 0.20205708719856652\n",
            "Epoch: 99 | Iteration: 38784 | Loss: 0.0067073247849394006\n",
            "Epoch: 99 | Iteration: 38848 | Loss: 0.07873852347183766\n",
            "Epoch: 99 | Iteration: 38912 | Loss: 0.2344515061960739\n",
            "Epoch: 99 | Iteration: 38976 | Loss: 0.19039397843555012\n",
            "Epoch: 99 | Iteration: 39040 | Loss: 0.006055066357371372\n",
            "Epoch: 99 | Iteration: 39104 | Loss: 0.09147588226760806\n",
            "Epoch: 99 | Iteration: 39168 | Loss: 0.6641667543272779\n",
            "Epoch: 99 | Iteration: 39232 | Loss: 0.36653121598963956\n",
            "Epoch: 99 | Iteration: 39296 | Loss: 0.8482963926016259\n",
            "Epoch: 99 | Iteration: 39360 | Loss: 0.9967512558374982\n",
            "Epoch: 99 | Iteration: 39424 | Loss: 0.16570981792644643\n",
            "Epoch: 99 | Iteration: 39488 | Loss: 0.047933242150606424\n",
            "Epoch: 99 | Iteration: 39552 | Loss: 0.40371990896252047\n",
            "Epoch: 99 | Iteration: 39616 | Loss: 0.44353946843623193\n",
            "Epoch: 99 | Iteration: 39680 | Loss: 0.3558402632857733\n",
            "Epoch: 99 | Iteration: 39744 | Loss: 0.6707736603722507\n",
            "Epoch: 99 | Iteration: 39808 | Loss: 0.6578915879753715\n",
            "Epoch: 99 | Iteration: 39872 | Loss: 0.14760869585482767\n",
            "Epoch: 99 | Iteration: 39936 | Loss: 0.05485171227383836\n",
            "Epoch: 99 | Iteration: 40000 | Loss: 0.2288198725101221\n",
            "Epoch: 99 | Iteration: 40064 | Loss: 0.21606661445709785\n",
            "Epoch: 99 | Iteration: 40128 | Loss: 0.13949741024941625\n",
            "Epoch: 99 | Iteration: 40192 | Loss: 0.2995310101604821\n",
            "Epoch: 99 | Iteration: 40256 | Loss: 0.4049191651946029\n",
            "Epoch: 99 | Iteration: 40320 | Loss: 0.18110805764021332\n",
            "Epoch: 99 | Iteration: 40384 | Loss: 0.2143237365525895\n",
            "Epoch: 99 | Iteration: 40448 | Loss: 0.05710001742607429\n",
            "Epoch: 99 | Iteration: 40512 | Loss: 0.08827230472667862\n",
            "Epoch: 99 | Iteration: 40576 | Loss: 0.6345670087960995\n",
            "Epoch: 99 | Iteration: 40640 | Loss: 0.591999590083665\n",
            "Epoch: 99 | Iteration: 40704 | Loss: 0.45465764738535663\n",
            "Epoch: 99 | Iteration: 40768 | Loss: 0.1955294493825638\n",
            "Epoch: 99 | Iteration: 40832 | Loss: 0.12388149294735562\n",
            "Epoch: 99 | Iteration: 40896 | Loss: 0.07298793854950347\n",
            "Epoch: 99 | Iteration: 40960 | Loss: 0.7614660957163473\n",
            "Epoch: 99 | Iteration: 41024 | Loss: 0.3846164479801498\n",
            "Epoch: 99 | Iteration: 41088 | Loss: 0.13179425521544363\n",
            "Epoch: 99 | Iteration: 41152 | Loss: 0.8099333811381599\n",
            "Epoch: 99 | Iteration: 41216 | Loss: 1.013359256825362\n",
            "Epoch: 99 | Iteration: 41280 | Loss: 0.758609761569356\n",
            "Epoch: 99 | Iteration: 41344 | Loss: 0.2736659142852895\n",
            "Epoch: 99 | Iteration: 41408 | Loss: 0.26050837452269615\n",
            "Epoch: 99 | Iteration: 41472 | Loss: 0.03769929210346791\n",
            "Epoch: 99 | Iteration: 41536 | Loss: 0.9628348425377931\n",
            "Epoch: 99 | Iteration: 41600 | Loss: 0.1218855717723972\n",
            "Epoch: 99 | Iteration: 41664 | Loss: 0.06734843003662588\n",
            "Epoch: 99 | Iteration: 41728 | Loss: 0.23877402681206383\n",
            "Epoch: 99 | Iteration: 41792 | Loss: 0.13805755809049833\n",
            "Epoch: 99 | Iteration: 41856 | Loss: 0.46227381189452876\n",
            "Epoch: 99 | Iteration: 41920 | Loss: 0.326893706739972\n",
            "Epoch: 99 | Iteration: 41984 | Loss: 0.08127099333466545\n",
            "Epoch: 99 | Iteration: 42048 | Loss: 0.029918088442454315\n",
            "Epoch: 99 | Iteration: 42112 | Loss: 0.17682682940241384\n",
            "Epoch: 99 | Iteration: 42176 | Loss: 0.18491669949575168\n",
            "Epoch: 99 | Iteration: 42240 | Loss: 0.06326969239659246\n",
            "Epoch: 99 | Iteration: 42304 | Loss: 0.6426968243964173\n",
            "Epoch: 99 | Iteration: 42368 | Loss: 0.5049492024004573\n",
            "Epoch: 99 | Iteration: 42432 | Loss: 0.7555064203305076\n",
            "Epoch: 99 | Iteration: 42496 | Loss: 0.5405541709271379\n",
            "Epoch: 99 | Iteration: 42560 | Loss: 0.8923983571577175\n",
            "Epoch: 99 | Iteration: 42624 | Loss: 0.5959399800138676\n",
            "Epoch: 99 | Iteration: 42688 | Loss: 0.35281104691153375\n",
            "Epoch: 99 | Iteration: 42752 | Loss: 1.0207167443040528\n",
            "Epoch: 99 | Iteration: 42816 | Loss: 1.0066777544018786\n",
            "Epoch: 99 | Iteration: 42880 | Loss: 0.7149417693143401\n",
            "Epoch: 99 | Iteration: 42944 | Loss: 0.40907176889659996\n",
            "Epoch: 99 | Iteration: 43008 | Loss: 0.5905444252844664\n",
            "Epoch: 99 | Iteration: 43072 | Loss: 1.314671153193575\n",
            "Epoch: 99 | Iteration: 43136 | Loss: 0.03596278721963386\n",
            "Epoch: 99 | Iteration: 43200 | Loss: 0.08349724129153877\n",
            "Epoch: 99 | Iteration: 43264 | Loss: 0.00256151758104015\n",
            "Epoch: 99 | Iteration: 43328 | Loss: 0.01468541179260206\n",
            "Epoch: 99 | Iteration: 43392 | Loss: 0.37970810902839314\n",
            "Epoch: 99 | Iteration: 43456 | Loss: 0.08457187703349661\n",
            "Epoch: 99 | Iteration: 43520 | Loss: 0.36270586599075344\n",
            "Epoch: 99 | Iteration: 43584 | Loss: 0.18516593172307214\n",
            "Epoch: 99 | Iteration: 43648 | Loss: 1.1945917261940582\n",
            "Epoch: 99 | Iteration: 43712 | Loss: 0.006106242707291764\n",
            "Epoch: 99 | Iteration: 43776 | Loss: 0.04676076433672776\n",
            "Epoch: 99 | Iteration: 43840 | Loss: 0.7800944156827408\n",
            "Epoch: 99 | Iteration: 43904 | Loss: 0.24070960773832428\n",
            "Epoch: 99 | Iteration: 43968 | Loss: 0.19086489732738512\n",
            "Epoch: 99 | Iteration: 44032 | Loss: 0.2392659870637568\n",
            "Epoch: 99 | Iteration: 44096 | Loss: 0.26236970013148014\n",
            "Epoch: 99 | Iteration: 44160 | Loss: 0.09698096516136154\n",
            "Epoch: 99 | Iteration: 44224 | Loss: 0.17348304139362564\n",
            "Epoch: 99 | Iteration: 44288 | Loss: 0.31808491885453205\n",
            "Epoch: 99 | Iteration: 44352 | Loss: 0.30182269846148535\n",
            "Epoch: 99 | Iteration: 44416 | Loss: 1.0451682498414154\n",
            "Epoch: 99 | Iteration: 44480 | Loss: 0.2967119276460178\n",
            "Epoch: 99 | Iteration: 44544 | Loss: 0.029565572245177862\n",
            "Epoch: 99 | Iteration: 44608 | Loss: 0.3789296457926753\n",
            "Epoch: 99 | Iteration: 44672 | Loss: 0.06067349528638175\n",
            "Epoch: 99 | Iteration: 44736 | Loss: 0.035879488551352486\n",
            "Epoch: 99 | Iteration: 44800 | Loss: 0.37190321465143256\n",
            "Epoch: 99 | Iteration: 44864 | Loss: 0.17670829402263943\n",
            "Epoch: 99 | Iteration: 44928 | Loss: 0.6084192615342711\n",
            "Epoch: 99 | Iteration: 44992 | Loss: 0.1499145000140812\n",
            "Epoch: 99 | Iteration: 45056 | Loss: 0.15653234075845343\n",
            "Epoch: 99 | Iteration: 45120 | Loss: 0.8361497890666477\n",
            "Epoch: 99 | Iteration: 45184 | Loss: 0.9491201843215058\n",
            "Epoch: 99 | Iteration: 45248 | Loss: 0.33072126449826184\n",
            "Epoch: 99 | Iteration: 45312 | Loss: 0.5591800973467517\n",
            "Epoch: 99 | Iteration: 45376 | Loss: 0.2911898401949597\n",
            "Epoch: 99 | Iteration: 45440 | Loss: 0.7623646691664996\n",
            "Epoch: 99 | Iteration: 45504 | Loss: 0.34357625836468997\n",
            "Epoch: 99 | Iteration: 45568 | Loss: 0.5579588477675488\n",
            "Epoch: 99 | Iteration: 45632 | Loss: 0.1422711836617288\n",
            "Epoch: 99 | Iteration: 45696 | Loss: 0.007017140560919505\n",
            "Epoch: 99 | Iteration: 45760 | Loss: 0.5035342617658022\n",
            "Epoch: 99 | Iteration: 45824 | Loss: 0.3083226068115439\n",
            "Epoch: 99 | Iteration: 45888 | Loss: 0.6468207543910051\n",
            "Epoch: 99 | Iteration: 45952 | Loss: 0.49176041878052157\n",
            "Epoch: 99 | Iteration: 46016 | Loss: 0.9986722375435587\n",
            "Epoch: 99 | Iteration: 46080 | Loss: 1.143461585880809\n",
            "Epoch: 99 | Iteration: 46144 | Loss: 0.06736832183312415\n",
            "Epoch: 99 | Iteration: 46208 | Loss: 1.2998735609298895\n",
            "Epoch: 99 | Iteration: 46272 | Loss: 1.2270691924621462\n",
            "Epoch: 99 | Iteration: 46336 | Loss: 0.4326399207789885\n",
            "Epoch: 99 | Iteration: 46400 | Loss: 0.2863736771472409\n",
            "Epoch: 99 | Iteration: 46464 | Loss: 0.015294287976569485\n",
            "Epoch: 99 | Iteration: 46528 | Loss: 0.03567819457098212\n",
            "Epoch: 99 | Iteration: 46592 | Loss: 0.04048363449842472\n",
            "Epoch: 99 | Iteration: 46656 | Loss: 0.21794641994853717\n",
            "Epoch: 99 | Iteration: 46720 | Loss: 0.8198233094089056\n",
            "Epoch: 99 | Iteration: 46784 | Loss: 0.03868089428608966\n",
            "Epoch: 99 | Iteration: 46848 | Loss: 0.6991177874828794\n",
            "Epoch: 99 | Iteration: 46912 | Loss: 0.30731642142767723\n",
            "Epoch: 99 | Iteration: 46976 | Loss: 1.236979492211089\n",
            "Epoch: 99 | Iteration: 47040 | Loss: 0.03643026496047107\n",
            "Epoch: 99 | Iteration: 47104 | Loss: 0.2778421732343737\n",
            "Epoch: 99 | Iteration: 47168 | Loss: 0.2969266503102448\n",
            "Epoch: 99 | Iteration: 47232 | Loss: 1.3047195745104778\n",
            "Epoch: 99 | Iteration: 47296 | Loss: 0.3316685404454896\n",
            "Epoch: 99 | Iteration: 47360 | Loss: 0.8419756296425445\n",
            "Epoch: 99 | Iteration: 47424 | Loss: 0.2547243857672468\n",
            "Epoch: 99 | Iteration: 47488 | Loss: 0.08459145887572328\n",
            "Epoch: 99 | Iteration: 47552 | Loss: 1.1795660284241396\n",
            "Epoch: 99 | Iteration: 47616 | Loss: 0.013249719420470162\n",
            "Epoch: 99 | Iteration: 47680 | Loss: 1.332040121517157\n",
            "Epoch: 99 | Iteration: 47744 | Loss: 0.8101000627260416\n",
            "Epoch: 99 | Iteration: 47808 | Loss: 0.07463531165212015\n",
            "Epoch: 99 | Iteration: 47872 | Loss: 0.7183971205030202\n",
            "Epoch: 99 | Iteration: 47936 | Loss: 0.439380576324438\n",
            "Test Accuracy: 0.9779\n",
            "Confusion Matrix:\n",
            "[[ 970    0    1    2    0    2    3    1    1    0]\n",
            " [   0 1122    3    1    0    1    2    2    4    0]\n",
            " [   5    0 1010    1    1    0    4    4    7    0]\n",
            " [   0    0    2  997    0    2    1    3    3    2]\n",
            " [   1    0    3    1  958    0    4    2    1   12]\n",
            " [   4    0    0   15    1  859    6    1    4    2]\n",
            " [   7    3    1    0    2    4  935    2    4    0]\n",
            " [   1    2   10    4    0    0    0 1003    1    7]\n",
            " [   3    0    3    5    2    4    4    3  949    1]\n",
            " [   3    2    0    7    8    4    1    7    1  976]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "\n",
        "def forward_propagation(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, x):\n",
        "    # Forward pass\n",
        "    hidden_input = np.dot(weights_input_hidden, x) + biases_input_hidden\n",
        "    hidden_output = sigmoid(hidden_input)\n",
        "    final_input = np.dot(weights_hidden_output, hidden_output) + biases_hidden_output\n",
        "    final_output = softmax(final_input)\n",
        "    return hidden_input, hidden_output, final_input, final_output\n",
        "\n",
        "def backward_propagation(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, x, y, hidden_input, hidden_output, final_input, final_output, learning_rate):\n",
        "    m = x.shape[1]  # Number of samples\n",
        "\n",
        "    # Compute error and cost (Cross-entropy loss)\n",
        "    error = final_output - y\n",
        "    cost = 0.5 * np.sum(error ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    delta_output = final_output - y\n",
        "    delta_hidden = np.dot(weights_hidden_output.T, delta_output) * hidden_output * (1 - hidden_output)\n",
        "\n",
        "    # Gradient for weights and biases\n",
        "    grad_weights_hidden_output = np.dot(delta_output, hidden_output.T) / m\n",
        "    grad_biases_hidden_output = np.sum(delta_output, axis=1, keepdims=True) / m\n",
        "    grad_weights_input_hidden = np.dot(delta_hidden, x.T) / m\n",
        "    grad_biases_input_hidden = np.sum(delta_hidden, axis=1, keepdims=True) / m\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights_hidden_output -= learning_rate * grad_weights_hidden_output\n",
        "    biases_hidden_output -= learning_rate * grad_biases_hidden_output\n",
        "    weights_input_hidden -= learning_rate * grad_weights_input_hidden\n",
        "    biases_input_hidden -= learning_rate * grad_biases_input_hidden\n",
        "\n",
        "    return cost, weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output\n",
        "\n",
        "def train(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, X, y, epochs, learning_rate, batch_size):\n",
        "    input_size, hidden_size, output_size = weights_input_hidden.shape[1], weights_hidden_output.shape[0], weights_hidden_output.shape[1]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, X.shape[0], batch_size):\n",
        "            x_batch = X[i:i + batch_size].T\n",
        "            y_batch = y[i:i + batch_size].T\n",
        "\n",
        "            hidden_input, hidden_output, final_input, final_output = forward_propagation(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, x_batch)\n",
        "\n",
        "            cost, weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output = backward_propagation(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, x_batch, y_batch, hidden_input, hidden_output, final_input, final_output, learning_rate)\n",
        "\n",
        "            print(\"Epoch:\", epoch, \"| Iteration:\", i, \"| Loss:\", cost)\n",
        "\n",
        "def predict(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, X):\n",
        "    _, _, _, final_output = forward_propagation(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, X.T)\n",
        "    return np.argmax(final_output, axis=0)\n",
        "\n",
        "# Assume mnist data is loaded here\n",
        "\n",
        "# Load and preprocess data\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X.reshape(train_X.shape[0], -1) / 255.0\n",
        "test_X = test_X.reshape(test_X.shape[0], -1) / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "train_y_one_hot = pd.get_dummies(train_y).values\n",
        "test_y_one_hot = pd.get_dummies(test_y).values\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "split_idx = int(0.8 * train_X.shape[0])\n",
        "train_X, val_X = train_X[:split_idx], train_X[split_idx:]\n",
        "train_y_one_hot, val_y_one_hot = train_y_one_hot[:split_idx], train_y_one_hot[split_idx:]\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_size = 784\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "weights_input_hidden = np.random.randn(hidden_size, input_size) * 0.01\n",
        "weights_hidden_output = np.random.randn(output_size, hidden_size) * 0.01\n",
        "biases_input_hidden = np.zeros((hidden_size, 1))\n",
        "biases_hidden_output = np.zeros((output_size, 1))\n",
        "\n",
        "# Train the neural network\n",
        "epochs = 100\n",
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "train(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, train_X, train_y_one_hot, epochs, learning_rate, batch_size)\n",
        "\n",
        "# Test the model\n",
        "predictions = predict(weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, test_X)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predictions == test_y)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(test_y, predictions)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    }
  ]
}